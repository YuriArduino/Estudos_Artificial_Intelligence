{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMaKTYFlm34imy83XTn91+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriArduino/Estudos_Artificial_Intelligence/blob/Imers%C3%A3o-Agentes-de-IA---Alura/Imers%C3%A3o_Dev_Agentes_IA_Google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Prepararando o ambiente"
      ],
      "metadata": {
        "id": "YLGLpS9SiwDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2Dependências"
      ],
      "metadata": {
        "id": "Fzn5Ri6gio40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NX0_N1ahdF1_"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3Imports"
      ],
      "metadata": {
        "id": "B3Wa9fsVigNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "# Garanta que a variável de ambiente está configurada\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "import sys\n",
        "import textwrap\n",
        "import time"
      ],
      "metadata": {
        "id": "ZFsJh7ZFfRTl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4Conexão com o modelo"
      ],
      "metadata": {
        "id": "3iYwtIlBi4XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=1.0, # Maior mais criativo, Menor mais objetivo\n",
        "    api_key= ('GOOGLE_API_KEY')\n",
        "    )"
      ],
      "metadata": {
        "id": "mUwH8RWxg9-f"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Para saber mais: Diferença **google-generativeai** X **langchain_google_genai**\n",
        "\n",
        "\n",
        "*   **`google-generativeai` (a biblioteca nativa do Google)**: É como o **motor de um carro**. É a peça de engenharia principal, super poderosa, que faz o trabalho fundamental (gerar texto, contar tokens, etc.). Ela te dá acesso direto e total a todas as funcionalidades específicas daquele motor (o Gemini).\n",
        "\n",
        "*   **LangChain (a biblioteca de orquestração)**: É como o **carro completo construído ao redor do motor**. Ele usa o motor (Gemini), mas adiciona o chassi, o volante, os pedais, o painel e os assentos. Ele torna o motor mais fácil de usar para um propósito maior (dirigir de um ponto A para um ponto B) e o conecta com outras partes (rodas, sistema de som, GPS).\n",
        "\n",
        "Vamos detalhar as diferenças práticas:\n",
        "\n",
        "---\n",
        "\n",
        "### Tabela Comparativa\n",
        "\n",
        "| Característica | `google-generativeai` (Nativa) | `langchain_google_genai` (Via LangChain) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Propósito Principal** | Acesso direto e completo à API do Gemini. | Construir aplicações complexas com LLMs, orquestrando várias etapas. |\n",
        "| **Nível de Abstração** | **Baixo.** Você interage diretamente com os conceitos da API do Google. | **Alto.** LangChain cria uma \"camada de compatibilidade\" sobre vários modelos. |\n",
        "| **Funcionalidades** | Geração de conteúdo, contagem de tokens, embeddings, ajuste de segurança, etc. | **Tudo da nativa, e mais:** Chains, Agentes, Memória, RAG, etc. |\n",
        "| **Flexibilidade de Modelo** | Feito exclusivamente para os modelos da família Gemini. | **Multi-modelo.** O código que você escreve pode ser facilmente adaptado para usar o GPT-4, Claude, etc. |\n",
        "| **Facilidade (Tarefas Simples)** | Geralmente mais simples para uma única chamada de API. | Envolve um pouco mais de \"boilerplate\" (código de configuração) inicial. |\n",
        "| **Facilidade (Tarefas Complexas)**| Você precisa construir toda a lógica (ex: memória de chat) do zero. | **Muito mais fácil.** Já fornece componentes prontos para tarefas complexas. |\n",
        "\n",
        "---\n",
        "\n",
        "### O que isso significa na prática?\n",
        "\n",
        "#### 1. Abstração e Portabilidade\n",
        "\n",
        "Com LangChain, o objeto `llm` que você criou é padronizado. Se amanhã você quisesse testar o modelo da OpenAI, você mudaria poucas linhas:\n",
        "\n",
        "```python\n",
        "# Com Google\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "# Com OpenAI (exemplo)\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# O resto do seu código LangChain (chains, agentes) continuaria funcionando igual!\n",
        "llm.invoke(\"Me conte uma piada.\")\n",
        "```\n",
        "\n",
        "Se você usasse a biblioteca nativa, teria que reescrever todo o código de interação com o modelo.\n",
        "\n",
        "#### 2. O Poder do Ecossistema LangChain\n",
        "\n",
        "A verdadeira magia de usar LangChain, e o motivo pelo qual é ensinado na imersão, não é apenas para fazer uma pergunta ao modelo. É para construir sistemas mais complexos:\n",
        "\n",
        "*   **Chains (Correntes):** Você pode criar sequências de tarefas. Por exemplo: \"Passo 1: Pegue a pergunta do usuário. Passo 2: Traduza para o inglês. Passo 3: Envie ao Gemini. Passo 4: Pegue a resposta e traduza de volta para o português.\"\n",
        "*   **Agentes (Agents):** Você pode dar ferramentas ao LLM. Por exemplo, dar a ele uma ferramenta de busca no Google. Se você perguntar \"Qual a previsão do tempo para amanhã?\", o agente pode decidir usar a ferramenta de busca, pegar o resultado e só então usar o Gemini para te dar uma resposta em linguagem natural.\n",
        "*   **RAG (Retrieval-Augmented Generation):** É a técnica mais popular. Você pode fazer o Gemini \"conversar com seus documentos\". Você fornece uma base de dados (PDFs, sites, etc.), e o LangChain cuida de buscar a informação relevante nesses documentos para que o Gemini possa responder perguntas sobre eles.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "Para a imersão, ficar com **LangChain é a decisão certa**. Você não está apenas aprendendo a usar o Gemini, mas sim a **como construir aplicações robustas em torno de um LLM**, que é a habilidade mais valiosa no mercado hoje.\n",
        "\n",
        "A contagem de tokens ser `llm.get_num_tokens()` em vez de `model.count_tokens()` é um pequeno sintoma dessa camada de abstração que o LangChain adiciona para tornar tudo mais padronizado e poderoso."
      ],
      "metadata": {
        "id": "k9qT4aVUo8is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "O resultado de `llm.invoke()` não é um texto puro, mas sim um **objeto**. No LangChain, esse objeto é chamado de `AIMessage`.\n",
        "\n",
        "Este objeto `AIMessage` funciona como um \"envelope\" que contém várias informações sobre a resposta do modelo:\n",
        "\n",
        "1.  **.content**: O conteúdo principal, ou seja, o texto da resposta que você queria. É o que está *dentro* do envelope.\n",
        "2.  **.response_metadata**: Informações extras sobre a execução, como o motivo pelo qual o modelo parou de gerar texto (`finish_reason`) e as classificações de segurança (`safety_ratings`).\n",
        "3.  **.usage_metadata**: Dados sobre o consumo de tokens na chamada (quantos tokens na entrada, quantos na saída e o total).\n",
        "4.  **.id**: Um identificador único para aquela execução específica, útil para depuração.\n",
        "\n",
        "\n",
        "### Comparando com o `StrOutputParser`\n",
        "\n",
        "Agora você entende perfeitamente a diferença:\n",
        "\n",
        "*   **Usar `.content`**: É a forma manual e direta de extrair o texto de uma **única** chamada. É perfeito para testes rápidos e quando você não está construindo uma sequência complexa de passos.\n",
        "\n",
        "*   **Usar `| StrOutputParser()`**: É a forma automática de fazer a mesma coisa, mas de um jeito que se integra com o sistema de \"correntes\" (chains) do LangChain. Ele já \"desempacota\" o conteúdo para você e o entrega pronto para o próximo passo da sua chain.\n"
      ],
      "metadata": {
        "id": "pDtdiqoNlEX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Estamos Online?\")\n",
        "print(resp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GvY8H7lKqk",
        "outputId": "1c6e7a45-1c58-4128-8dfe-36c8afbeef31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Sim, estamos!\\n\\nO fato de estarmos conversando agora confirma que ambos temos uma conexão com a internet.\\n\\nEm que posso te ajudar?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--802d9b49-e9b7-4d55-bf88-5b6b57ce4d2b-0' usage_metadata={'input_tokens': 4, 'output_tokens': 30, 'total_tokens': 744, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#.content organiza o texto\n",
        "resp_test = llm.invoke(\"Estamos Online?\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-17dGtnjAp1",
        "outputId": "4583f403-7577-4549-da19-b8cc1e505122"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sim, estamos!\n",
            "\n",
            "O fato de estarmos conversando agora confirma que ambos temos uma conexão com a internet.\n",
            "\n",
            "Em que posso te ajudar?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Tokens\n",
        "\n",
        "Para um modelo de linguagem como o Gemini, os **tokens são os blocos de construção do texto**.\n",
        "\n",
        "Um token não é exatamente uma palavra, uma letra ou uma sílaba. É a unidade fundamental que o modelo usa para processar e entender a linguagem.\n",
        "\n",
        "*   Uma palavra curta como `\"Eu\"` pode ser **1 token**.\n",
        "*   Uma palavra comum como `\"gato\"` pode ser **1 token**.\n",
        "*   Uma palavra mais longa ou incomum como `\"inteligência\"` pode ser dividida em **2 ou mais tokens** (ex: `inteli` + `gência`).\n",
        "*   Pontuação e espaços também contam e podem ser tokens.\n",
        "\n",
        "**Exemplo Rápido:**\n",
        "A frase `Olá, mundo!` é quebrada pelo modelo em algo como: `[\"Olá\", \",\", \" mundo\", \"!\"]` - resultando em **4 tokens**.\n",
        "\n",
        "Em resumo, para o modelo, toda a nossa conversa (tanto o que enviamos quanto o que ele responde) é uma sequência de tokens.\n",
        "\n",
        "### A Importância de um Contador de Tokens na nossa Imersão\n",
        "\n",
        "No desenvolvimento de software, todo recurso é finito. Contar tokens é uma prática de **gerenciamento de recursos computacionais**. Ignorar essa métrica é como escrever um código que consome memória sem controle. A ferramenta `count_querys` que construímos é crucial por três razões técnicas:\n",
        "\n",
        "**1. Gerenciamento de Custos e Carga de API:**\n",
        "   Cada token processado representa uma unidade de computação que tem um custo. Em um ambiente de produção, esse custo é financeiro. Na Imersão, ele se reflete no consumo da nossa cota gratuita. O contador funciona como um *profiler*, permitindo-nos:\n",
        "   *   Medir a \"carga\" exata que cada chamada impõe à API.\n",
        "   *   Otimizar a comunicação para ser mais eficiente e evitar exceder os limites de requisições por minuto (*rate limits*).\n",
        "\n",
        "**2. Respeitar a Janela de Contexto (Payload Capacity):**\n",
        "   Todo modelo de linguagem opera com uma \"janela de contexto\" finita – a quantidade máxima de tokens que ele pode considerar em uma única requisição. Essa janela é a memória de trabalho da operação.\n",
        "   *   Enviar um `payload` (dados de entrada) que exceda essa capacidade resultará em erro.\n",
        "   *   Nosso contador é uma ferramenta de validação prévia, garantindo que nossas requisições sejam sempre viáveis e estejam dentro das especificações do modelo.\n",
        "\n",
        "**3. Otimização de Prompts e Latência:**\n",
        "   A engenharia de prompts é uma disciplina de otimização. Um prompt eficiente transmite a instrução máxima com o mínimo de tokens.\n",
        "   *   **Sinal vs. Ruído:** Tokens irrelevantes são \"ruído\" que pode degradar a qualidade da resposta e aumentar a latência (tempo de resposta).\n",
        "   *   Ao monitorar a contagem, aprendemos a construir prompts mais concisos e potentes, melhorando tanto a performance da aplicação quanto a precisão dos resultados.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LpaIjPfXsFMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Utilitário de Desenvolvimento: Profiler de Custo para API Gemini"
      ],
      "metadata": {
        "id": "YyEVla-oe_5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class count_querys:\n",
        "    def __init__(self):\n",
        "        self.total_tokens = 0\n",
        "        print(\" Count de querys ativo!\")\n",
        "\n",
        "    def processar_query(self, llm):\n",
        "        \"\"\"Processa uma query com count automático e formatação de saída.\"\"\"\n",
        "        query = input(\"Digite seu texto: \")\n",
        "\n",
        "        if not query.strip():\n",
        "            print(\"Nenhum texto inserido.\")\n",
        "            return\n",
        "\n",
        "        # Envia para o LLM\n",
        "        resposta = llm.invoke(query)\n",
        "\n",
        "        # Conta os tokens desta query\n",
        "        tokens_desta_msg = resposta.usage_metadata.get('total_tokens', 0) if resposta.usage_metadata else 0\n",
        "        self.total_tokens += tokens_desta_msg\n",
        "\n",
        "        # Formata a resposta para ter no máximo 90 caracteres por linha\n",
        "        prefixo = \" Resposta: \"\n",
        "        largura_maxima = 90\n",
        "\n",
        "        # A função fill quebra o texto em linhas do tamanho desejado\n",
        "        # e adiciona um recuo nas linhas seguintes para manter o alinhamento.\n",
        "        texto_formatado = textwrap.fill(\n",
        "            resposta.content,\n",
        "            width=largura_maxima,\n",
        "            initial_indent=prefixo,\n",
        "            subsequent_indent=' ' * len(prefixo) # Recuo para alinhar com o início do texto\n",
        "        )\n",
        "\n",
        "        # Mostra a resposta da IA já formatada\n",
        "        print(f\"\\n{texto_formatado}\")\n",
        "\n",
        "        # Mostra o count no final\n",
        "        print(f\"\\n {tokens_desta_msg} tokens, total {self.total_tokens}\")\n",
        "# Crie o count uma vez\n",
        "count = count_querys()\n",
        "\n",
        "def query():\n",
        "    global count\n",
        "    if 'count' not in globals():\n",
        "        count = count_querys()\n",
        "    count.processar_query(llm)\n",
        "\n",
        "# Status do count\n",
        "def status():\n",
        "    \"\"\"Mostra o status atual dos tokens\"\"\"\n",
        "    global count\n",
        "    if 'count' not in globals():\n",
        "        print(\"❌ Nenhuma conversa iniciada ainda. Use query() primeiro!\")\n",
        "    else:\n",
        "        print(f\" Total de tokens usados: {count.total_tokens}\")\n",
        "\n",
        "def reset():\n",
        "    \"\"\"Reseta o count de tokens\"\"\"\n",
        "    global count\n",
        "    count = count_querys()\n",
        "    print(\" count resetado!\")\n",
        "\n",
        "def help_chat():\n",
        "  \"\"\"Mostra as utilidades do chat.\"\"\"\n",
        "  print(\"\"\"\n",
        "UTILIDADES:\n",
        "  query()     - Enviar mensagem\n",
        "  status()     - Ver total tokens\n",
        "  reset()      - Resetar count\n",
        "  help_chat()  - Esta ajuda\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iFcthA8sSbM",
        "outputId": "4a4ef2e8-f8eb-4f32-9d2b-fe6943b514d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Count de querys ativo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0pcMzuSvI_e",
        "outputId": "3c0a9fe5-a258-4d32-f6a1-db10d1cb2cc4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite seu texto: Estou testando a contagem de tokens!\n",
            "\n",
            " Resposta: Ótimo teste!  A sua frase \"Estou testando a contagem de tokens!\" tem **7\n",
            "           tokens**.  A divisão fica assim:  1.  `Estou` 2.  `testando` 3.  `a` 4.\n",
            "           `contagem` 5.  `de` 6.  `tokens` 7.  `!`  Neste caso, cada palavra e o sinal de\n",
            "           pontuação contaram como um token individual.  Quer testar outra frase para ver\n",
            "           como palavras mais complexas ou em outros idiomas são divididas?\n",
            "\n",
            " 1610 tokens, total 1610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTSPccFi2P7h",
        "outputId": "d9bbd45c-741c-47cb-f009-24129b643581"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total de tokens usados: 1610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset()\n",
        "print(status())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88TluarX2VLf",
        "outputId": "2d396c1c-1b8d-462d-fe00-dabbbd710a0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Count de querys ativo!\n",
            " count resetado!\n",
            " Total de tokens usados: 0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnGdvGJVfQuP",
        "outputId": "6e5459ea-c4c6-42b5-e413-bee17922b7e9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UTILIDADES:\n",
            "  query()     - Enviar mensagem\n",
            "  status()     - Ver total tokens\n",
            "  reset()      - Resetar count\n",
            "  help_chat()  - Esta ajuda\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Estruturando a Saída com Pydantic\n",
        "\n",
        "\n",
        "\n",
        "Para um programa de computador, processar variações  é propenso a erros. É importante uma estrutura de dados confiável, não um texto para interpretar.\n",
        "\n",
        "### **Pydantic como um \"Contrato\" de Dados**\n",
        "\n",
        "**Pydantic** é uma biblioteca Python para **validação de dados**. Ela nos permite definir \"esquemas\" ou \"modelos\" de dados usando classes Python. Em outras palavras, criamos um **contrato** que define a estrutura e os tipos de dados que esperamos receber.\n",
        "\n",
        "O LangChain se integra perfeitamente com o Pydantic através do **`PydanticOutputParser`**. Este \"parser\" (analisador) faz duas coisas mágicas:\n",
        "\n",
        "1.  **Instrui o LLM:** Ele analisa nosso modelo Pydantic e gera instruções claras de formatação que são enviadas ao LLM junto com nosso prompt.\n",
        "2.  **Valida a Resposta:** Ele pega a resposta em texto do LLM, verifica se ela segue o \"contrato\", e a converte em um objeto Python real e validado.\n"
      ],
      "metadata": {
        "id": "dBUQWVC9VXt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1Configuração Inicial e Imports"
      ],
      "metadata": {
        "id": "YDBLQCSjauX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports de bibliotecas de tipagem e validação\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "# Imports principais do LangChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Configuração da API Key (maneira segura para o Colab)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Garanta que a variável de ambiente está configurada\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "jkzugn8faleX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta célula inicial é responsável por preparar nosso ambiente de trabalho.\n",
        ">\n",
        ">Importamos todas as bibliotecas necessárias, incluindo o Pydantic para definir nossa estrutura de dados e o LangChain para orquestrar a comunicação com o modelo Gemini.\n",
        ">\n",
        ">Também configuramos nossa chave de API do Google de forma segura usando os segredos do Colab."
      ],
      "metadata": {
        "id": "kgBVwcNia2vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2Definindo o \"Cérebro\" do Agente (O Prompt Principal)"
      ],
      "metadata": {
        "id": "xbu1psTzbcMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "jDfTiUi0bSv4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Aqui definimos o SystemMessage, a personalidade e as diretrizes do nosso agente de IA.\n",
        ">\n",
        ">Este prompt é o \"cérebro\" da operação, explicando ao modelo seu papel, as regras de negócio para cada tipo de decisão e o que esperamos como resultado."
      ],
      "metadata": {
        "id": "FVXg7gLzahXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3 Definindo o \"Contrato\" de Saída (O Esquema Pydantic)"
      ],
      "metadata": {
        "id": "4BOjY4SpZmw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriagemOut(BaseModel):\n",
        "    \"\"\"Define a estrutura de saída esperada para a triagem de chamados.\"\"\"\n",
        "\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"] = Field(\n",
        "        description=\"A decisão principal baseada na mensagem do usuário.\"\n",
        "    )\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"] = Field(\n",
        "        description=\"A urgência estimada do chamado.\"\n",
        "    )\n",
        "    campos_faltantes: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Uma lista de informações que o usuário precisa fornecer se a decisão for 'PEDIR_INFO'.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "NN8s9J-qZkcR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta é a parte central da validação. Usando Pydantic, criamos a classe TriagemOut que funciona como um \"contrato\".\n",
        ">\n",
        ">Ela força o LLM a gerar uma resposta que contenha exatamente os campos decisao, urgencia e campos_faltantes, com os tipos e valores que definimos. Se a IA gerar algo fora deste padrão, o LangChain acusará um erro."
      ],
      "metadata": {
        "id": "2WYvQEewabzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.4Célula Principal - Construindo e Executando a Lógica de Triagem"
      ],
      "metadata": {
        "id": "miGW3hh5Z7lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inicializa o LLM com temperatura 0.0 para respostas mais determinísticas\n",
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",  # Usando um modelo padrão e eficiente para essa tarefa\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# 2. Cria a \"chain\" que conecta o LLM ao nosso esquema de saída Pydantic\n",
        "# O método .with_structured_output() é o que faz a mágica acontecer!\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "# 3. Define a função que invoca a chain e formata a saída\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Invoca a chain de triagem com a mensagem do usuário e retorna um dicionário.\n",
        "    \"\"\"\n",
        "    # A chain recebe uma lista de mensagens (Sistema e Usuário)\n",
        "    saida_pydantic: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    # .model_dump() converte o objeto Pydantic validado em um dicionário Python\n",
        "    return saida_pydantic.model_dump()"
      ],
      "metadata": {
        "id": "wOzrV99IZurN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta é a célula principal onde a orquestração acontece. Nós:\n",
        "\n",
        "1)   Instanciamos o modelo Gemini, configurando a temperature para 0.0 para obter resultados mais consistentes e previsíveis.\n",
        "2)   Criamos a triagem_chain usando o poderoso método .with_structured_output(TriagemOut). É este comando que \"ensina\" o LLM a seguir nosso esquema Pydantic.\n",
        "3)   Definimos a função triagem(), que encapsula a lógica de chamada, enviando o prompt do sistema e a mensagem do usuário para a chain."
      ],
      "metadata": {
        "id": "AffYj2jhaPvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.5 Teste e Validação"
      ],
      "metadata": {
        "id": "K1rqrRCXaIWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\",\n",
        "          \"Tenho uma dúvida sobre um benefício.\"]\n",
        "\n",
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: '{msg_teste}'\")\n",
        "    resultado = triagem(msg_teste)\n",
        "    print(f\" -> Resposta Estruturada: {resultado}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdIrC1t8aGN6",
        "outputId": "98a54f96-3710-4906-fa81-a1b8475fa1b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: 'Posso reembolsar a internet?'\n",
            " -> Resposta Estruturada: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: 'Quero mais 5 dias de trabalho remoto. Como faço?'\n",
            " -> Resposta Estruturada: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: 'Posso reembolsar cursos ou treinamentos da Alura?'\n",
            " -> Resposta Estruturada: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: 'Quantas capivaras tem no Rio Pinheiros?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 2\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 15\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Resposta Estruturada: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['Qual política específica?', 'Qual a sua dúvida?']}\n",
            "\n",
            "Pergunta: 'Tenho uma dúvida sobre um benefício.'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhausted",
          "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 2\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 13\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1666252756.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmsg_teste\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pergunta: '{msg_teste}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresultado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_teste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" -> Resposta Estruturada: {resultado}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4169848087.py\u001b[0m in \u001b[0;36mtriagem\u001b[0;34m(mensagem)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# A chain recebe uma lista de mensagens (Sistema e Usuário)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     saida_pydantic: TriagemOut = triagem_chain.invoke([\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mSystemMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRIAGEM_PROMPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmensagem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5493\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5494\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5495\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5496\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         return cast(\n\u001b[1;32m    392\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    394\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1018\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 results.append(\n\u001b[0;32m--> 837\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    838\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n\u001b[0;32m--> 961\u001b[0;31m         response: GenerateContentResponse = _chat_with_retry(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             ) from e\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Do not retry for these errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFailedPrecondition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-pro\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 2\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 13\n}\n]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.6 **Teste e Validação(Com Controle de Taxa)**\n",
        "\n",
        "> **Nota Importante sobre Limites de API (Rate Limiting)**\n",
        ">\n",
        "> Ao executar o loop `for` sem pausas, enviamos todas as nossas perguntas à API quase simultaneamente. Isso aciona um mecanismo de proteção da API do Google chamado **rate limiting**, resultando no erro `ResourceExhausted: 429`. Essencialmente, a API nos diz: \"Por favor, vá com mais calma!\".\n",
        ">\n",
        "> Para resolver isso, introduzimos o comando **`time.sleep(10)`** dentro do loop. Este comando pausa a execução por 10 segundos entre cada chamada, garantindo que respeitamos os limites da camada gratuita (ex: 2-5 requisições por minuto).\n",
        ">\n",
        "> Gerenciar a frequência das chamadas é uma **prática fundamental** no desenvolvimento com qualquer serviço externo, garantindo que nossa aplicação seja robusta e não seja bloqueada."
      ],
      "metadata": {
        "id": "5BfPDj8gdyzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos a biblioteca 'time' para adicionar pausas\n",
        "import time\n",
        "\n",
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\",\n",
        "          \"Tenho uma dúvida sobre um benefício.\"]\n",
        "\n",
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: '{msg_teste}'\")\n",
        "    resultado = triagem(msg_teste)\n",
        "    print(f\" -> Resposta Estruturada: {resultado}\\n\")\n",
        "\n",
        "    # --- AJUSTE IMPORTANTE AQUI ---\n",
        "    # Adiciona uma pausa de 10 segundos para não exceder o limite da API.\n",
        "    # Para o free tier, um valor entre 10 e 20 segundos é seguro.\n",
        "    print(\"...aguardando 10 segundos para a próxima requisição...\\n\")\n",
        "    time.sleep(10)\n",
        "\n",
        "print(\"✅ Todos os testes foram concluídos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7n-VdWyc1KW",
        "outputId": "4eb990cf-6c2d-4265-d5b0-46184d02c0f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: 'Posso reembolsar a internet?'\n",
            " -> Resposta Estruturada: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "...aguardando 10 segundos para a próxima requisição...\n",
            "\n",
            "Pergunta: 'Quero mais 5 dias de trabalho remoto. Como faço?'\n",
            " -> Resposta Estruturada: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "...aguardando 10 segundos para a próxima requisição...\n",
            "\n",
            "Pergunta: 'Posso reembolsar cursos ou treinamentos da Alura?'\n",
            " -> Resposta Estruturada: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "...aguardando 10 segundos para a próxima requisição...\n",
            "\n",
            "Pergunta: 'Quantas capivaras tem no Rio Pinheiros?'\n",
            " -> Resposta Estruturada: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['Qual política ou procedimento interno você tem dúvida?']}\n",
            "\n",
            "...aguardando 10 segundos para a próxima requisição...\n",
            "\n",
            "Pergunta: 'Tenho uma dúvida sobre um benefício.'\n",
            " -> Resposta Estruturada: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['qual benefício']}\n",
            "\n",
            "...aguardando 10 segundos para a próxima requisição...\n",
            "\n",
            "✅ Todos os testes foram concluídos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Com tudo configurado, esta célula executa um conjunto de testes para validar nosso sistema. Observamos como ele classifica diferentes tipos de perguntas, demonstrando a eficácia do prompt e da estruturação de saída com Pydantic."
      ],
      "metadata": {
        "id": "ygBixDwUaMTz"
      }
    }
  ]
}