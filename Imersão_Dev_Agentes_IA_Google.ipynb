{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1N/qKLCMJSKwh1Sqp3T4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriArduino/Estudos_Artificial_Intelligence/blob/Imers%C3%A3o-Agentes-de-IA---Alura/Imers%C3%A3o_Dev_Agentes_IA_Google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Prepararando o ambiente"
      ],
      "metadata": {
        "id": "YLGLpS9SiwDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Depend√™ncias"
      ],
      "metadata": {
        "id": "Fzn5Ri6gio40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NX0_N1ahdF1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2e2d75-cd99-41a6-b586-8e4649c805af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Imports"
      ],
      "metadata": {
        "id": "B3Wa9fsVigNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "import sys\n",
        "import textwrap\n",
        "import time"
      ],
      "metadata": {
        "id": "ZFsJh7ZFfRTl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Conex√£o com o modelo"
      ],
      "metadata": {
        "id": "3iYwtIlBi4XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=1.0, # Maior mais criativo, Menor mais objetivo\n",
        "    )"
      ],
      "metadata": {
        "id": "mUwH8RWxg9-f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Para saber mais: Diferen√ßa **google-generativeai** X **langchain_google_genai**\n",
        "\n",
        "\n",
        "*   **`google-generativeai` (a biblioteca nativa do Google)**: √â como o **motor de um carro**. √â a pe√ßa de engenharia principal, super poderosa, que faz o trabalho fundamental (gerar texto, contar tokens, etc.). Ela te d√° acesso direto e total a todas as funcionalidades espec√≠ficas daquele motor (o Gemini).\n",
        "\n",
        "*   **LangChain (a biblioteca de orquestra√ß√£o)**: √â como o **carro completo constru√≠do ao redor do motor**. Ele usa o motor (Gemini), mas adiciona o chassi, o volante, os pedais, o painel e os assentos. Ele torna o motor mais f√°cil de usar para um prop√≥sito maior (dirigir de um ponto A para um ponto B) e o conecta com outras partes (rodas, sistema de som, GPS).\n",
        "\n",
        "Vamos detalhar as diferen√ßas pr√°ticas:\n",
        "\n",
        "---\n",
        "\n",
        "### Tabela Comparativa\n",
        "\n",
        "| Caracter√≠stica | `google-generativeai` (Nativa) | `langchain_google_genai` (Via LangChain) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Prop√≥sito Principal** | Acesso direto e completo √† API do Gemini. | Construir aplica√ß√µes complexas com LLMs, orquestrando v√°rias etapas. |\n",
        "| **N√≠vel de Abstra√ß√£o** | **Baixo.** Voc√™ interage diretamente com os conceitos da API do Google. | **Alto.** LangChain cria uma \"camada de compatibilidade\" sobre v√°rios modelos. |\n",
        "| **Funcionalidades** | Gera√ß√£o de conte√∫do, contagem de tokens, embeddings, ajuste de seguran√ßa, etc. | **Tudo da nativa, e mais:** Chains, Agentes, Mem√≥ria, RAG, etc. |\n",
        "| **Flexibilidade de Modelo** | Feito exclusivamente para os modelos da fam√≠lia Gemini. | **Multi-modelo.** O c√≥digo que voc√™ escreve pode ser facilmente adaptado para usar o GPT-4, Claude, etc. |\n",
        "| **Facilidade (Tarefas Simples)** | Geralmente mais simples para uma √∫nica chamada de API. | Envolve um pouco mais de \"boilerplate\" (c√≥digo de configura√ß√£o) inicial. |\n",
        "| **Facilidade (Tarefas Complexas)**| Voc√™ precisa construir toda a l√≥gica (ex: mem√≥ria de chat) do zero. | **Muito mais f√°cil.** J√° fornece componentes prontos para tarefas complexas. |\n",
        "\n",
        "---\n",
        "\n",
        "### O que isso significa na pr√°tica?\n",
        "\n",
        "#### 1. Abstra√ß√£o e Portabilidade\n",
        "\n",
        "Com LangChain, o objeto `llm` que voc√™ criou √© padronizado. Se amanh√£ voc√™ quisesse testar o modelo da OpenAI, voc√™ mudaria poucas linhas:\n",
        "\n",
        "```python\n",
        "# Com Google\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "# Com OpenAI (exemplo)\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# O resto do seu c√≥digo LangChain (chains, agentes) continuaria funcionando igual!\n",
        "llm.invoke(\"Me conte uma piada.\")\n",
        "```\n",
        "\n",
        "Se voc√™ usasse a biblioteca nativa, teria que reescrever todo o c√≥digo de intera√ß√£o com o modelo.\n",
        "\n",
        "#### 2. O Poder do Ecossistema LangChain\n",
        "\n",
        "A verdadeira magia de usar LangChain, e o motivo pelo qual √© ensinado na imers√£o, n√£o √© apenas para fazer uma pergunta ao modelo. √â para construir sistemas mais complexos:\n",
        "\n",
        "*   **Chains (Correntes):** Voc√™ pode criar sequ√™ncias de tarefas. Por exemplo: \"Passo 1: Pegue a pergunta do usu√°rio. Passo 2: Traduza para o ingl√™s. Passo 3: Envie ao Gemini. Passo 4: Pegue a resposta e traduza de volta para o portugu√™s.\"\n",
        "*   **Agentes (Agents):** Voc√™ pode dar ferramentas ao LLM. Por exemplo, dar a ele uma ferramenta de busca no Google. Se voc√™ perguntar \"Qual a previs√£o do tempo para amanh√£?\", o agente pode decidir usar a ferramenta de busca, pegar o resultado e s√≥ ent√£o usar o Gemini para te dar uma resposta em linguagem natural.\n",
        "*   **RAG (Retrieval-Augmented Generation):** √â a t√©cnica mais popular. Voc√™ pode fazer o Gemini \"conversar com seus documentos\". Voc√™ fornece uma base de dados (PDFs, sites, etc.), e o LangChain cuida de buscar a informa√ß√£o relevante nesses documentos para que o Gemini possa responder perguntas sobre eles.\n",
        "\n",
        "### Conclus√£o\n",
        "\n",
        "Para a imers√£o, ficar com **LangChain √© a decis√£o certa**. Voc√™ n√£o est√° apenas aprendendo a usar o Gemini, mas sim a **como construir aplica√ß√µes robustas em torno de um LLM**, que √© a habilidade mais valiosa no mercado hoje.\n",
        "\n",
        "A contagem de tokens ser `llm.get_num_tokens()` em vez de `model.count_tokens()` √© um pequeno sintoma dessa camada de abstra√ß√£o que o LangChain adiciona para tornar tudo mais padronizado e poderoso."
      ],
      "metadata": {
        "id": "k9qT4aVUo8is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Content\n"
      ],
      "metadata": {
        "id": "hkmDX-uk-OQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "O resultado de `llm.invoke()` n√£o √© um texto puro, mas sim um **objeto**. No LangChain, esse objeto √© chamado de `AIMessage`.\n",
        "\n",
        "Este objeto `AIMessage` funciona como um \"envelope\" que cont√©m v√°rias informa√ß√µes sobre a resposta do modelo:\n",
        "\n",
        "1.  **.content**: O conte√∫do principal, ou seja, o texto da resposta que voc√™ queria. √â o que est√° *dentro* do envelope.\n",
        "2.  **.response_metadata**: Informa√ß√µes extras sobre a execu√ß√£o, como o motivo pelo qual o modelo parou de gerar texto (`finish_reason`) e as classifica√ß√µes de seguran√ßa (`safety_ratings`).\n",
        "3.  **.usage_metadata**: Dados sobre o consumo de tokens na chamada (quantos tokens na entrada, quantos na sa√≠da e o total).\n",
        "4.  **.id**: Um identificador √∫nico para aquela execu√ß√£o espec√≠fica, √∫til para depura√ß√£o.\n",
        "\n",
        "\n",
        "### Comparando com o `StrOutputParser`\n",
        "\n",
        "Agora voc√™ entende perfeitamente a diferen√ßa:\n",
        "\n",
        "*   **Usar `.content`**: √â a forma manual e direta de extrair o texto de uma **√∫nica** chamada. √â perfeito para testes r√°pidos e quando voc√™ n√£o est√° construindo uma sequ√™ncia complexa de passos.\n",
        "\n",
        "*   **Usar `| StrOutputParser()`**: √â a forma autom√°tica de fazer a mesma coisa, mas de um jeito que se integra com o sistema de \"correntes\" (chains) do LangChain. Ele j√° \"desempacota\" o conte√∫do para voc√™ e o entrega pronto para o pr√≥ximo passo da sua chain.\n"
      ],
      "metadata": {
        "id": "pDtdiqoNlEX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Estamos Online?\")\n",
        "print(resp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GvY8H7lKqk",
        "outputId": "b02dfff2-649d-42ba-8570-b7ae3ab287ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='¬°S√≠! Estoy en l√≠nea y listo para ayudarte.\\n\\n¬øEn qu√© puedo asistirte hoy?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--8c3f49b7-c9d3-47b9-aad3-0de5ee8dcf4a-0' usage_metadata={'input_tokens': 4, 'output_tokens': 20, 'total_tokens': 751, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#.content organiza o texto\n",
        "resp_test = llm.invoke(\"Estamos Online?\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-17dGtnjAp1",
        "outputId": "cd0dafa3-44ab-4d96-a6ac-e5142700dca7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sim, estamos online! Se voc√™ est√° lendo esta resposta, significa que a conex√£o est√° funcionando perfeitamente. üòä\n",
            "\n",
            "Como posso te ajudar hoje?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Tokens\n",
        "\n",
        "Para um modelo de linguagem como o Gemini, os **tokens s√£o os blocos de constru√ß√£o do texto**.\n",
        "\n",
        "Um token n√£o √© exatamente uma palavra, uma letra ou uma s√≠laba. √â a unidade fundamental que o modelo usa para processar e entender a linguagem.\n",
        "\n",
        "*   Uma palavra curta como `\"Eu\"` pode ser **1 token**.\n",
        "*   Uma palavra comum como `\"gato\"` pode ser **1 token**.\n",
        "*   Uma palavra mais longa ou incomum como `\"intelig√™ncia\"` pode ser dividida em **2 ou mais tokens** (ex: `inteli` + `g√™ncia`).\n",
        "*   Pontua√ß√£o e espa√ßos tamb√©m contam e podem ser tokens.\n",
        "\n",
        "**Exemplo R√°pido:**\n",
        "A frase `Ol√°, mundo!` √© quebrada pelo modelo em algo como: `[\"Ol√°\", \",\", \" mundo\", \"!\"]` - resultando em **4 tokens**.\n",
        "\n",
        "Em resumo, para o modelo, toda a nossa conversa (tanto o que enviamos quanto o que ele responde) √© uma sequ√™ncia de tokens.\n",
        "\n",
        "### A Import√¢ncia de um Contador de Tokens na nossa Imers√£o\n",
        "\n",
        "No desenvolvimento de software, todo recurso √© finito. Contar tokens √© uma pr√°tica de **gerenciamento de recursos computacionais**. Ignorar essa m√©trica √© como escrever um c√≥digo que consome mem√≥ria sem controle. A ferramenta `count_querys` que constru√≠mos √© crucial por tr√™s raz√µes t√©cnicas:\n",
        "\n",
        "**1. Gerenciamento de Custos e Carga de API:**\n",
        "   Cada token processado representa uma unidade de computa√ß√£o que tem um custo. Em um ambiente de produ√ß√£o, esse custo √© financeiro. Na Imers√£o, ele se reflete no consumo da nossa cota gratuita. O contador funciona como um *profiler*, permitindo-nos:\n",
        "   *   Medir a \"carga\" exata que cada chamada imp√µe √† API.\n",
        "   *   Otimizar a comunica√ß√£o para ser mais eficiente e evitar exceder os limites de requisi√ß√µes por minuto (*rate limits*).\n",
        "\n",
        "**2. Respeitar a Janela de Contexto (Payload Capacity):**\n",
        "   Todo modelo de linguagem opera com uma \"janela de contexto\" finita ‚Äì a quantidade m√°xima de tokens que ele pode considerar em uma √∫nica requisi√ß√£o. Essa janela √© a mem√≥ria de trabalho da opera√ß√£o.\n",
        "   *   Enviar um `payload` (dados de entrada) que exceda essa capacidade resultar√° em erro.\n",
        "   *   Nosso contador √© uma ferramenta de valida√ß√£o pr√©via, garantindo que nossas requisi√ß√µes sejam sempre vi√°veis e estejam dentro das especifica√ß√µes do modelo.\n",
        "\n",
        "**3. Otimiza√ß√£o de Prompts e Lat√™ncia:**\n",
        "   A engenharia de prompts √© uma disciplina de otimiza√ß√£o. Um prompt eficiente transmite a instru√ß√£o m√°xima com o m√≠nimo de tokens.\n",
        "   *   **Sinal vs. Ru√≠do:** Tokens irrelevantes s√£o \"ru√≠do\" que pode degradar a qualidade da resposta e aumentar a lat√™ncia (tempo de resposta).\n",
        "   *   Ao monitorar a contagem, aprendemos a construir prompts mais concisos e potentes, melhorando tanto a performance da aplica√ß√£o quanto a precis√£o dos resultados.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LpaIjPfXsFMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Utilit√°rio de Desenvolvimento: Profiler de Custo para API Gemini"
      ],
      "metadata": {
        "id": "YyEVla-oe_5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class count_querys:\n",
        "    def __init__(self):\n",
        "        self.total_tokens = 0\n",
        "        print(\" Count de querys ativo!\")\n",
        "\n",
        "    def processar_query(self, llm):\n",
        "        \"\"\"Processa uma query com count autom√°tico e formata√ß√£o de sa√≠da.\"\"\"\n",
        "        query = input(\"Digite seu texto: \")\n",
        "\n",
        "        if not query.strip():\n",
        "            print(\"Nenhum texto inserido.\")\n",
        "            return\n",
        "\n",
        "        # Envia para o LLM\n",
        "        resposta = llm.invoke(query)\n",
        "\n",
        "        # Conta os tokens desta query\n",
        "        tokens_desta_msg = resposta.usage_metadata.get('total_tokens', 0) if resposta.usage_metadata else 0\n",
        "        self.total_tokens += tokens_desta_msg\n",
        "\n",
        "        # Formata a resposta para ter no m√°ximo 90 caracteres por linha\n",
        "        prefixo = \" Resposta: \"\n",
        "        largura_maxima = 90\n",
        "\n",
        "        # A fun√ß√£o fill quebra o texto em linhas do tamanho desejado\n",
        "        # e adiciona um recuo nas linhas seguintes para manter o alinhamento.\n",
        "        texto_formatado = textwrap.fill(\n",
        "            resposta.content,\n",
        "            width=largura_maxima,\n",
        "            initial_indent=prefixo,\n",
        "            subsequent_indent=' ' * len(prefixo) # Recuo para alinhar com o in√≠cio do texto\n",
        "        )\n",
        "\n",
        "        # Mostra a resposta da IA j√° formatada\n",
        "        print(f\"\\n{texto_formatado}\")\n",
        "\n",
        "        # Mostra o count no final\n",
        "        print(f\"\\n {tokens_desta_msg} tokens, total {self.total_tokens}\")\n",
        "# Crie o count uma vez\n",
        "count = count_querys()\n",
        "\n",
        "def query():\n",
        "    global count\n",
        "    if 'count' not in globals():\n",
        "        count = count_querys()\n",
        "    count.processar_query(llm)\n",
        "\n",
        "# Status do count\n",
        "def status():\n",
        "    \"\"\"Mostra o status atual dos tokens\"\"\"\n",
        "    global count\n",
        "    if 'count' not in globals():\n",
        "        print(\"‚ùå Nenhuma conversa iniciada ainda. Use query() primeiro!\")\n",
        "    else:\n",
        "        print(f\" Total de tokens usados: {count.total_tokens}\")\n",
        "\n",
        "def reset():\n",
        "    \"\"\"Reseta o count de tokens\"\"\"\n",
        "    global count\n",
        "    count = count_querys()\n",
        "    print(\" count resetado!\")\n",
        "\n",
        "def help_chat():\n",
        "  \"\"\"Mostra as utilidades do chat.\"\"\"\n",
        "  print(\"\"\"\n",
        "UTILIDADES:\n",
        "  query()     - Enviar mensagem\n",
        "  status()     - Ver total tokens\n",
        "  reset()      - Resetar count\n",
        "  help_chat()  - Esta ajuda\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "4iFcthA8sSbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query()"
      ],
      "metadata": {
        "id": "i0pcMzuSvI_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "status()"
      ],
      "metadata": {
        "id": "cTSPccFi2P7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset()\n",
        "print(status())"
      ],
      "metadata": {
        "id": "88TluarX2VLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help_chat()"
      ],
      "metadata": {
        "id": "KnGdvGJVfQuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Estruturando a Sa√≠da com Pydantic\n",
        "\n",
        "\n",
        "\n",
        "Para um programa de computador, processar varia√ß√µes  √© propenso a erros. √â importante uma estrutura de dados confi√°vel, n√£o um texto para interpretar.\n",
        "\n",
        "### **Pydantic como um \"Contrato\" de Dados**\n",
        "\n",
        "**Pydantic** √© uma biblioteca Python para **valida√ß√£o de dados**. Ela nos permite definir \"esquemas\" ou \"modelos\" de dados usando classes Python. Em outras palavras, criamos um **contrato** que define a estrutura e os tipos de dados que esperamos receber.\n",
        "\n",
        "O LangChain se integra perfeitamente com o Pydantic atrav√©s do **`PydanticOutputParser`**. Este \"parser\" (analisador) faz duas coisas m√°gicas:\n",
        "\n",
        "1.  **Instrui o LLM:** Ele analisa nosso modelo Pydantic e gera instru√ß√µes claras de formata√ß√£o que s√£o enviadas ao LLM junto com nosso prompt.\n",
        "2.  **Valida a Resposta:** Ele pega a resposta em texto do LLM, verifica se ela segue o \"contrato\", e a converte em um objeto Python real e validado.\n"
      ],
      "metadata": {
        "id": "dBUQWVC9VXt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1 Configura√ß√£o Inicial e Imports"
      ],
      "metadata": {
        "id": "YDBLQCSjauX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports de bibliotecas de tipagem e valida√ß√£o\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "# Imports principais do LangChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Configura√ß√£o da API Key (maneira segura para o Colab)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Garanta que a vari√°vel de ambiente est√° configurada\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "jkzugn8faleX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta c√©lula inicial √© respons√°vel por preparar nosso ambiente de trabalho.\n",
        ">\n",
        ">Importamos todas as bibliotecas necess√°rias, incluindo o Pydantic para definir nossa estrutura de dados e o LangChain para orquestrar a comunica√ß√£o com o modelo Gemini.\n",
        ">\n",
        ">Tamb√©m configuramos nossa chave de API do Google de forma segura usando os segredos do Colab."
      ],
      "metadata": {
        "id": "kgBVwcNia2vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 Definindo o \"C√©rebro\" do Agente (O Prompt Principal)"
      ],
      "metadata": {
        "id": "xbu1psTzbcMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Voc√™ √© um triador de Service Desk para pol√≠ticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usu√°rio, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas pol√≠ticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a pol√≠tica de alimenta√ß√£o em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informa√ß√µes para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma pol√≠tica\", \"Tenho uma d√∫vida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exce√ß√£o, libera√ß√£o, aprova√ß√£o ou acesso especial, ou quando o usu√°rio explicitamente pede para abrir um chamado (Ex: \"Quero exce√ß√£o para trabalhar 5 dias remoto.\", \"Solicito libera√ß√£o para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a a√ß√£o mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "jDfTiUi0bSv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Aqui definimos o SystemMessage, a personalidade e as diretrizes do nosso agente de IA.\n",
        ">\n",
        ">Este prompt √© o \"c√©rebro\" da opera√ß√£o, explicando ao modelo seu papel, as regras de neg√≥cio para cada tipo de decis√£o e o que esperamos como resultado."
      ],
      "metadata": {
        "id": "FVXg7gLzahXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3 Definindo o \"Contrato\" de Sa√≠da (O Esquema Pydantic)"
      ],
      "metadata": {
        "id": "4BOjY4SpZmw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriagemOut(BaseModel):\n",
        "    \"\"\"Define a estrutura de sa√≠da esperada para a triagem de chamados.\"\"\"\n",
        "\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"] = Field(\n",
        "        description=\"A decis√£o principal baseada na mensagem do usu√°rio.\"\n",
        "    )\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"] = Field(\n",
        "        description=\"A urg√™ncia estimada do chamado.\"\n",
        "    )\n",
        "    campos_faltantes: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Uma lista de informa√ß√µes que o usu√°rio precisa fornecer se a decis√£o for 'PEDIR_INFO'.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "NN8s9J-qZkcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta √© a parte central da valida√ß√£o. Usando Pydantic, criamos a classe TriagemOut que funciona como um \"contrato\".\n",
        ">\n",
        ">Ela for√ßa o LLM a gerar uma resposta que contenha exatamente os campos decisao, urgencia e campos_faltantes, com os tipos e valores que definimos. Se a IA gerar algo fora deste padr√£o, o LangChain acusar√° um erro."
      ],
      "metadata": {
        "id": "2WYvQEewabzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.4 C√©lula Principal - Construindo e Executando a L√≥gica de Triagem"
      ],
      "metadata": {
        "id": "miGW3hh5Z7lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inicializa o LLM com temperatura 0.0 para respostas mais determin√≠sticas\n",
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",  # Usando um modelo padr√£o e eficiente para essa tarefa\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# 2. Cria a \"chain\" que conecta o LLM ao nosso esquema de sa√≠da Pydantic\n",
        "# O m√©todo .with_structured_output() √© o que faz a m√°gica acontecer!\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "# 3. Define a fun√ß√£o que invoca a chain e formata a sa√≠da\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Invoca a chain de triagem com a mensagem do usu√°rio e retorna um dicion√°rio.\n",
        "    \"\"\"\n",
        "    # A chain recebe uma lista de mensagens (Sistema e Usu√°rio)\n",
        "    saida_pydantic: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    # .model_dump() converte o objeto Pydantic validado em um dicion√°rio Python\n",
        "    return saida_pydantic.model_dump()"
      ],
      "metadata": {
        "id": "wOzrV99IZurN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta √© a c√©lula principal onde a orquestra√ß√£o acontece. N√≥s:\n",
        "\n",
        "1)   Instanciamos o modelo Gemini, configurando a temperature para 0.0 para obter resultados mais consistentes e previs√≠veis.\n",
        "2)   Criamos a triagem_chain usando o poderoso m√©todo .with_structured_output(TriagemOut). √â este comando que \"ensina\" o LLM a seguir nosso esquema Pydantic.\n",
        "3)   Definimos a fun√ß√£o triagem(), que encapsula a l√≥gica de chamada, enviando o prompt do sistema e a mensagem do usu√°rio para a chain."
      ],
      "metadata": {
        "id": "AffYj2jhaPvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.5 Teste e Valida√ß√£o"
      ],
      "metadata": {
        "id": "K1rqrRCXaIWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como fa√ßo?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\",\n",
        "          \"Tenho uma d√∫vida sobre um benef√≠cio.\"]\n",
        "\n",
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: '{msg_teste}'\")\n",
        "    resultado = triagem(msg_teste)\n",
        "    print(f\" -> Resposta Estruturada: {resultado}\\n\")"
      ],
      "metadata": {
        "id": "SdIrC1t8aGN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.6 **Teste e Valida√ß√£o(Com Controle de Taxa)**\n",
        "\n",
        "> **Nota Importante sobre Limites de API (Rate Limiting)**\n",
        ">\n",
        "> Ao executar o loop `for` sem pausas, enviamos todas as nossas perguntas √† API quase simultaneamente. Isso aciona um mecanismo de prote√ß√£o da API do Google chamado **rate limiting**, resultando no erro `ResourceExhausted: 429`. Essencialmente, a API nos diz: \"Por favor, v√° com mais calma!\".\n",
        ">\n",
        "> Para resolver isso, introduzimos o comando **`time.sleep(10)`** dentro do loop. Este comando pausa a execu√ß√£o por 10 segundos entre cada chamada, garantindo que respeitamos os limites da camada gratuita (ex: 2-5 requisi√ß√µes por minuto).\n",
        ">\n",
        "> Gerenciar a frequ√™ncia das chamadas √© uma **pr√°tica fundamental** no desenvolvimento com qualquer servi√ßo externo, garantindo que nossa aplica√ß√£o seja robusta e n√£o seja bloqueada."
      ],
      "metadata": {
        "id": "5BfPDj8gdyzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos a biblioteca 'time' para adicionar pausas\n",
        "import time\n",
        "\n",
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como fa√ßo?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\",\n",
        "          \"Tenho uma d√∫vida sobre um benef√≠cio.\"]\n",
        "\n",
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: '{msg_teste}'\")\n",
        "    resultado = triagem(msg_teste)\n",
        "    print(f\" -> Resposta Estruturada: {resultado}\\n\")\n",
        "\n",
        "    # --- AJUSTE IMPORTANTE AQUI ---\n",
        "    # Adiciona uma pausa de 10 segundos para n√£o exceder o limite da API.\n",
        "    # Para o free tier, um valor entre 10 e 20 segundos √© seguro.\n",
        "    print(\"...aguardando 10 segundos para a pr√≥xima requisi√ß√£o...\\n\")\n",
        "    time.sleep(10)\n",
        "\n",
        "print(\"‚úÖ Todos os testes foram conclu√≠dos.\")"
      ],
      "metadata": {
        "id": "R7n-VdWyc1KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Com tudo configurado, esta c√©lula executa um conjunto de testes para validar nosso sistema. Observamos como ele classifica diferentes tipos de perguntas, demonstrando a efic√°cia do prompt e da estrutura√ß√£o de sa√≠da com Pydantic."
      ],
      "metadata": {
        "id": "ygBixDwUaMTz"
      }
    }
  ]
}