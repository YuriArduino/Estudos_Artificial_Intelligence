{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1N/qKLCMJSKwh1Sqp3T4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriArduino/Estudos_Artificial_Intelligence/blob/Imers%C3%A3o-Agentes-de-IA---Alura/Imers%C3%A3o_Dev_Agentes_IA_Google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Prepararando o ambiente"
      ],
      "metadata": {
        "id": "YLGLpS9SiwDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Dependências"
      ],
      "metadata": {
        "id": "Fzn5Ri6gio40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NX0_N1ahdF1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2e2d75-cd99-41a6-b586-8e4649c805af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Imports"
      ],
      "metadata": {
        "id": "B3Wa9fsVigNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "import sys\n",
        "import textwrap\n",
        "import time"
      ],
      "metadata": {
        "id": "ZFsJh7ZFfRTl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Conexão com o modelo"
      ],
      "metadata": {
        "id": "3iYwtIlBi4XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=1.0, # Maior mais criativo, Menor mais objetivo\n",
        "    )"
      ],
      "metadata": {
        "id": "mUwH8RWxg9-f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Para saber mais: Diferença **google-generativeai** X **langchain_google_genai**\n",
        "\n",
        "\n",
        "*   **`google-generativeai` (a biblioteca nativa do Google)**: É como o **motor de um carro**. É a peça de engenharia principal, super poderosa, que faz o trabalho fundamental (gerar texto, contar tokens, etc.). Ela te dá acesso direto e total a todas as funcionalidades específicas daquele motor (o Gemini).\n",
        "\n",
        "*   **LangChain (a biblioteca de orquestração)**: É como o **carro completo construído ao redor do motor**. Ele usa o motor (Gemini), mas adiciona o chassi, o volante, os pedais, o painel e os assentos. Ele torna o motor mais fácil de usar para um propósito maior (dirigir de um ponto A para um ponto B) e o conecta com outras partes (rodas, sistema de som, GPS).\n",
        "\n",
        "Vamos detalhar as diferenças práticas:\n",
        "\n",
        "---\n",
        "\n",
        "### Tabela Comparativa\n",
        "\n",
        "| Característica | `google-generativeai` (Nativa) | `langchain_google_genai` (Via LangChain) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Propósito Principal** | Acesso direto e completo à API do Gemini. | Construir aplicações complexas com LLMs, orquestrando várias etapas. |\n",
        "| **Nível de Abstração** | **Baixo.** Você interage diretamente com os conceitos da API do Google. | **Alto.** LangChain cria uma \"camada de compatibilidade\" sobre vários modelos. |\n",
        "| **Funcionalidades** | Geração de conteúdo, contagem de tokens, embeddings, ajuste de segurança, etc. | **Tudo da nativa, e mais:** Chains, Agentes, Memória, RAG, etc. |\n",
        "| **Flexibilidade de Modelo** | Feito exclusivamente para os modelos da família Gemini. | **Multi-modelo.** O código que você escreve pode ser facilmente adaptado para usar o GPT-4, Claude, etc. |\n",
        "| **Facilidade (Tarefas Simples)** | Geralmente mais simples para uma única chamada de API. | Envolve um pouco mais de \"boilerplate\" (código de configuração) inicial. |\n",
        "| **Facilidade (Tarefas Complexas)**| Você precisa construir toda a lógica (ex: memória de chat) do zero. | **Muito mais fácil.** Já fornece componentes prontos para tarefas complexas. |\n",
        "\n",
        "---\n",
        "\n",
        "### O que isso significa na prática?\n",
        "\n",
        "#### 1. Abstração e Portabilidade\n",
        "\n",
        "Com LangChain, o objeto `llm` que você criou é padronizado. Se amanhã você quisesse testar o modelo da OpenAI, você mudaria poucas linhas:\n",
        "\n",
        "```python\n",
        "# Com Google\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "# Com OpenAI (exemplo)\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# O resto do seu código LangChain (chains, agentes) continuaria funcionando igual!\n",
        "llm.invoke(\"Me conte uma piada.\")\n",
        "```\n",
        "\n",
        "Se você usasse a biblioteca nativa, teria que reescrever todo o código de interação com o modelo.\n",
        "\n",
        "#### 2. O Poder do Ecossistema LangChain\n",
        "\n",
        "A verdadeira magia de usar LangChain, e o motivo pelo qual é ensinado na imersão, não é apenas para fazer uma pergunta ao modelo. É para construir sistemas mais complexos:\n",
        "\n",
        "*   **Chains (Correntes):** Você pode criar sequências de tarefas. Por exemplo: \"Passo 1: Pegue a pergunta do usuário. Passo 2: Traduza para o inglês. Passo 3: Envie ao Gemini. Passo 4: Pegue a resposta e traduza de volta para o português.\"\n",
        "*   **Agentes (Agents):** Você pode dar ferramentas ao LLM. Por exemplo, dar a ele uma ferramenta de busca no Google. Se você perguntar \"Qual a previsão do tempo para amanhã?\", o agente pode decidir usar a ferramenta de busca, pegar o resultado e só então usar o Gemini para te dar uma resposta em linguagem natural.\n",
        "*   **RAG (Retrieval-Augmented Generation):** É a técnica mais popular. Você pode fazer o Gemini \"conversar com seus documentos\". Você fornece uma base de dados (PDFs, sites, etc.), e o LangChain cuida de buscar a informação relevante nesses documentos para que o Gemini possa responder perguntas sobre eles.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "Para a imersão, ficar com **LangChain é a decisão certa**. Você não está apenas aprendendo a usar o Gemini, mas sim a **como construir aplicações robustas em torno de um LLM**, que é a habilidade mais valiosa no mercado hoje.\n",
        "\n",
        "A contagem de tokens ser `llm.get_num_tokens()` em vez de `model.count_tokens()` é um pequeno sintoma dessa camada de abstração que o LangChain adiciona para tornar tudo mais padronizado e poderoso."
      ],
      "metadata": {
        "id": "k9qT4aVUo8is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Content\n"
      ],
      "metadata": {
        "id": "hkmDX-uk-OQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "O resultado de `llm.invoke()` não é um texto puro, mas sim um **objeto**. No LangChain, esse objeto é chamado de `AIMessage`.\n",
        "\n",
        "Este objeto `AIMessage` funciona como um \"envelope\" que contém várias informações sobre a resposta do modelo:\n",
        "\n",
        "1.  **.content**: O conteúdo principal, ou seja, o texto da resposta que você queria. É o que está *dentro* do envelope.\n",
        "2.  **.response_metadata**: Informações extras sobre a execução, como o motivo pelo qual o modelo parou de gerar texto (`finish_reason`) e as classificações de segurança (`safety_ratings`).\n",
        "3.  **.usage_metadata**: Dados sobre o consumo de tokens na chamada (quantos tokens na entrada, quantos na saída e o total).\n",
        "4.  **.id**: Um identificador único para aquela execução específica, útil para depuração.\n",
        "\n",
        "\n",
        "### Comparando com o `StrOutputParser`\n",
        "\n",
        "Agora você entende perfeitamente a diferença:\n",
        "\n",
        "*   **Usar `.content`**: É a forma manual e direta de extrair o texto de uma **única** chamada. É perfeito para testes rápidos e quando você não está construindo uma sequência complexa de passos.\n",
        "\n",
        "*   **Usar `| StrOutputParser()`**: É a forma automática de fazer a mesma coisa, mas de um jeito que se integra com o sistema de \"correntes\" (chains) do LangChain. Ele já \"desempacota\" o conteúdo para você e o entrega pronto para o próximo passo da sua chain.\n"
      ],
      "metadata": {
        "id": "pDtdiqoNlEX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Estamos Online?\")\n",
        "print(resp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GvY8H7lKqk",
        "outputId": "b02dfff2-649d-42ba-8570-b7ae3ab287ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='¡Sí! Estoy en línea y listo para ayudarte.\\n\\n¿En qué puedo asistirte hoy?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--8c3f49b7-c9d3-47b9-aad3-0de5ee8dcf4a-0' usage_metadata={'input_tokens': 4, 'output_tokens': 20, 'total_tokens': 751, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#.content organiza o texto\n",
        "resp_test = llm.invoke(\"Estamos Online?\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-17dGtnjAp1",
        "outputId": "cd0dafa3-44ab-4d96-a6ac-e5142700dca7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sim, estamos online! Se você está lendo esta resposta, significa que a conexão está funcionando perfeitamente. 😊\n",
            "\n",
            "Como posso te ajudar hoje?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Tokens\n",
        "\n",
        "Para um modelo de linguagem como o Gemini, os **tokens são os blocos de construção do texto**.\n",
        "\n",
        "Um token não é exatamente uma palavra, uma letra ou uma sílaba. É a unidade fundamental que o modelo usa para processar e entender a linguagem.\n",
        "\n",
        "*   Uma palavra curta como `\"Eu\"` pode ser **1 token**.\n",
        "*   Uma palavra comum como `\"gato\"` pode ser **1 token**.\n",
        "*   Uma palavra mais longa ou incomum como `\"inteligência\"` pode ser dividida em **2 ou mais tokens** (ex: `inteli` + `gência`).\n",
        "*   Pontuação e espaços também contam e podem ser tokens.\n",
        "\n",
        "**Exemplo Rápido:**\n",
        "A frase `Olá, mundo!` é quebrada pelo modelo em algo como: `[\"Olá\", \",\", \" mundo\", \"!\"]` - resultando em **4 tokens**.\n",
        "\n",
        "Em resumo, para o modelo, toda a nossa conversa (tanto o que enviamos quanto o que ele responde) é uma sequência de tokens.\n",
        "\n",
        "### A Importância de um Contador de Tokens na nossa Imersão\n",
        "\n",
        "No desenvolvimento de software, todo recurso é finito. Contar tokens é uma prática de **gerenciamento de recursos computacionais**. Ignorar essa métrica é como escrever um código que consome memória sem controle. A ferramenta `count_querys` que construímos é crucial por três razões técnicas:\n",
        "\n",
        "**1. Gerenciamento de Custos e Carga de API:**\n",
        "   Cada token processado representa uma unidade de computação que tem um custo. Em um ambiente de produção, esse custo é financeiro. Na Imersão, ele se reflete no consumo da nossa cota gratuita. O contador funciona como um *profiler*, permitindo-nos:\n",
        "   *   Medir a \"carga\" exata que cada chamada impõe à API.\n",
        "   *   Otimizar a comunicação para ser mais eficiente e evitar exceder os limites de requisições por minuto (*rate limits*).\n",
        "\n",
        "**2. Respeitar a Janela de Contexto (Payload Capacity):**\n",
        "   Todo modelo de linguagem opera com uma \"janela de contexto\" finita – a quantidade máxima de tokens que ele pode considerar em uma única requisição. Essa janela é a memória de trabalho da operação.\n",
        "   *   Enviar um `payload` (dados de entrada) que exceda essa capacidade resultará em erro.\n",
        "   *   Nosso contador é uma ferramenta de validação prévia, garantindo que nossas requisições sejam sempre viáveis e estejam dentro das especificações do modelo.\n",
        "\n",
        "**3. Otimização de Prompts e Latência:**\n",
        "   A engenharia de prompts é uma disciplina de otimização. Um prompt eficiente transmite a instrução máxima com o mínimo de tokens.\n",
        "   *   **Sinal vs. Ruído:** Tokens irrelevantes são \"ruído\" que pode degradar a qualidade da resposta e aumentar a latência (tempo de resposta).\n",
        "   *   Ao monitorar a contagem, aprendemos a construir prompts mais concisos e potentes, melhorando tanto a performance da aplicação quanto a precisão dos resultados.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LpaIjPfXsFMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Utilitário de Desenvolvimento: Profiler de Custo para API Gemini"
      ],
      "metadata": {
        "id": "YyEVla-oe_5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class count_querys:\n",
        "    def __init__(self):\n",
        "        self.total_tokens = 0\n",
        "        print(\" Count de querys ativo!\")\n",
        "\n",
        "    def processar_query(self, llm):\n",
        "        \"\"\"Processa uma query com count automático e formatação de saída.\"\"\"\n",
        "        query = input(\"Digite seu texto: \")\n",
        "\n",
        "        if not query.strip():\n",
        "            print(\"Nenhum texto inserido.\")\n",
        "            return\n",
        "\n",
        "        # Envia para o LLM\n",
        "        resposta = llm.invoke(query)\n",
        "\n",
        "        # Conta os tokens desta query\n",
        "        tokens_desta_msg = resposta.usage_metadata.get('total_tokens', 0) if resposta.usage_metadata else 0\n",
        "        self.total_tokens += tokens_desta_msg\n",
        "\n",
        "        # Formata a resposta para ter no máximo 90 caracteres por linha\n",
        "        prefixo = \" Resposta: \"\n",
        "        largura_maxima = 90\n",
        "\n",
        "        # A função fill quebra o texto em linhas do tamanho desejado\n",
        "        # e adiciona um recuo nas linhas seguintes para manter o alinhamento.\n",
        "        texto_formatado = textwrap.fill(\n",
        "            resposta.content,\n",
        "            width=largura_maxima,\n",
        "            initial_indent=prefixo,\n",
        "            subsequent_indent=' ' * len(prefixo) # Recuo para alinhar com o início do texto\n",
        "        )\n",
        "\n",
        "        # Mostra a resposta da IA já formatada\n",
        "        print(f\"\\n{texto_formatado}\")\n",
        "\n",
        "        # Mostra o count no final\n",
        "        print(f\"\\n {tokens_desta_msg} tokens, total {self.total_tokens}\")\n",
        "# Crie o count uma vez\n",
        "count = count_querys()\n",
        "\n",
        "def query():\n",
        "    global count\n",
        "    if 'count' not in globals():\n",
        "        count = count_querys()\n",
        "    count.processar_query(llm)\n",
        "\n",
        "# Status do count\n",
        "def status():\n",
        "    \"\"\"Mostra o status atual dos tokens\"\"\"\n",
        "    global count\n",
        "    if 'count' not in globals():\n",
        "        print(\"❌ Nenhuma conversa iniciada ainda. Use query() primeiro!\")\n",
        "    else:\n",
        "        print(f\" Total de tokens usados: {count.total_tokens}\")\n",
        "\n",
        "def reset():\n",
        "    \"\"\"Reseta o count de tokens\"\"\"\n",
        "    global count\n",
        "    count = count_querys()\n",
        "    print(\" count resetado!\")\n",
        "\n",
        "def help_chat():\n",
        "  \"\"\"Mostra as utilidades do chat.\"\"\"\n",
        "  print(\"\"\"\n",
        "UTILIDADES:\n",
        "  query()     - Enviar mensagem\n",
        "  status()     - Ver total tokens\n",
        "  reset()      - Resetar count\n",
        "  help_chat()  - Esta ajuda\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "4iFcthA8sSbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query()"
      ],
      "metadata": {
        "id": "i0pcMzuSvI_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "status()"
      ],
      "metadata": {
        "id": "cTSPccFi2P7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset()\n",
        "print(status())"
      ],
      "metadata": {
        "id": "88TluarX2VLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help_chat()"
      ],
      "metadata": {
        "id": "KnGdvGJVfQuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Estruturando a Saída com Pydantic\n",
        "\n",
        "\n",
        "\n",
        "Para um programa de computador, processar variações  é propenso a erros. É importante uma estrutura de dados confiável, não um texto para interpretar.\n",
        "\n",
        "### **Pydantic como um \"Contrato\" de Dados**\n",
        "\n",
        "**Pydantic** é uma biblioteca Python para **validação de dados**. Ela nos permite definir \"esquemas\" ou \"modelos\" de dados usando classes Python. Em outras palavras, criamos um **contrato** que define a estrutura e os tipos de dados que esperamos receber.\n",
        "\n",
        "O LangChain se integra perfeitamente com o Pydantic através do **`PydanticOutputParser`**. Este \"parser\" (analisador) faz duas coisas mágicas:\n",
        "\n",
        "1.  **Instrui o LLM:** Ele analisa nosso modelo Pydantic e gera instruções claras de formatação que são enviadas ao LLM junto com nosso prompt.\n",
        "2.  **Valida a Resposta:** Ele pega a resposta em texto do LLM, verifica se ela segue o \"contrato\", e a converte em um objeto Python real e validado.\n"
      ],
      "metadata": {
        "id": "dBUQWVC9VXt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1 Configuração Inicial e Imports"
      ],
      "metadata": {
        "id": "YDBLQCSjauX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports de bibliotecas de tipagem e validação\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "# Imports principais do LangChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Configuração da API Key (maneira segura para o Colab)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Garanta que a variável de ambiente está configurada\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "jkzugn8faleX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta célula inicial é responsável por preparar nosso ambiente de trabalho.\n",
        ">\n",
        ">Importamos todas as bibliotecas necessárias, incluindo o Pydantic para definir nossa estrutura de dados e o LangChain para orquestrar a comunicação com o modelo Gemini.\n",
        ">\n",
        ">Também configuramos nossa chave de API do Google de forma segura usando os segredos do Colab."
      ],
      "metadata": {
        "id": "kgBVwcNia2vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 Definindo o \"Cérebro\" do Agente (O Prompt Principal)"
      ],
      "metadata": {
        "id": "xbu1psTzbcMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "jDfTiUi0bSv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Aqui definimos o SystemMessage, a personalidade e as diretrizes do nosso agente de IA.\n",
        ">\n",
        ">Este prompt é o \"cérebro\" da operação, explicando ao modelo seu papel, as regras de negócio para cada tipo de decisão e o que esperamos como resultado."
      ],
      "metadata": {
        "id": "FVXg7gLzahXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3 Definindo o \"Contrato\" de Saída (O Esquema Pydantic)"
      ],
      "metadata": {
        "id": "4BOjY4SpZmw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriagemOut(BaseModel):\n",
        "    \"\"\"Define a estrutura de saída esperada para a triagem de chamados.\"\"\"\n",
        "\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"] = Field(\n",
        "        description=\"A decisão principal baseada na mensagem do usuário.\"\n",
        "    )\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"] = Field(\n",
        "        description=\"A urgência estimada do chamado.\"\n",
        "    )\n",
        "    campos_faltantes: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Uma lista de informações que o usuário precisa fornecer se a decisão for 'PEDIR_INFO'.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "NN8s9J-qZkcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta é a parte central da validação. Usando Pydantic, criamos a classe TriagemOut que funciona como um \"contrato\".\n",
        ">\n",
        ">Ela força o LLM a gerar uma resposta que contenha exatamente os campos decisao, urgencia e campos_faltantes, com os tipos e valores que definimos. Se a IA gerar algo fora deste padrão, o LangChain acusará um erro."
      ],
      "metadata": {
        "id": "2WYvQEewabzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.4 Célula Principal - Construindo e Executando a Lógica de Triagem"
      ],
      "metadata": {
        "id": "miGW3hh5Z7lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inicializa o LLM com temperatura 0.0 para respostas mais determinísticas\n",
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",  # Usando um modelo padrão e eficiente para essa tarefa\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# 2. Cria a \"chain\" que conecta o LLM ao nosso esquema de saída Pydantic\n",
        "# O método .with_structured_output() é o que faz a mágica acontecer!\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "# 3. Define a função que invoca a chain e formata a saída\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Invoca a chain de triagem com a mensagem do usuário e retorna um dicionário.\n",
        "    \"\"\"\n",
        "    # A chain recebe uma lista de mensagens (Sistema e Usuário)\n",
        "    saida_pydantic: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    # .model_dump() converte o objeto Pydantic validado em um dicionário Python\n",
        "    return saida_pydantic.model_dump()"
      ],
      "metadata": {
        "id": "wOzrV99IZurN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Esta é a célula principal onde a orquestração acontece. Nós:\n",
        "\n",
        "1)   Instanciamos o modelo Gemini, configurando a temperature para 0.0 para obter resultados mais consistentes e previsíveis.\n",
        "2)   Criamos a triagem_chain usando o poderoso método .with_structured_output(TriagemOut). É este comando que \"ensina\" o LLM a seguir nosso esquema Pydantic.\n",
        "3)   Definimos a função triagem(), que encapsula a lógica de chamada, enviando o prompt do sistema e a mensagem do usuário para a chain."
      ],
      "metadata": {
        "id": "AffYj2jhaPvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.5 Teste e Validação"
      ],
      "metadata": {
        "id": "K1rqrRCXaIWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\",\n",
        "          \"Tenho uma dúvida sobre um benefício.\"]\n",
        "\n",
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: '{msg_teste}'\")\n",
        "    resultado = triagem(msg_teste)\n",
        "    print(f\" -> Resposta Estruturada: {resultado}\\n\")"
      ],
      "metadata": {
        "id": "SdIrC1t8aGN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.6 **Teste e Validação(Com Controle de Taxa)**\n",
        "\n",
        "> **Nota Importante sobre Limites de API (Rate Limiting)**\n",
        ">\n",
        "> Ao executar o loop `for` sem pausas, enviamos todas as nossas perguntas à API quase simultaneamente. Isso aciona um mecanismo de proteção da API do Google chamado **rate limiting**, resultando no erro `ResourceExhausted: 429`. Essencialmente, a API nos diz: \"Por favor, vá com mais calma!\".\n",
        ">\n",
        "> Para resolver isso, introduzimos o comando **`time.sleep(10)`** dentro do loop. Este comando pausa a execução por 10 segundos entre cada chamada, garantindo que respeitamos os limites da camada gratuita (ex: 2-5 requisições por minuto).\n",
        ">\n",
        "> Gerenciar a frequência das chamadas é uma **prática fundamental** no desenvolvimento com qualquer serviço externo, garantindo que nossa aplicação seja robusta e não seja bloqueada."
      ],
      "metadata": {
        "id": "5BfPDj8gdyzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos a biblioteca 'time' para adicionar pausas\n",
        "import time\n",
        "\n",
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\",\n",
        "          \"Tenho uma dúvida sobre um benefício.\"]\n",
        "\n",
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: '{msg_teste}'\")\n",
        "    resultado = triagem(msg_teste)\n",
        "    print(f\" -> Resposta Estruturada: {resultado}\\n\")\n",
        "\n",
        "    # --- AJUSTE IMPORTANTE AQUI ---\n",
        "    # Adiciona uma pausa de 10 segundos para não exceder o limite da API.\n",
        "    # Para o free tier, um valor entre 10 e 20 segundos é seguro.\n",
        "    print(\"...aguardando 10 segundos para a próxima requisição...\\n\")\n",
        "    time.sleep(10)\n",
        "\n",
        "print(\"✅ Todos os testes foram concluídos.\")"
      ],
      "metadata": {
        "id": "R7n-VdWyc1KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Com tudo configurado, esta célula executa um conjunto de testes para validar nosso sistema. Observamos como ele classifica diferentes tipos de perguntas, demonstrando a eficácia do prompt e da estruturação de saída com Pydantic."
      ],
      "metadata": {
        "id": "ygBixDwUaMTz"
      }
    }
  ]
}