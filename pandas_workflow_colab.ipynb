{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas Workflow com LlamaIndex Workflows\n",
        "\n",
        ">Este notebook demonstra como criar um sistema de an√°lise de dados usando LlamaIndex, Pandas e Groq para consultas em linguagem natural.\n",
        "\n",
        "\n",
        "## Sobre a Atualiza√ß√£o do LlamaIndex\n",
        "\n",
        "O **LlamaIndex** descontinuou o m√≥dulo **QueryPipeline** em favor de uma nova abordagem chamada **Workflows**.\n",
        "Essa mudan√ßa foi implementada na vers√£o `0.11` do LlamaIndex, com o objetivo de oferecer uma arquitetura mais\n",
        "flex√≠vel e escal√°vel para a constru√ß√£o de aplica√ß√µes de IA generativa.\n",
        "\n",
        "### Por que o QueryPipeline foi descontinuado?\n",
        "O QueryPipeline era uma API declarativa √∫til para orquestrar consultas simples a avan√ßadas sobre os dados.\n",
        "Por√©m, ele se mostrou limitado para cen√°rios mais din√¢micos. Por isso, os **Workflows** foram introduzidos,\n",
        "trazendo uma arquitetura orientada a eventos para fluxos de trabalho mais sofisticados.\n",
        "\n",
        "### O que s√£o Workflows?\n",
        "Workflows permitem orquestrar etapas personalizadas de forma ass√≠ncrona e condicional, facilitando\n",
        "integra√ß√µes complexas e manipula√ß√£o de dados em tempo real.\n",
        "\n",
        "**Documenta√ß√£o oficial:** [docs.llamaindex.ai](https://docs.llamaindex.ai)\n",
        "\n"
      ],
      "metadata": {
        "id": "3pzznnjN3W-Z"
      },
      "id": "3pzznnjN3W-Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C√≥digo:\n"
      ],
      "metadata": {
        "id": "KYxQbJBGAjb7"
      },
      "id": "KYxQbJBGAjb7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 Instalar Depend√™ncias:"
      ],
      "metadata": {
        "id": "ne3hDRwyAlhT"
      },
      "id": "ne3hDRwyAlhT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eaa06244",
      "metadata": {
        "id": "eaa06244"
      },
      "outputs": [],
      "source": [
        "# Instalar depend√™ncias (-q para instala√ß√£o silenciosa)\n",
        "!pip install -q jedi>=0.16 llama-index llama-index-llms-openai llama-index-llms-groq llama-index-experimental gradio fpdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bbf94cd",
      "metadata": {
        "id": "8bbf94cd"
      },
      "source": [
        "##1.1 Para uso no Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "MnIk3zEA-Vaw"
      },
      "id": "MnIk3zEA-Vaw",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 Imports"
      ],
      "metadata": {
        "id": "TfVHgRx83GYA"
      },
      "id": "TfVHgRx83GYA"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "327f6a9c",
      "metadata": {
        "id": "327f6a9c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import textwrap\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict, ValidationError\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core.workflow import Workflow, Event, StartEvent as BaseStartEvent, StopEvent, step\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "from typing import Any, List, Tuple, Optional\n",
        "import asyncio\n",
        "import matplotlib.font_manager as fm\n",
        "import gradio as gr\n",
        "from fpdf import FPDF\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 Instru√ß√µes e Prompts\n",
        "\n",
        "(Importante para validar a sa√≠da, pode ser usado como debug ou  )"
      ],
      "metadata": {
        "id": "3YY-0c2l-i5M"
      },
      "id": "3YY-0c2l-i5M"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== INSTRU√á√ïES PARA CONVERS√ÉO DE CONSULTAS =====\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para c√≥digo Python execut√°vel usando Pandas.\\n\"\n",
        "    \"2. A linha final do c√≥digo deve ser uma express√£o Python que possa ser chamada com a fun√ß√£o `eval()`.\\n\"\n",
        "    \"3. O c√≥digo deve representar uma solu√ß√£o para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESS√ÉO FINAL.\\n\"\n",
        "    \"5. N√£o coloque a express√£o entre aspas.\\n\"\n",
        "    \"6. Evite atribui√ß√µes (=) na linha final - prefira express√µes que retornem valores.\\n\"\n",
        "    \"7. Para opera√ß√µes de m√∫ltiplas linhas, termine com uma express√£o que retorne o resultado.\\n\"\n",
        "    \"8. Exemplos v√°lidos:\\n\"\n",
        "    \"   - df.groupby('coluna')['valor'].sum()\\n\"\n",
        "    \"   - df['coluna'].value_counts().head(5)\\n\"\n",
        "    \"   - df.describe()\\n\"\n",
        "    \"9. Evite c√≥digos como 'df['coluna'] = valor' - prefira consultas que retornem dados.\\n\"\n",
        ")\n",
        "\n",
        "# ===== PROMPTS =====\n",
        "pandas_prompt_str = (\n",
        "    \"Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este √© o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instru√ß√µes:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Express√£o:\"\n",
        ")\n",
        "\n",
        "RESPONSE_SYNTHESIS_PROMPT_STR = (\n",
        "   \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "   \"Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante.\\n\"\n",
        "   \"Consulta: {query_str}\\n\\n\"\n",
        "   \"Instru√ß√µes do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "   \"Sa√≠da do Pandas: {pandas_output}\\n\\n\"\n",
        "   \"Resposta: \"\n",
        "   \"Ao final, exibir o c√≥digo usado para gerar a resposta, no formato: O c√≥digo utilizado foi {pandas_instructions}\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Prompts configurados com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElcLXNqi-m2w",
        "outputId": "a9f9704f-2e73-4fa8-d20a-8edcac72b1eb"
      },
      "id": "ElcLXNqi-m2w",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prompts configurados com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4 Configura√ß√£o Pydantic V2"
      ],
      "metadata": {
        "id": "eaOg-rIf3jaL"
      },
      "id": "eaOg-rIf3jaL"
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMConfig(BaseModel):\n",
        "    model: str = Field(..., description=\"Nome do modelo Groq a ser usado\")\n",
        "    api_key: str = Field(..., description=\"Chave da API Groq\")\n",
        "    data_url: str = Field(..., description=\"URL do CSV com os dados\")\n",
        "\n",
        "    @field_validator(\"data_url\")\n",
        "    @classmethod\n",
        "    def validar_url(cls, v: str) -> str:\n",
        "        if not (v.startswith(\"http://\") or v.startswith(\"https://\")):\n",
        "            raise ValueError(\"data_url deve come√ßar com http:// ou https://\")\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"api_key\")\n",
        "    @classmethod\n",
        "    def validar_api_key(cls, v: str) -> str:\n",
        "        if not v or len(v.strip()) == 0:\n",
        "            raise ValueError(\"api_key n√£o pode ser vazia\")\n",
        "        return v\n",
        "\n",
        "print(\"‚úÖ Configura√ß√£o LLM com Pydantic criada!\")\n",
        "\n",
        "class Historico(BaseModel):\n",
        "    entradas: List[Tuple[str, str]] = []\n",
        "\n",
        "    def adicionar(self, pergunta: str, resposta: str):\n",
        "        \"\"\"Adiciona uma entrada ao hist√≥rico se pergunta e resposta forem v√°lidas.\"\"\"\n",
        "        if pergunta and resposta:\n",
        "            self.entradas.append((pergunta, resposta))\n",
        "\n",
        "print(\"‚úÖ Modelo de hist√≥rico com Pydantic criado!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf65iXIp3riw",
        "outputId": "c07a8bbb-cbca-41fa-a5f6-9ce3ad1aa6a7"
      },
      "id": "zf65iXIp3riw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configura√ß√£o LLM com Pydantic criada!\n",
            "‚úÖ Modelo de hist√≥rico com Pydantic criado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5 Modelos de Eventos\n",
        "\n",
        "(Rastrear cada passo da intera√ß√£o, como um hist√≥rico organizado)"
      ],
      "metadata": {
        "id": "lHeWjLLu-tjv"
      },
      "id": "lHeWjLLu-tjv"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos de Eventos com Pydantic ---\n",
        "\n",
        "# --- Evento de in√≠cio customizado para garantir os dados de entrada ---\n",
        "class StartEvent(BaseStartEvent):\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "# --- Eventos intermedi√°rios para propagar os dados no workflow ---\n",
        "class CodeEvent(Event):\n",
        "    pandas_prompt: str\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "class OutputEvent(Event):\n",
        "    pandas_code: str\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "class ExecutedEvent(Event):\n",
        "    pandas_code: str\n",
        "    pandas_output: Any\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "print(\"‚úÖ Modelos de eventos do workflow definidos com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJvzUYbg-zP0",
        "outputId": "822860b0-9605-4a50-de1b-8f302b8ad9d5"
      },
      "id": "MJvzUYbg-zP0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelos de eventos do workflow definidos com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6 Fun√ß√µes Auxiliares"
      ],
      "metadata": {
        "id": "zoVRLbkg38zx"
      },
      "id": "zoVRLbkg38zx"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2ecab43e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ecab43e",
        "outputId": "62e7c55b-f1e8-4bf9-9599-394fefda396b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fun√ß√µes auxiliares configuradas!\n"
          ]
        }
      ],
      "source": [
        "def carregar_arquivo(caminho_arquivo: str, df_estado: Optional[pd.DataFrame] = None) -> Tuple[str, Optional[pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Carrega um CSV do caminho fornecido. Se caminho_arquivo for None, mant√©m df_estado.\n",
        "    Retorna mensagem de status e DataFrame atualizado.\n",
        "    \"\"\"\n",
        "    if not caminho_arquivo:\n",
        "        return \"Nenhum arquivo enviado. Usando dados atuais (se houver).\", df_estado\n",
        "    try:\n",
        "        df_novo = pd.read_csv(caminho_arquivo)\n",
        "        return \"Arquivo carregado com sucesso!\", df_novo\n",
        "    except Exception as e:\n",
        "        return f\"Erro ao carregar arquivo: {str(e)}\", df_estado\n",
        "\n",
        "def formatar_texto(response: str, largura: int = 100, imprimir: bool = True):\n",
        "    texto_formatado = textwrap.fill(response, width=largura)\n",
        "    if imprimir:\n",
        "        print(texto_formatado)\n",
        "    return texto_formatado\n",
        "\n",
        "def descricao_colunas(df: pd.DataFrame) -> str:\n",
        "    descricao = \"\\n\".join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return \"Colunas do DataFrame:\\n\" + descricao\n",
        "\n",
        "def limpar_codigo_pandas(codigo: str) -> str:\n",
        "    \"\"\"Limpa e valida o c√≥digo Pandas gerado pela LLM.\"\"\"\n",
        "    codigo = re.sub(r'```(?:python)?\\n?', '', codigo) # Remove ```python\n",
        "    codigo = re.sub(r'```', '', codigo) # Remove ``` de fechamento\n",
        "    linhas = [linha.strip() for linha in codigo.split('\\n') if linha.strip()]\n",
        "\n",
        "    # Filtra linhas que s√£o coment√°rios ou texto explicativo\n",
        "    codigo_filtrado = [\n",
        "        linha for linha in linhas if not (\n",
        "            linha.startswith('#') or\n",
        "            'resposta:' in linha.lower() or\n",
        "            'resultado:' in linha.lower()\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if not codigo_filtrado:\n",
        "        return \"\"\n",
        "\n",
        "    # Se houver v√°rias linhas, a √∫ltima √© geralmente a express√£o de retorno\n",
        "    return codigo_filtrado[-1]\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes auxiliares configuradas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7 Configura√ß√£o da API e Dados"
      ],
      "metadata": {
        "id": "ynQMHepO4AOC"
      },
      "id": "ynQMHepO4AOC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure sua chave da API Groq nos Secrets do Colab\n",
        "# V√° em:  (√≠cone da chave) -> Add new secret\n",
        "# Name: Groq_API\n",
        "# Value: sua_chave_aqui\n",
        "\n",
        "try:\n",
        "    key = userdata.get(\"Groq_API\")\n",
        "    print(\"‚úÖ Chave API carregada com sucesso!\")\n",
        "except Exception as e:\n",
        "    key = None\n",
        "    print(f\"‚ùå Erro ao carregar chave API: {e}. Por favor, configure a chave 'Groq_API' nos Secrets do Colab.\")\n",
        "\n",
        "# ===== CONFIGURA√á√ÉO E DADOS =====\n",
        "if key:\n",
        "    config = LLMConfig(\n",
        "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "        api_key=key,\n",
        "        data_url=\"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        "    )\n",
        "\n",
        "    df = pd.read_csv(config.data_url)\n",
        "    Settings.llm = Groq(model=config.model, api_key=config.api_key)\n",
        "\n",
        "    print(\"\\n Estrutura do DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\n‚úÖ Configura√ß√£o completa! Dataset: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
        "else:\n",
        "    df = None\n",
        "    print(\"\\n‚ö†Ô∏è A execu√ß√£o ser√° interrompida pois a API Key n√£o foi carregada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPIEtIXE4Inc",
        "outputId": "4a845863-0590-48ed-bdda-ba083cea75ed"
      },
      "id": "FPIEtIXE4Inc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Chave API carregada com sucesso!\n",
            "\n",
            " Estrutura do DataFrame:\n",
            "     ID_compra filial       cidade tipo_cliente     genero       tipo_produto  \\\n",
            "0  750-67-8428      A  Santo Andr√©       Membro   Feminino     Sa√∫de e Beleza   \n",
            "1  226-31-3081      C  S√£o Caetano       Normal   Feminino        Eletr√¥nicos   \n",
            "2  631-41-3108      A  Santo Andr√©       Normal  Masculino               Casa   \n",
            "3  123-19-1176      A  Santo Andr√©       Membro  Masculino     Sa√∫de e Beleza   \n",
            "4  373-73-7910      A  Santo Andr√©       Normal  Masculino  Esportes e Viagem   \n",
            "\n",
            "   preco_unitario  quantidade  imposto_5%     total        data      hora  \\\n",
            "0           74.69           7     26.1415  548.9715  2024-01-05  13:08:00   \n",
            "1           15.28           5      3.8200   80.2200  2024-03-08  10:29:00   \n",
            "2           46.33           7     16.2155  340.5255  2024-03-03  13:23:00   \n",
            "3           58.22           8     23.2880  489.0480  2024-01-27  20:33:00   \n",
            "4           86.31           7     30.2085  634.3785  2024-02-08  10:37:00   \n",
            "\n",
            "     forma_pagamento  avaliacao  \n",
            "0   Carteira Digital        9.1  \n",
            "1           Dinheiro        9.6  \n",
            "2  Cart√£o de Cr√©dito        7.4  \n",
            "3   Carteira Digital        8.4  \n",
            "4   Carteira Digital        5.3  \n",
            "\n",
            "‚úÖ Configura√ß√£o completa! Dataset: 1000 linhas, 14 colunas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8 Classe Workflow Principal"
      ],
      "metadata": {
        "id": "yYMx5O6c_EPc"
      },
      "id": "yYMx5O6c_EPc"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b825ff55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b825ff55",
        "outputId": "501bc2cf-4584-4310-924b-4e0bdefeb8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Workflow robusto e simplificado definido!\n"
          ]
        }
      ],
      "source": [
        "class PandasWorkflow(Workflow):\n",
        "\n",
        "    @step\n",
        "    async def iniciar_processamento(self, ev: StartEvent) -> CodeEvent:\n",
        "        \"\"\"\n",
        "        Prepara o prompt para gerar o c√≥digo Pandas.\n",
        "        Os dados 'query' e 'df' s√£o recebidos diretamente do StartEvent.\n",
        "        \"\"\"\n",
        "        print(\"  [Workflow] Etapa 1: Iniciando processamento...\")\n",
        "        colunas_info = descricao_colunas(ev.df)\n",
        "        prompt_text = pandas_prompt_str.format(\n",
        "            colunas_detalhes=colunas_info,\n",
        "            df_str=ev.df.head(5).to_string(),\n",
        "            instruction_str=instruction_str,\n",
        "            query_str=ev.query\n",
        "        )\n",
        "        return CodeEvent(pandas_prompt=prompt_text, query=ev.query, df=ev.df)\n",
        "\n",
        "    @step\n",
        "    async def gerar_codigo(self, ev: CodeEvent) -> OutputEvent:\n",
        "        \"\"\"Gera c√≥digo pandas a partir do prompt.\"\"\"\n",
        "        print(\"  [Workflow] Etapa 2: Gerando c√≥digo Pandas...\")\n",
        "        response = await Settings.llm.acomplete(ev.pandas_prompt)\n",
        "        codigo_limpo = limpar_codigo_pandas(str(response).strip())\n",
        "        print(f\"   ‚úÖ C√≥digo gerado: {codigo_limpo}\")\n",
        "        return OutputEvent(pandas_code=codigo_limpo, query=ev.query, df=ev.df)\n",
        "\n",
        "    @step\n",
        "    async def executar_codigo(self, ev: OutputEvent) -> ExecutedEvent:\n",
        "        \"\"\"Executa o c√≥digo Pandas gerado.\"\"\"\n",
        "        print(\" [Workflow] Etapa 3: Executando c√≥digo...\")\n",
        "        try:\n",
        "            # Contexto seguro para a execu√ß√£o do eval\n",
        "            contexto = {\"df\": ev.df, \"pd\": pd}\n",
        "            resultado = eval(ev.pandas_code, {\"__builtins__\": {}}, contexto)\n",
        "            print(f\"   ‚úÖ Resultado da execu√ß√£o: {resultado}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erro na execu√ß√£o: {e}\")\n",
        "            resultado = f\"Erro ao executar o c√≥digo: {str(e)}\"\n",
        "        return ExecutedEvent(\n",
        "            pandas_code=ev.pandas_code,\n",
        "            pandas_output=resultado,\n",
        "            query=ev.query,\n",
        "            df=ev.df\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def finalizar_e_sintetizar(self, ev: ExecutedEvent) -> StopEvent:\n",
        "        \"\"\"Gera a resposta final em linguagem natural.\"\"\"\n",
        "        print(\"  [Workflow] Etapa 4: Sintetizando resposta final...\")\n",
        "        if isinstance(ev.pandas_output, str) and \"Erro\" in ev.pandas_output:\n",
        "            resposta_final = f\"N√£o foi poss√≠vel processar a consulta devido a um erro. {ev.pandas_output}\"\n",
        "        else:\n",
        "            prompt_synthesis = RESPONSE_SYNTHESIS_PROMPT_STR.format(\n",
        "                query_str=ev.query,\n",
        "                pandas_instructions=ev.pandas_code,\n",
        "                pandas_output=str(ev.pandas_output)\n",
        "            )\n",
        "            response = await Settings.llm.acomplete(prompt_synthesis)\n",
        "            resposta_final = str(response).strip()\n",
        "\n",
        "        print(\" [Workflow] Finalizado com sucesso!\")\n",
        "        return StopEvent(result={\n",
        "            \"resposta_final\": resposta_final,\n",
        "            \"pandas_code\": ev.pandas_code,\n",
        "        })\n",
        "\n",
        "print(\"‚úÖ Workflow robusto e simplificado definido!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9 Fun√ß√£o de Execu√ß√£o"
      ],
      "metadata": {
        "id": "XUvrzCXQ_KWI"
      },
      "id": "XUvrzCXQ_KWI"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6e04d1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e04d1ba",
        "outputId": "1182824b-6fb1-4509-ff5c-ff51fd24fd98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fun√ß√£o de execu√ß√£o do workflow corrigida: sem fallback e com tratamento de erro expl√≠cito!\n"
          ]
        }
      ],
      "source": [
        "async def executar_consulta(query: str, df_local: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Executa o workflow de ponta a ponta e trata os erros de forma expl√≠cita, sem fallback.\n",
        "    \"\"\"\n",
        "    if df_local is None:\n",
        "        return {\n",
        "            \"resposta_final\": \"Erro: O DataFrame n√£o foi carregado.\",\n",
        "            \"pandas_code\": \"N/A\"\n",
        "        }\n",
        "    if not query or not query.strip():\n",
        "        return {\n",
        "            \"resposta_final\": \"Erro: A consulta n√£o pode ser vazia.\",\n",
        "            \"pandas_code\": \"N/A\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n Iniciando workflow para a consulta: '{query}'\")\n",
        "        wf = PandasWorkflow()\n",
        "\n",
        "        # O m√©todo run() est√° retornando o dicion√°rio diretamente.\n",
        "        # A corre√ß√£o √© tratar o resultado como o dicion√°rio final.\n",
        "        resultado_final = await wf.run(query=query, df=df_local)\n",
        "\n",
        "        # Verificamos se o resultado √© de fato um dicion√°rio, como esperado.\n",
        "        if not isinstance(resultado_final, dict):\n",
        "             raise TypeError(f\"O workflow retornou um tipo inesperado: {type(resultado_final)}\")\n",
        "\n",
        "        return resultado_final\n",
        "\n",
        "    except Exception as e:\n",
        "        # SEM FALLBACK: Se o workflow falhar, reportamos o erro diretamente.\n",
        "        print(f\"‚ùå Erro cr√≠tico durante a execu√ß√£o do workflow: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Imprime o stack trace completo para depura√ß√£o\n",
        "        return {\n",
        "            \"resposta_final\": f\"Ocorreu um erro cr√≠tico no workflow. Verifique os logs para detalhes. Erro: {str(e)}\",\n",
        "            \"pandas_code\": \"N/A - Falha no workflow\"\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o de execu√ß√£o do workflow corrigida: sem fallback e com tratamento de erro expl√≠cito!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10 Gradio"
      ],
      "metadata": {
        "id": "v47UtKN4HHhi"
      },
      "id": "v47UtKN4HHhi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.1 C√©lula de prepara√ß√£o"
      ],
      "metadata": {
        "id": "wjIzncuUyPFT"
      },
      "id": "wjIzncuUyPFT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define uma vari√°vel global para o caminho da fonte\n",
        "FONT_PATH = \"\"\n",
        "FONT_NAME_TO_FIND = \"DejaVu Sans\"\n",
        "\n",
        "try:\n",
        "    # A maneira mais robusta: usar o gerenciador de fontes do Matplotlib\n",
        "    print(f\"Procurando a fonte '{FONT_NAME_TO_FIND}' localmente com Matplotlib...\")\n",
        "    FONT_PATH = fm.findfont(FONT_NAME_TO_FIND, fallback_to_default=True)\n",
        "\n",
        "    if not os.path.exists(FONT_PATH):\n",
        "        raise FileNotFoundError  # For√ßa a entrada no bloco de exce√ß√£o se o caminho for inv√°lido\n",
        "\n",
        "    print(f\"‚úÖ Fonte encontrada com sucesso em: {FONT_PATH}\")\n",
        "\n",
        "except Exception:\n",
        "    print(f\"‚ö†Ô∏è Fonte '{FONT_NAME_TO_FIND}' n√£o encontrada. Tentando baixar uma alternativa confi√°vel (Roboto)...\")\n",
        "\n",
        "    # --- PLANO B: Se o Matplotlib falhar, baixa uma fonte de um link est√°vel ---\n",
        "    FONT_FILE_NAME = \"Roboto-Regular.ttf\"\n",
        "    FONT_URL = \"https://github.com/google/fonts/raw/main/ofl/roboto/Roboto-Regular.ttf\"\n",
        "\n",
        "    if not os.path.exists(FONT_FILE_NAME):\n",
        "        try:\n",
        "            print(f\"Baixando '{FONT_FILE_NAME}'...\")\n",
        "            response = requests.get(FONT_URL)\n",
        "            response.raise_for_status()\n",
        "            with open(FONT_FILE_NAME, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            FONT_PATH = FONT_FILE_NAME\n",
        "            print(f\"‚úÖ Fonte alternativa baixada com sucesso!\")\n",
        "        except Exception as download_error:\n",
        "            print(f\"‚ùå FALHA CR√çTICA: N√£o foi poss√≠vel encontrar a fonte localmente nem baix√°-la. Erro: {download_error}\")\n",
        "            FONT_PATH = \"\"  # Garante que o caminho esteja vazio em caso de falha\n",
        "    else:\n",
        "        FONT_PATH = FONT_FILE_NAME\n",
        "        print(f\"‚úÖ Fonte alternativa '{FONT_FILE_NAME}' j√° existe localmente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33DLlhFtyTSa",
        "outputId": "5c4d8d1c-c6c6-436b-fe1a-670b5400566e"
      },
      "id": "33DLlhFtyTSa",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procurando a fonte 'DejaVu Sans' localmente com Matplotlib...\n",
            "‚úÖ Fonte encontrada com sucesso em: /usr/local/lib/python3.12/dist-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.2 Fun√ß√µes auxiliares Gradio"
      ],
      "metadata": {
        "id": "-296ewDprziH"
      },
      "id": "-296ewDprziH"
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_dados(file, df_estado):\n",
        "    if file is None:\n",
        "        return \"‚ùå Nenhum arquivo selecionado\", df_estado\n",
        "    try:\n",
        "        df_local = pd.read_csv(file.name)\n",
        "        return f\"‚úÖ Arquivo carregado! {df_local.shape[0]} linhas, {df_local.shape[1]} colunas.\", df_local\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Erro ao carregar arquivo: {str(e)}\", df_estado\n",
        "\n",
        "def processar_pergunta(pergunta: str, df_local: Optional[pd.DataFrame], historico: Historico):\n",
        "    if df_local is None:\n",
        "        resposta = \"‚ùå Por favor, carregue um arquivo CSV primeiro.\"\n",
        "        return resposta, historico\n",
        "\n",
        "    if not pergunta or not pergunta.strip():\n",
        "        resposta = \"‚ùå Por favor, digite uma pergunta v√°lida.\"\n",
        "        return resposta, historico\n",
        "\n",
        "    try:\n",
        "        resultado = asyncio.run(executar_consulta(pergunta, df_local))\n",
        "        resposta = resultado.get(\"resposta_final\", \"‚ùå Erro: a resposta n√£o foi gerada.\")\n",
        "        historico.adicionar(pergunta, resposta)\n",
        "        return resposta, historico\n",
        "    except Exception as e:\n",
        "        erro_msg = f\"‚ùå Ocorreu um erro inesperado: {str(e)}\"\n",
        "        historico.adicionar(pergunta, erro_msg)\n",
        "        return erro_msg, historico\n",
        "\n",
        "# --- FUN√á√ÉO GERAR_PDF ---\n",
        "# A vari√°vel FONT_PATH √© definida na c√©lula de prepara√ß√£o anterior.\n",
        "if 'FONT_PATH' not in globals() or not FONT_PATH:\n",
        "    print(\"‚ùå ERRO: O caminho da fonte (FONT_PATH) n√£o foi definido. Execute a c√©lula de prepara√ß√£o.\")\n",
        "    FONT_PATH = \"\"\n",
        "\n",
        "def gerar_pdf(historico: Historico):\n",
        "    if not historico or not historico.entradas:\n",
        "        print(\"‚ö†Ô∏è Tentativa de gerar PDF com hist√≥rico vazio.\")\n",
        "        return None\n",
        "\n",
        "    # Verifica se o caminho da fonte √© v√°lido ANTES de tentar gerar o PDF\n",
        "    if not FONT_PATH or not os.path.exists(FONT_PATH):\n",
        "        msg = f\"‚ùå Erro ao gerar PDF: Arquivo da fonte n√£o foi encontrado em '{FONT_PATH}'. Execute a c√©lula de prepara√ß√£o.\"\n",
        "        print(msg)\n",
        "        # Opcional: retornar um PDF de erro\n",
        "        return None\n",
        "\n",
        "    print(f\"Gerando PDF com {len(historico.entradas)} entradas usando a fonte: {FONT_PATH}\")\n",
        "\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        caminho_pdf = f\"relatorio_consultas_{timestamp}.pdf\"\n",
        "\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "\n",
        "        # --- MUDAN√áA FINAL ---\n",
        "        # Usa o caminho absoluto da fonte encontrado no sistema.\n",
        "        pdf.add_font('DejaVu', '', FONT_PATH, uni=True)\n",
        "        pdf.set_font('DejaVu', '', 10)\n",
        "\n",
        "        pdf.set_font('DejaVu', '', 16)\n",
        "        pdf.cell(0, 10, \"Relat√≥rio de Consultas ao CSV\", ln=True, align=\"C\")\n",
        "        pdf.ln(10)\n",
        "\n",
        "        for i, (pergunta, resposta) in enumerate(historico.entradas, 1):\n",
        "            pdf.set_font('DejaVu', '', 12)\n",
        "            pdf.multi_cell(0, 8, f\"Pergunta {i}: {pergunta}\")\n",
        "            pdf.ln(2)\n",
        "            pdf.set_font('DejaVu', '', 10)\n",
        "            pdf.multi_cell(0, 6, f\"Resposta: {resposta}\")\n",
        "            pdf.ln(8)\n",
        "            pdf.cell(0, 0, '', 'T')\n",
        "            pdf.ln(8)\n",
        "\n",
        "        pdf.output(caminho_pdf)\n",
        "        print(f\"‚úÖ PDF gerado com sucesso: {caminho_pdf}\")\n",
        "        return caminho_pdf\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro cr√≠tico ao gerar PDF: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes auxiliares do Gradio prontas para usar fonte local.\")\n",
        "\n",
        "# --- FUN√á√ÉO EXTRA PARA LIMPAR A INTERFACE ---\n",
        "def limpar_tudo():\n",
        "    # Retorna valores vazios/padr√£o para limpar os componentes\n",
        "    return \"\", None, Historico()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CC_I9Rlr_YF",
        "outputId": "4768691c-88a2-4dd3-ce84-930297f1f701"
      },
      "id": "7CC_I9Rlr_YF",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fun√ß√µes auxiliares do Gradio prontas para usar fonte local.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.3 Interface Gradio"
      ],
      "metadata": {
        "id": "10D8zK05iSbj"
      },
      "id": "10D8zK05iSbj"
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"Analisador de CSV com Pandas e LlamaIndex\", theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center;\">\n",
        "            <h1>üêº Analisador de CSV com Pandas e LlamaIndex</h1>\n",
        "            <p>Fa√ßa upload de um arquivo CSV, fa√ßa perguntas em linguagem natural e baixe o relat√≥rio!</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Estados\n",
        "    df_estado = gr.State()\n",
        "    historico_estado = gr.State(value=Historico())\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            input_arquivo = gr.File(label=\"1. Fa√ßa upload do seu arquivo CSV\", file_types=[\".csv\"])\n",
        "            upload_status = gr.Textbox(label=\"Status do Upload\", interactive=False, placeholder=\"Aguardando arquivo...\")\n",
        "            botao_gerar_pdf = gr.Button(\"üìÑ Gerar e Baixar PDF\", variant=\"secondary\")\n",
        "            output_pdf = gr.File(label=\"Baixar Relat√≥rio\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            # --- Componente Chatbot ---\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Hist√≥rico da Conversa\",\n",
        "                height=550,\n",
        "                type='messages'\n",
        "            )\n",
        "            with gr.Row():\n",
        "                input_pergunta = gr.Textbox(\n",
        "                    scale=4,\n",
        "                    show_label=False,\n",
        "                    placeholder=\"Ex: Qual o total de vendas por filial?\",\n",
        "                    container=False\n",
        "                )\n",
        "                botao_enviar = gr.Button(\" Enviar\", variant=\"primary\", scale=1, min_width=100)\n",
        "\n",
        "    # --- L√≥gica da Interface ---\n",
        "\n",
        "    def chat_interface(pergunta, chat_history, df_local, historico_obj):\n",
        "        # O chat_history agora √© uma lista de dicion√°rios.\n",
        "        # Adiciona a mensagem do usu√°rio ao hist√≥rico no formato correto.\n",
        "        chat_history.append({\"role\": \"user\", \"content\": pergunta})\n",
        "\n",
        "        if df_local is None:\n",
        "            resposta = \"‚ùå Por favor, carregue um arquivo CSV primeiro.\"\n",
        "            # Adiciona a resposta do assistente ao hist√≥rico.\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": resposta})\n",
        "            return \"\", chat_history, historico_obj\n",
        "\n",
        "        # Chama a fun√ß√£o de processamento principal, que n√£o precisa mudar.\n",
        "        resposta, historico_obj_atualizado = processar_pergunta(pergunta, df_local, historico_obj)\n",
        "\n",
        "        # Adiciona a resposta do assistente ao hist√≥rico .\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": resposta})\n",
        "\n",
        "        return \"\", chat_history, historico_obj_atualizado\n",
        "\n",
        "    # Fun√ß√£o para limpar os estados\n",
        "    def limpar_conversa():\n",
        "        return Historico(), None\n",
        "\n",
        "    # --- Conex√µes dos Eventos ---\n",
        "    input_arquivo.upload(fn=carregar_dados, inputs=[input_arquivo, df_estado], outputs=[upload_status, df_estado])\n",
        "\n",
        "    botao_enviar.click(\n",
        "        fn=chat_interface,\n",
        "        inputs=[input_pergunta, chatbot, df_estado, historico_estado],\n",
        "        outputs=[input_pergunta, chatbot, historico_estado]\n",
        "    )\n",
        "    input_pergunta.submit(\n",
        "        fn=chat_interface,\n",
        "        inputs=[input_pergunta, chatbot, df_estado, historico_estado],\n",
        "        outputs=[input_pergunta, chatbot, historico_estado]\n",
        "    )\n",
        "\n",
        "    botao_gerar_pdf.click(fn=gerar_pdf, inputs=[historico_estado], outputs=[output_pdf])\n",
        "\n",
        "    chatbot.clear(fn=limpar_conversa, inputs=[], outputs=[historico_estado, output_pdf])\n",
        "\n",
        "print(\"‚úÖ Interface Gradio finalizada!\")\n",
        "\n",
        "# Lan√ßa a interface\n",
        "if __name__ == \"__main__\":\n",
        "    if df is not None:\n",
        "        app.launch(debug=True)\n",
        "    else:\n",
        "        print(\"üî¥ A interface Gradio n√£o ser√° iniciada pois a configura√ß√£o da API falhou.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "xBhCifgHiU9r",
        "outputId": "d5d4de68-96d3-4fc3-deeb-98d46d914bcc"
      },
      "id": "xBhCifgHiU9r",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interface Gradio finalizada, moderna e sem warnings!\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ef5393479a158f0e87.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ef5393479a158f0e87.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ef5393479a158f0e87.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11 Teste R√°pido"
      ],
      "metadata": {
        "id": "SnrigB_l_QVm"
      },
      "id": "SnrigB_l_QVm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste r√°pido para verificar se a fun√ß√£o principal est√° funcionando\n",
        "async def teste_rapido():\n",
        "    if df is not None:\n",
        "        query_teste = \"Qual √© a avalia√ß√£o m√©dia de cada filial?\"\n",
        "        # Corrigido: passando o df_local como argumento\n",
        "        resultado = await executar_consulta(query_teste, df_local=df)\n",
        "\n",
        "        if resultado and \"Erro\" not in resultado.get(\"resposta_final\", \"\"):\n",
        "            print(\"\\n‚úÖ Sistema funcionando perfeitamente!\")\n",
        "            print(f\"Resposta: {resultado['resposta_final']}\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå Houve um problema no teste: {resultado}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Teste n√£o executado: DataFrame n√£o foi carregado.\")\n",
        "\n",
        "# Executar o teste\n",
        "await teste_rapido()"
      ],
      "metadata": {
        "id": "mGuURyz__TPK"
      },
      "id": "mGuURyz__TPK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12 Exemplos de Uso Individual"
      ],
      "metadata": {
        "id": "w7XxG1eC_Wk5"
      },
      "id": "w7XxG1eC_Wk5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f8a50d",
      "metadata": {
        "id": "69f8a50d"
      },
      "outputs": [],
      "source": [
        "# Execute uma consulta por vez para testar\n",
        "\n",
        "# Exemplo 1\n",
        "query = \"Quais s√£o os 5 produtos mais vendidos?\"\n",
        "result = await executar_consulta(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 2\n",
        "query = \"Qual o total de vendas por regi√£o?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "cLYZHwUK_ZwW"
      },
      "id": "cLYZHwUK_ZwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 3\n",
        "query = \"Qual produto tem a maior margem de lucro?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "2rGJQRL__g-k"
      },
      "id": "2rGJQRL__g-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 4\n",
        "query = \"Quantas vendas foram feitas em cada m√™s?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "7GmSKrxt_ia9"
      },
      "id": "7GmSKrxt_ia9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Qual o gen√™ro que mais consome e o segmento do produto?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "QeXUbCZaOY2l"
      },
      "id": "QeXUbCZaOY2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13 Bateria de Testes"
      ],
      "metadata": {
        "id": "7gieDaX2_e2n"
      },
      "id": "7gieDaX2_e2n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Executar m√∫ltiplas consultas de uma vez\n",
        "consultas_exemplo = [\n",
        "    \"Qual √© a avalia√ß√£o m√©dia de cada filial?\",\n",
        "    \"Quais s√£o os 5 produtos mais vendidos?\",\n",
        "    \"Qual o total de vendas por regi√£o?\",\n",
        "    \"Qual produto tem a maior margem de lucro?\",\n",
        "    \"Quantas vendas foram feitas em cada m√™s?\",\n",
        "    \"Qual √© a receita total por categoria de produto?\",\n",
        "    \"Qual filial teve melhor desempenho?\",\n",
        "    \"Mostre a distribui√ß√£o de pre√ßos dos produtos\"\n",
        "]\n",
        "\n",
        "print(\" Executando bateria de testes...\")\n",
        "resultados = await executar_multiplas_consultas(consultas_exemplo)\n",
        "\n",
        "print(f\"\\n‚úÖ Conclu√≠do! {len([r for r in resultados if r])} consultas processadas com sucesso.\")"
      ],
      "metadata": {
        "id": "7t268z2a_rK5"
      },
      "id": "7t268z2a_rK5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14 Modo Interativo S/ interface Gradio"
      ],
      "metadata": {
        "id": "YU3bIuXj_ty_"
      },
      "id": "YU3bIuXj_ty_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Modo interativo - digite suas pr√≥prias consultas\n",
        "print(\" MODO INTERATIVO\")\n",
        "print(\"Digite suas consultas (digite 'sair' para terminar)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"\\n Sua consulta: \").strip()\n",
        "\n",
        "        if query.lower() in ['sair', 'exit', 'quit', '']:\n",
        "            print(\" At√© logo!\")\n",
        "            break\n",
        "\n",
        "        await executar_consulta(query, mostrar_detalhes=False)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n Interrompido pelo usu√°rio!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\" Erro: {e}\")"
      ],
      "metadata": {
        "id": "DXM0Gh67_oiY"
      },
      "id": "DXM0Gh67_oiY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##15 An√°lise Manual dos Dados\n"
      ],
      "metadata": {
        "id": "YvXLkuzD_yrz"
      },
      "id": "YvXLkuzD_yrz"
    },
    {
      "cell_type": "code",
      "source": [
        "# An√°lise dos dados originais para compara√ß√£o\n",
        "print(\" AN√ÅLISE MANUAL DOS DADOS (para compara√ß√£o)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Estat√≠sticas b√°sicas:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(f\"\\nContagem por categoria:\")\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].value_counts().head())\n",
        "\n",
        "print(f\"\\nValores nulos:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "ohovTGoP_7tG"
      },
      "id": "ohovTGoP_7tG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dicas e Informa√ß√µes"
      ],
      "metadata": {
        "id": "ETccYDWS_4Zc"
      },
      "id": "ETccYDWS_4Zc"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üí° DICAS PARA USO:\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. Para consultas complexas, seja espec√≠fico\")\n",
        "print(\"2. Use nomes exatos das colunas mostradas na an√°lise\")\n",
        "print(\"3. Experimente diferentes tipos de an√°lise:\")\n",
        "print(\"   - Agrupamentos: 'm√©dia por categoria'\")\n",
        "print(\"   - Rankings: 'top 10 produtos'\")\n",
        "print(\"   - Filtros: 'vendas acima de X valor'\")\n",
        "print(\"   - S√©ries temporais: 'vendas por m√™s'\")\n",
        "print(\"4. O sistema mostra o c√≥digo Pandas usado!\")\n",
        "\n",
        "print(f\"\\n INFORMA√á√ïES DO DATASET:\")\n",
        "print(f\"Suas colunas: {list(df.columns)}\")\n",
        "print(f\"Quantidade de dados: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
        "print(f\"Tipos de dados: {df.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "print(f\"\\n SUGEST√ïES DE CONSULTAS:\")\n",
        "example_queries = [\n",
        "    \"Qual a correla√ß√£o entre pre√ßo e quantidade vendida?\",\n",
        "    \"Mostre a distribui√ß√£o de vendas por dia da semana\",\n",
        "    \"Qual filial tem o melhor desempenho?\",\n",
        "    \"Compare as vendas do primeiro e √∫ltimo trimestre\",\n",
        "    \"Identifique produtos com baixo estoque\",\n",
        "    \"Qual √© o produto mais caro por categoria?\",\n",
        "    \"Mostre as vendas m√©dias por vendedor\",\n",
        "    \"Qual regi√£o tem menor varia√ß√£o de pre√ßos?\"\n",
        "]\n",
        "\n",
        "for i, query in enumerate(example_queries, 1):\n",
        "    print(f\"{i}. {query}\")\n",
        "\n",
        "print(f\"\\n MODELO UTILIZADO: {config.model}\")\n",
        "print(f\" URL DOS DADOS: {config.data_url}\")"
      ],
      "metadata": {
        "id": "lEPP6tuj_925"
      },
      "id": "lEPP6tuj_925",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notas de Uso:\n",
        "\n",
        "*   Configura√ß√£o da API: Configure sua chave Groq nos Secrets do Colab\n",
        "*   Execu√ß√£o sequencial: Execute as c√©lulas na ordem\n",
        "*   Personaliza√ß√£o: Modifique a URL dos dados na C√©lula 6\n",
        "*   Debug: Os logs mostram o c√≥digo Pandas gerado\n",
        "*   Interativo: Use a C√©lula 12 para consultas livres\n",
        "\n",
        "##Estrutura final:\n",
        "\n",
        "*   Instala√ß√£o autom√°tica de depend√™ncias\n",
        "* Configura√ß√£o completa da API\n",
        "* Explora√ß√£o autom√°tica dos dados\n",
        "* Sistema de workflow robusto\n",
        "* Exemplos prontos para uso\n",
        "* Modo interativo\n",
        "* Sistema de debug integrado\n",
        "\n",
        "\n",
        ">Desenvolvido por Yuri Arduino Bernardineli Alves | GitHub: [YuriArduino](https://github.com/YuriArduino) | E-mail: yuriarduino@gmail.com"
      ],
      "metadata": {
        "id": "_TbE6Un3AGjs"
      },
      "id": "_TbE6Un3AGjs"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}