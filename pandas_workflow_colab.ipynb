{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas Workflow com LlamaIndex Workflows\n",
        "\n",
        ">Este notebook demonstra como criar um sistema de análise de dados usando LlamaIndex, Pandas e Groq para consultas em linguagem natural.\n",
        "\n",
        "\n",
        "## Sobre a Atualização do LlamaIndex\n",
        "\n",
        "O **LlamaIndex** descontinuou o módulo **QueryPipeline** em favor de uma nova abordagem chamada **Workflows**.\n",
        "Essa mudança foi implementada na versão `0.11` do LlamaIndex, com o objetivo de oferecer uma arquitetura mais\n",
        "flexível e escalável para a construção de aplicações de IA generativa.\n",
        "\n",
        "### Por que o QueryPipeline foi descontinuado?\n",
        "O QueryPipeline era uma API declarativa útil para orquestrar consultas simples a avançadas sobre os dados.\n",
        "Porém, ele se mostrou limitado para cenários mais dinâmicos. Por isso, os **Workflows** foram introduzidos,\n",
        "trazendo uma arquitetura orientada a eventos para fluxos de trabalho mais sofisticados.\n",
        "\n",
        "### O que são Workflows?\n",
        "Workflows permitem orquestrar etapas personalizadas de forma assíncrona e condicional, facilitando\n",
        "integrações complexas e manipulação de dados em tempo real.\n",
        "\n",
        "**Documentação oficial:** [docs.llamaindex.ai](https://docs.llamaindex.ai)\n",
        "\n"
      ],
      "metadata": {
        "id": "3pzznnjN3W-Z"
      },
      "id": "3pzznnjN3W-Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Código:\n"
      ],
      "metadata": {
        "id": "KYxQbJBGAjb7"
      },
      "id": "KYxQbJBGAjb7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 Instalar Dependências:"
      ],
      "metadata": {
        "id": "ne3hDRwyAlhT"
      },
      "id": "ne3hDRwyAlhT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eaa06244",
      "metadata": {
        "id": "eaa06244"
      },
      "outputs": [],
      "source": [
        "# Instalar dependências (-q para instalação silenciosa)\n",
        "!pip install -q jedi>=0.16 llama-index llama-index-llms-openai llama-index-llms-groq llama-index-experimental gradio fpdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bbf94cd",
      "metadata": {
        "id": "8bbf94cd"
      },
      "source": [
        "##1.1 Para uso no Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "MnIk3zEA-Vaw"
      },
      "id": "MnIk3zEA-Vaw",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 Imports"
      ],
      "metadata": {
        "id": "TfVHgRx83GYA"
      },
      "id": "TfVHgRx83GYA"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "327f6a9c",
      "metadata": {
        "id": "327f6a9c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import textwrap\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict, ValidationError\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core.workflow import Workflow, Event, StartEvent as BaseStartEvent, StopEvent, step\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "from typing import Any, List, Tuple, Optional\n",
        "import asyncio\n",
        "import matplotlib.font_manager as fm\n",
        "import gradio as gr\n",
        "from fpdf import FPDF\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 Instruções e Prompts\n",
        "\n",
        "(Importante para validar a saída, pode ser usado como debug ou  )"
      ],
      "metadata": {
        "id": "3YY-0c2l-i5M"
      },
      "id": "3YY-0c2l-i5M"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== INSTRUÇÕES PARA CONVERSÃO DE CONSULTAS =====\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
        "    \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
        "    \"3. O código deve representar uma solução para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESSÃO FINAL.\\n\"\n",
        "    \"5. Não coloque a expressão entre aspas.\\n\"\n",
        "    \"6. Evite atribuições (=) na linha final - prefira expressões que retornem valores.\\n\"\n",
        "    \"7. Para operações de múltiplas linhas, termine com uma expressão que retorne o resultado.\\n\"\n",
        "    \"8. Exemplos válidos:\\n\"\n",
        "    \"   - df.groupby('coluna')['valor'].sum()\\n\"\n",
        "    \"   - df['coluna'].value_counts().head(5)\\n\"\n",
        "    \"   - df.describe()\\n\"\n",
        "    \"9. Evite códigos como 'df['coluna'] = valor' - prefira consultas que retornem dados.\\n\"\n",
        ")\n",
        "\n",
        "# ===== PROMPTS =====\n",
        "pandas_prompt_str = (\n",
        "    \"Você está trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este é o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instruções:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Expressão:\"\n",
        ")\n",
        "\n",
        "RESPONSE_SYNTHESIS_PROMPT_STR = (\n",
        "   \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "   \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
        "   \"Consulta: {query_str}\\n\\n\"\n",
        "   \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "   \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
        "   \"Resposta: \"\n",
        "   \"Ao final, exibir o código usado para gerar a resposta, no formato: O código utilizado foi {pandas_instructions}\"\n",
        ")\n",
        "\n",
        "print(\"✅ Prompts configurados com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElcLXNqi-m2w",
        "outputId": "a9f9704f-2e73-4fa8-d20a-8edcac72b1eb"
      },
      "id": "ElcLXNqi-m2w",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Prompts configurados com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4 Configuração Pydantic V2"
      ],
      "metadata": {
        "id": "eaOg-rIf3jaL"
      },
      "id": "eaOg-rIf3jaL"
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMConfig(BaseModel):\n",
        "    model: str = Field(..., description=\"Nome do modelo Groq a ser usado\")\n",
        "    api_key: str = Field(..., description=\"Chave da API Groq\")\n",
        "    data_url: str = Field(..., description=\"URL do CSV com os dados\")\n",
        "\n",
        "    @field_validator(\"data_url\")\n",
        "    @classmethod\n",
        "    def validar_url(cls, v: str) -> str:\n",
        "        if not (v.startswith(\"http://\") or v.startswith(\"https://\")):\n",
        "            raise ValueError(\"data_url deve começar com http:// ou https://\")\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"api_key\")\n",
        "    @classmethod\n",
        "    def validar_api_key(cls, v: str) -> str:\n",
        "        if not v or len(v.strip()) == 0:\n",
        "            raise ValueError(\"api_key não pode ser vazia\")\n",
        "        return v\n",
        "\n",
        "print(\"✅ Configuração LLM com Pydantic criada!\")\n",
        "\n",
        "class Historico(BaseModel):\n",
        "    entradas: List[Tuple[str, str]] = []\n",
        "\n",
        "    def adicionar(self, pergunta: str, resposta: str):\n",
        "        \"\"\"Adiciona uma entrada ao histórico se pergunta e resposta forem válidas.\"\"\"\n",
        "        if pergunta and resposta:\n",
        "            self.entradas.append((pergunta, resposta))\n",
        "\n",
        "print(\"✅ Modelo de histórico com Pydantic criado!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf65iXIp3riw",
        "outputId": "c07a8bbb-cbca-41fa-a5f6-9ce3ad1aa6a7"
      },
      "id": "zf65iXIp3riw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Configuração LLM com Pydantic criada!\n",
            "✅ Modelo de histórico com Pydantic criado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5 Modelos de Eventos\n",
        "\n",
        "(Rastrear cada passo da interação, como um histórico organizado)"
      ],
      "metadata": {
        "id": "lHeWjLLu-tjv"
      },
      "id": "lHeWjLLu-tjv"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos de Eventos com Pydantic ---\n",
        "\n",
        "# --- Evento de início customizado para garantir os dados de entrada ---\n",
        "class StartEvent(BaseStartEvent):\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "# --- Eventos intermediários para propagar os dados no workflow ---\n",
        "class CodeEvent(Event):\n",
        "    pandas_prompt: str\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "class OutputEvent(Event):\n",
        "    pandas_code: str\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "class ExecutedEvent(Event):\n",
        "    pandas_code: str\n",
        "    pandas_output: Any\n",
        "    query: str\n",
        "    df: pd.DataFrame\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "print(\"✅ Modelos de eventos do workflow definidos com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJvzUYbg-zP0",
        "outputId": "822860b0-9605-4a50-de1b-8f302b8ad9d5"
      },
      "id": "MJvzUYbg-zP0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modelos de eventos do workflow definidos com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6 Funções Auxiliares"
      ],
      "metadata": {
        "id": "zoVRLbkg38zx"
      },
      "id": "zoVRLbkg38zx"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2ecab43e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ecab43e",
        "outputId": "62e7c55b-f1e8-4bf9-9599-394fefda396b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Funções auxiliares configuradas!\n"
          ]
        }
      ],
      "source": [
        "def carregar_arquivo(caminho_arquivo: str, df_estado: Optional[pd.DataFrame] = None) -> Tuple[str, Optional[pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Carrega um CSV do caminho fornecido. Se caminho_arquivo for None, mantém df_estado.\n",
        "    Retorna mensagem de status e DataFrame atualizado.\n",
        "    \"\"\"\n",
        "    if not caminho_arquivo:\n",
        "        return \"Nenhum arquivo enviado. Usando dados atuais (se houver).\", df_estado\n",
        "    try:\n",
        "        df_novo = pd.read_csv(caminho_arquivo)\n",
        "        return \"Arquivo carregado com sucesso!\", df_novo\n",
        "    except Exception as e:\n",
        "        return f\"Erro ao carregar arquivo: {str(e)}\", df_estado\n",
        "\n",
        "def formatar_texto(response: str, largura: int = 100, imprimir: bool = True):\n",
        "    texto_formatado = textwrap.fill(response, width=largura)\n",
        "    if imprimir:\n",
        "        print(texto_formatado)\n",
        "    return texto_formatado\n",
        "\n",
        "def descricao_colunas(df: pd.DataFrame) -> str:\n",
        "    descricao = \"\\n\".join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return \"Colunas do DataFrame:\\n\" + descricao\n",
        "\n",
        "def limpar_codigo_pandas(codigo: str) -> str:\n",
        "    \"\"\"Limpa e valida o código Pandas gerado pela LLM.\"\"\"\n",
        "    codigo = re.sub(r'```(?:python)?\\n?', '', codigo) # Remove ```python\n",
        "    codigo = re.sub(r'```', '', codigo) # Remove ``` de fechamento\n",
        "    linhas = [linha.strip() for linha in codigo.split('\\n') if linha.strip()]\n",
        "\n",
        "    # Filtra linhas que são comentários ou texto explicativo\n",
        "    codigo_filtrado = [\n",
        "        linha for linha in linhas if not (\n",
        "            linha.startswith('#') or\n",
        "            'resposta:' in linha.lower() or\n",
        "            'resultado:' in linha.lower()\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if not codigo_filtrado:\n",
        "        return \"\"\n",
        "\n",
        "    # Se houver várias linhas, a última é geralmente a expressão de retorno\n",
        "    return codigo_filtrado[-1]\n",
        "\n",
        "print(\"✅ Funções auxiliares configuradas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7 Configuração da API e Dados"
      ],
      "metadata": {
        "id": "ynQMHepO4AOC"
      },
      "id": "ynQMHepO4AOC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure sua chave da API Groq nos Secrets do Colab\n",
        "# Vá em:  (ícone da chave) -> Add new secret\n",
        "# Name: Groq_API\n",
        "# Value: sua_chave_aqui\n",
        "\n",
        "try:\n",
        "    key = userdata.get(\"Groq_API\")\n",
        "    print(\"✅ Chave API carregada com sucesso!\")\n",
        "except Exception as e:\n",
        "    key = None\n",
        "    print(f\"❌ Erro ao carregar chave API: {e}. Por favor, configure a chave 'Groq_API' nos Secrets do Colab.\")\n",
        "\n",
        "# ===== CONFIGURAÇÃO E DADOS =====\n",
        "if key:\n",
        "    config = LLMConfig(\n",
        "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "        api_key=key,\n",
        "        data_url=\"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        "    )\n",
        "\n",
        "    df = pd.read_csv(config.data_url)\n",
        "    Settings.llm = Groq(model=config.model, api_key=config.api_key)\n",
        "\n",
        "    print(\"\\n Estrutura do DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\n✅ Configuração completa! Dataset: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
        "else:\n",
        "    df = None\n",
        "    print(\"\\n⚠️ A execução será interrompida pois a API Key não foi carregada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPIEtIXE4Inc",
        "outputId": "4a845863-0590-48ed-bdda-ba083cea75ed"
      },
      "id": "FPIEtIXE4Inc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chave API carregada com sucesso!\n",
            "\n",
            " Estrutura do DataFrame:\n",
            "     ID_compra filial       cidade tipo_cliente     genero       tipo_produto  \\\n",
            "0  750-67-8428      A  Santo André       Membro   Feminino     Saúde e Beleza   \n",
            "1  226-31-3081      C  São Caetano       Normal   Feminino        Eletrônicos   \n",
            "2  631-41-3108      A  Santo André       Normal  Masculino               Casa   \n",
            "3  123-19-1176      A  Santo André       Membro  Masculino     Saúde e Beleza   \n",
            "4  373-73-7910      A  Santo André       Normal  Masculino  Esportes e Viagem   \n",
            "\n",
            "   preco_unitario  quantidade  imposto_5%     total        data      hora  \\\n",
            "0           74.69           7     26.1415  548.9715  2024-01-05  13:08:00   \n",
            "1           15.28           5      3.8200   80.2200  2024-03-08  10:29:00   \n",
            "2           46.33           7     16.2155  340.5255  2024-03-03  13:23:00   \n",
            "3           58.22           8     23.2880  489.0480  2024-01-27  20:33:00   \n",
            "4           86.31           7     30.2085  634.3785  2024-02-08  10:37:00   \n",
            "\n",
            "     forma_pagamento  avaliacao  \n",
            "0   Carteira Digital        9.1  \n",
            "1           Dinheiro        9.6  \n",
            "2  Cartão de Crédito        7.4  \n",
            "3   Carteira Digital        8.4  \n",
            "4   Carteira Digital        5.3  \n",
            "\n",
            "✅ Configuração completa! Dataset: 1000 linhas, 14 colunas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8 Classe Workflow Principal"
      ],
      "metadata": {
        "id": "yYMx5O6c_EPc"
      },
      "id": "yYMx5O6c_EPc"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b825ff55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b825ff55",
        "outputId": "501bc2cf-4584-4310-924b-4e0bdefeb8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Workflow robusto e simplificado definido!\n"
          ]
        }
      ],
      "source": [
        "class PandasWorkflow(Workflow):\n",
        "\n",
        "    @step\n",
        "    async def iniciar_processamento(self, ev: StartEvent) -> CodeEvent:\n",
        "        \"\"\"\n",
        "        Prepara o prompt para gerar o código Pandas.\n",
        "        Os dados 'query' e 'df' são recebidos diretamente do StartEvent.\n",
        "        \"\"\"\n",
        "        print(\"  [Workflow] Etapa 1: Iniciando processamento...\")\n",
        "        colunas_info = descricao_colunas(ev.df)\n",
        "        prompt_text = pandas_prompt_str.format(\n",
        "            colunas_detalhes=colunas_info,\n",
        "            df_str=ev.df.head(5).to_string(),\n",
        "            instruction_str=instruction_str,\n",
        "            query_str=ev.query\n",
        "        )\n",
        "        return CodeEvent(pandas_prompt=prompt_text, query=ev.query, df=ev.df)\n",
        "\n",
        "    @step\n",
        "    async def gerar_codigo(self, ev: CodeEvent) -> OutputEvent:\n",
        "        \"\"\"Gera código pandas a partir do prompt.\"\"\"\n",
        "        print(\"  [Workflow] Etapa 2: Gerando código Pandas...\")\n",
        "        response = await Settings.llm.acomplete(ev.pandas_prompt)\n",
        "        codigo_limpo = limpar_codigo_pandas(str(response).strip())\n",
        "        print(f\"   ✅ Código gerado: {codigo_limpo}\")\n",
        "        return OutputEvent(pandas_code=codigo_limpo, query=ev.query, df=ev.df)\n",
        "\n",
        "    @step\n",
        "    async def executar_codigo(self, ev: OutputEvent) -> ExecutedEvent:\n",
        "        \"\"\"Executa o código Pandas gerado.\"\"\"\n",
        "        print(\" [Workflow] Etapa 3: Executando código...\")\n",
        "        try:\n",
        "            # Contexto seguro para a execução do eval\n",
        "            contexto = {\"df\": ev.df, \"pd\": pd}\n",
        "            resultado = eval(ev.pandas_code, {\"__builtins__\": {}}, contexto)\n",
        "            print(f\"   ✅ Resultado da execução: {resultado}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Erro na execução: {e}\")\n",
        "            resultado = f\"Erro ao executar o código: {str(e)}\"\n",
        "        return ExecutedEvent(\n",
        "            pandas_code=ev.pandas_code,\n",
        "            pandas_output=resultado,\n",
        "            query=ev.query,\n",
        "            df=ev.df\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def finalizar_e_sintetizar(self, ev: ExecutedEvent) -> StopEvent:\n",
        "        \"\"\"Gera a resposta final em linguagem natural.\"\"\"\n",
        "        print(\"  [Workflow] Etapa 4: Sintetizando resposta final...\")\n",
        "        if isinstance(ev.pandas_output, str) and \"Erro\" in ev.pandas_output:\n",
        "            resposta_final = f\"Não foi possível processar a consulta devido a um erro. {ev.pandas_output}\"\n",
        "        else:\n",
        "            prompt_synthesis = RESPONSE_SYNTHESIS_PROMPT_STR.format(\n",
        "                query_str=ev.query,\n",
        "                pandas_instructions=ev.pandas_code,\n",
        "                pandas_output=str(ev.pandas_output)\n",
        "            )\n",
        "            response = await Settings.llm.acomplete(prompt_synthesis)\n",
        "            resposta_final = str(response).strip()\n",
        "\n",
        "        print(\" [Workflow] Finalizado com sucesso!\")\n",
        "        return StopEvent(result={\n",
        "            \"resposta_final\": resposta_final,\n",
        "            \"pandas_code\": ev.pandas_code,\n",
        "        })\n",
        "\n",
        "print(\"✅ Workflow robusto e simplificado definido!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9 Função de Execução"
      ],
      "metadata": {
        "id": "XUvrzCXQ_KWI"
      },
      "id": "XUvrzCXQ_KWI"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6e04d1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e04d1ba",
        "outputId": "1182824b-6fb1-4509-ff5c-ff51fd24fd98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Função de execução do workflow corrigida: sem fallback e com tratamento de erro explícito!\n"
          ]
        }
      ],
      "source": [
        "async def executar_consulta(query: str, df_local: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Executa o workflow de ponta a ponta e trata os erros de forma explícita, sem fallback.\n",
        "    \"\"\"\n",
        "    if df_local is None:\n",
        "        return {\n",
        "            \"resposta_final\": \"Erro: O DataFrame não foi carregado.\",\n",
        "            \"pandas_code\": \"N/A\"\n",
        "        }\n",
        "    if not query or not query.strip():\n",
        "        return {\n",
        "            \"resposta_final\": \"Erro: A consulta não pode ser vazia.\",\n",
        "            \"pandas_code\": \"N/A\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n Iniciando workflow para a consulta: '{query}'\")\n",
        "        wf = PandasWorkflow()\n",
        "\n",
        "        # O método run() está retornando o dicionário diretamente.\n",
        "        # A correção é tratar o resultado como o dicionário final.\n",
        "        resultado_final = await wf.run(query=query, df=df_local)\n",
        "\n",
        "        # Verificamos se o resultado é de fato um dicionário, como esperado.\n",
        "        if not isinstance(resultado_final, dict):\n",
        "             raise TypeError(f\"O workflow retornou um tipo inesperado: {type(resultado_final)}\")\n",
        "\n",
        "        return resultado_final\n",
        "\n",
        "    except Exception as e:\n",
        "        # SEM FALLBACK: Se o workflow falhar, reportamos o erro diretamente.\n",
        "        print(f\"❌ Erro crítico durante a execução do workflow: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Imprime o stack trace completo para depuração\n",
        "        return {\n",
        "            \"resposta_final\": f\"Ocorreu um erro crítico no workflow. Verifique os logs para detalhes. Erro: {str(e)}\",\n",
        "            \"pandas_code\": \"N/A - Falha no workflow\"\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"✅ Função de execução do workflow corrigida: sem fallback e com tratamento de erro explícito!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10 Gradio"
      ],
      "metadata": {
        "id": "v47UtKN4HHhi"
      },
      "id": "v47UtKN4HHhi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.1 Célula de preparação"
      ],
      "metadata": {
        "id": "wjIzncuUyPFT"
      },
      "id": "wjIzncuUyPFT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define uma variável global para o caminho da fonte\n",
        "FONT_PATH = \"\"\n",
        "FONT_NAME_TO_FIND = \"DejaVu Sans\"\n",
        "\n",
        "try:\n",
        "    # A maneira mais robusta: usar o gerenciador de fontes do Matplotlib\n",
        "    print(f\"Procurando a fonte '{FONT_NAME_TO_FIND}' localmente com Matplotlib...\")\n",
        "    FONT_PATH = fm.findfont(FONT_NAME_TO_FIND, fallback_to_default=True)\n",
        "\n",
        "    if not os.path.exists(FONT_PATH):\n",
        "        raise FileNotFoundError  # Força a entrada no bloco de exceção se o caminho for inválido\n",
        "\n",
        "    print(f\"✅ Fonte encontrada com sucesso em: {FONT_PATH}\")\n",
        "\n",
        "except Exception:\n",
        "    print(f\"⚠️ Fonte '{FONT_NAME_TO_FIND}' não encontrada. Tentando baixar uma alternativa confiável (Roboto)...\")\n",
        "\n",
        "    # --- PLANO B: Se o Matplotlib falhar, baixa uma fonte de um link estável ---\n",
        "    FONT_FILE_NAME = \"Roboto-Regular.ttf\"\n",
        "    FONT_URL = \"https://github.com/google/fonts/raw/main/ofl/roboto/Roboto-Regular.ttf\"\n",
        "\n",
        "    if not os.path.exists(FONT_FILE_NAME):\n",
        "        try:\n",
        "            print(f\"Baixando '{FONT_FILE_NAME}'...\")\n",
        "            response = requests.get(FONT_URL)\n",
        "            response.raise_for_status()\n",
        "            with open(FONT_FILE_NAME, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            FONT_PATH = FONT_FILE_NAME\n",
        "            print(f\"✅ Fonte alternativa baixada com sucesso!\")\n",
        "        except Exception as download_error:\n",
        "            print(f\"❌ FALHA CRÍTICA: Não foi possível encontrar a fonte localmente nem baixá-la. Erro: {download_error}\")\n",
        "            FONT_PATH = \"\"  # Garante que o caminho esteja vazio em caso de falha\n",
        "    else:\n",
        "        FONT_PATH = FONT_FILE_NAME\n",
        "        print(f\"✅ Fonte alternativa '{FONT_FILE_NAME}' já existe localmente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33DLlhFtyTSa",
        "outputId": "5c4d8d1c-c6c6-436b-fe1a-670b5400566e"
      },
      "id": "33DLlhFtyTSa",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procurando a fonte 'DejaVu Sans' localmente com Matplotlib...\n",
            "✅ Fonte encontrada com sucesso em: /usr/local/lib/python3.12/dist-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.2 Funções auxiliares Gradio"
      ],
      "metadata": {
        "id": "-296ewDprziH"
      },
      "id": "-296ewDprziH"
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_dados(file, df_estado):\n",
        "    if file is None:\n",
        "        return \"❌ Nenhum arquivo selecionado\", df_estado\n",
        "    try:\n",
        "        df_local = pd.read_csv(file.name)\n",
        "        return f\"✅ Arquivo carregado! {df_local.shape[0]} linhas, {df_local.shape[1]} colunas.\", df_local\n",
        "    except Exception as e:\n",
        "        return f\"❌ Erro ao carregar arquivo: {str(e)}\", df_estado\n",
        "\n",
        "def processar_pergunta(pergunta: str, df_local: Optional[pd.DataFrame], historico: Historico):\n",
        "    if df_local is None:\n",
        "        resposta = \"❌ Por favor, carregue um arquivo CSV primeiro.\"\n",
        "        return resposta, historico\n",
        "\n",
        "    if not pergunta or not pergunta.strip():\n",
        "        resposta = \"❌ Por favor, digite uma pergunta válida.\"\n",
        "        return resposta, historico\n",
        "\n",
        "    try:\n",
        "        resultado = asyncio.run(executar_consulta(pergunta, df_local))\n",
        "        resposta = resultado.get(\"resposta_final\", \"❌ Erro: a resposta não foi gerada.\")\n",
        "        historico.adicionar(pergunta, resposta)\n",
        "        return resposta, historico\n",
        "    except Exception as e:\n",
        "        erro_msg = f\"❌ Ocorreu um erro inesperado: {str(e)}\"\n",
        "        historico.adicionar(pergunta, erro_msg)\n",
        "        return erro_msg, historico\n",
        "\n",
        "# --- FUNÇÃO GERAR_PDF ---\n",
        "# A variável FONT_PATH é definida na célula de preparação anterior.\n",
        "if 'FONT_PATH' not in globals() or not FONT_PATH:\n",
        "    print(\"❌ ERRO: O caminho da fonte (FONT_PATH) não foi definido. Execute a célula de preparação.\")\n",
        "    FONT_PATH = \"\"\n",
        "\n",
        "def gerar_pdf(historico: Historico):\n",
        "    if not historico or not historico.entradas:\n",
        "        print(\"⚠️ Tentativa de gerar PDF com histórico vazio.\")\n",
        "        return None\n",
        "\n",
        "    # Verifica se o caminho da fonte é válido ANTES de tentar gerar o PDF\n",
        "    if not FONT_PATH or not os.path.exists(FONT_PATH):\n",
        "        msg = f\"❌ Erro ao gerar PDF: Arquivo da fonte não foi encontrado em '{FONT_PATH}'. Execute a célula de preparação.\"\n",
        "        print(msg)\n",
        "        # Opcional: retornar um PDF de erro\n",
        "        return None\n",
        "\n",
        "    print(f\"Gerando PDF com {len(historico.entradas)} entradas usando a fonte: {FONT_PATH}\")\n",
        "\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        caminho_pdf = f\"relatorio_consultas_{timestamp}.pdf\"\n",
        "\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "\n",
        "        # --- MUDANÇA FINAL ---\n",
        "        # Usa o caminho absoluto da fonte encontrado no sistema.\n",
        "        pdf.add_font('DejaVu', '', FONT_PATH, uni=True)\n",
        "        pdf.set_font('DejaVu', '', 10)\n",
        "\n",
        "        pdf.set_font('DejaVu', '', 16)\n",
        "        pdf.cell(0, 10, \"Relatório de Consultas ao CSV\", ln=True, align=\"C\")\n",
        "        pdf.ln(10)\n",
        "\n",
        "        for i, (pergunta, resposta) in enumerate(historico.entradas, 1):\n",
        "            pdf.set_font('DejaVu', '', 12)\n",
        "            pdf.multi_cell(0, 8, f\"Pergunta {i}: {pergunta}\")\n",
        "            pdf.ln(2)\n",
        "            pdf.set_font('DejaVu', '', 10)\n",
        "            pdf.multi_cell(0, 6, f\"Resposta: {resposta}\")\n",
        "            pdf.ln(8)\n",
        "            pdf.cell(0, 0, '', 'T')\n",
        "            pdf.ln(8)\n",
        "\n",
        "        pdf.output(caminho_pdf)\n",
        "        print(f\"✅ PDF gerado com sucesso: {caminho_pdf}\")\n",
        "        return caminho_pdf\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro crítico ao gerar PDF: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "print(\"✅ Funções auxiliares do Gradio prontas para usar fonte local.\")\n",
        "\n",
        "# --- FUNÇÃO EXTRA PARA LIMPAR A INTERFACE ---\n",
        "def limpar_tudo():\n",
        "    # Retorna valores vazios/padrão para limpar os componentes\n",
        "    return \"\", None, Historico()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CC_I9Rlr_YF",
        "outputId": "4768691c-88a2-4dd3-ce84-930297f1f701"
      },
      "id": "7CC_I9Rlr_YF",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Funções auxiliares do Gradio prontas para usar fonte local.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.3 Interface Gradio"
      ],
      "metadata": {
        "id": "10D8zK05iSbj"
      },
      "id": "10D8zK05iSbj"
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"Analisador de CSV com Pandas e LlamaIndex\", theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center;\">\n",
        "            <h1>🐼 Analisador de CSV com Pandas e LlamaIndex</h1>\n",
        "            <p>Faça upload de um arquivo CSV, faça perguntas em linguagem natural e baixe o relatório!</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Estados\n",
        "    df_estado = gr.State()\n",
        "    historico_estado = gr.State(value=Historico())\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            input_arquivo = gr.File(label=\"1. Faça upload do seu arquivo CSV\", file_types=[\".csv\"])\n",
        "            upload_status = gr.Textbox(label=\"Status do Upload\", interactive=False, placeholder=\"Aguardando arquivo...\")\n",
        "            botao_gerar_pdf = gr.Button(\"📄 Gerar e Baixar PDF\", variant=\"secondary\")\n",
        "            output_pdf = gr.File(label=\"Baixar Relatório\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            # --- Componente Chatbot ---\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Histórico da Conversa\",\n",
        "                height=550,\n",
        "                type='messages'\n",
        "            )\n",
        "            with gr.Row():\n",
        "                input_pergunta = gr.Textbox(\n",
        "                    scale=4,\n",
        "                    show_label=False,\n",
        "                    placeholder=\"Ex: Qual o total de vendas por filial?\",\n",
        "                    container=False\n",
        "                )\n",
        "                botao_enviar = gr.Button(\" Enviar\", variant=\"primary\", scale=1, min_width=100)\n",
        "\n",
        "    # --- Lógica da Interface ---\n",
        "\n",
        "    def chat_interface(pergunta, chat_history, df_local, historico_obj):\n",
        "        # O chat_history agora é uma lista de dicionários.\n",
        "        # Adiciona a mensagem do usuário ao histórico no formato correto.\n",
        "        chat_history.append({\"role\": \"user\", \"content\": pergunta})\n",
        "\n",
        "        if df_local is None:\n",
        "            resposta = \"❌ Por favor, carregue um arquivo CSV primeiro.\"\n",
        "            # Adiciona a resposta do assistente ao histórico.\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": resposta})\n",
        "            return \"\", chat_history, historico_obj\n",
        "\n",
        "        # Chama a função de processamento principal, que não precisa mudar.\n",
        "        resposta, historico_obj_atualizado = processar_pergunta(pergunta, df_local, historico_obj)\n",
        "\n",
        "        # Adiciona a resposta do assistente ao histórico .\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": resposta})\n",
        "\n",
        "        return \"\", chat_history, historico_obj_atualizado\n",
        "\n",
        "    # Função para limpar os estados\n",
        "    def limpar_conversa():\n",
        "        return Historico(), None\n",
        "\n",
        "    # --- Conexões dos Eventos ---\n",
        "    input_arquivo.upload(fn=carregar_dados, inputs=[input_arquivo, df_estado], outputs=[upload_status, df_estado])\n",
        "\n",
        "    botao_enviar.click(\n",
        "        fn=chat_interface,\n",
        "        inputs=[input_pergunta, chatbot, df_estado, historico_estado],\n",
        "        outputs=[input_pergunta, chatbot, historico_estado]\n",
        "    )\n",
        "    input_pergunta.submit(\n",
        "        fn=chat_interface,\n",
        "        inputs=[input_pergunta, chatbot, df_estado, historico_estado],\n",
        "        outputs=[input_pergunta, chatbot, historico_estado]\n",
        "    )\n",
        "\n",
        "    botao_gerar_pdf.click(fn=gerar_pdf, inputs=[historico_estado], outputs=[output_pdf])\n",
        "\n",
        "    chatbot.clear(fn=limpar_conversa, inputs=[], outputs=[historico_estado, output_pdf])\n",
        "\n",
        "print(\"✅ Interface Gradio finalizada!\")\n",
        "\n",
        "# Lança a interface\n",
        "if __name__ == \"__main__\":\n",
        "    if df is not None:\n",
        "        app.launch(debug=True)\n",
        "    else:\n",
        "        print(\"🔴 A interface Gradio não será iniciada pois a configuração da API falhou.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "xBhCifgHiU9r",
        "outputId": "d5d4de68-96d3-4fc3-deeb-98d46d914bcc"
      },
      "id": "xBhCifgHiU9r",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Interface Gradio finalizada, moderna e sem warnings!\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ef5393479a158f0e87.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ef5393479a158f0e87.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ef5393479a158f0e87.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11 Teste Rápido"
      ],
      "metadata": {
        "id": "SnrigB_l_QVm"
      },
      "id": "SnrigB_l_QVm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste rápido para verificar se a função principal está funcionando\n",
        "async def teste_rapido():\n",
        "    if df is not None:\n",
        "        query_teste = \"Qual é a avaliação média de cada filial?\"\n",
        "        # Corrigido: passando o df_local como argumento\n",
        "        resultado = await executar_consulta(query_teste, df_local=df)\n",
        "\n",
        "        if resultado and \"Erro\" not in resultado.get(\"resposta_final\", \"\"):\n",
        "            print(\"\\n✅ Sistema funcionando perfeitamente!\")\n",
        "            print(f\"Resposta: {resultado['resposta_final']}\")\n",
        "        else:\n",
        "            print(f\"\\n❌ Houve um problema no teste: {resultado}\")\n",
        "    else:\n",
        "        print(\"⚠️ Teste não executado: DataFrame não foi carregado.\")\n",
        "\n",
        "# Executar o teste\n",
        "await teste_rapido()"
      ],
      "metadata": {
        "id": "mGuURyz__TPK"
      },
      "id": "mGuURyz__TPK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12 Exemplos de Uso Individual"
      ],
      "metadata": {
        "id": "w7XxG1eC_Wk5"
      },
      "id": "w7XxG1eC_Wk5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f8a50d",
      "metadata": {
        "id": "69f8a50d"
      },
      "outputs": [],
      "source": [
        "# Execute uma consulta por vez para testar\n",
        "\n",
        "# Exemplo 1\n",
        "query = \"Quais são os 5 produtos mais vendidos?\"\n",
        "result = await executar_consulta(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 2\n",
        "query = \"Qual o total de vendas por região?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "cLYZHwUK_ZwW"
      },
      "id": "cLYZHwUK_ZwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 3\n",
        "query = \"Qual produto tem a maior margem de lucro?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "2rGJQRL__g-k"
      },
      "id": "2rGJQRL__g-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 4\n",
        "query = \"Quantas vendas foram feitas em cada mês?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "7GmSKrxt_ia9"
      },
      "id": "7GmSKrxt_ia9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Qual o genêro que mais consome e o segmento do produto?\"\n",
        "result = await executar_consulta(query)"
      ],
      "metadata": {
        "id": "QeXUbCZaOY2l"
      },
      "id": "QeXUbCZaOY2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13 Bateria de Testes"
      ],
      "metadata": {
        "id": "7gieDaX2_e2n"
      },
      "id": "7gieDaX2_e2n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Executar múltiplas consultas de uma vez\n",
        "consultas_exemplo = [\n",
        "    \"Qual é a avaliação média de cada filial?\",\n",
        "    \"Quais são os 5 produtos mais vendidos?\",\n",
        "    \"Qual o total de vendas por região?\",\n",
        "    \"Qual produto tem a maior margem de lucro?\",\n",
        "    \"Quantas vendas foram feitas em cada mês?\",\n",
        "    \"Qual é a receita total por categoria de produto?\",\n",
        "    \"Qual filial teve melhor desempenho?\",\n",
        "    \"Mostre a distribuição de preços dos produtos\"\n",
        "]\n",
        "\n",
        "print(\" Executando bateria de testes...\")\n",
        "resultados = await executar_multiplas_consultas(consultas_exemplo)\n",
        "\n",
        "print(f\"\\n✅ Concluído! {len([r for r in resultados if r])} consultas processadas com sucesso.\")"
      ],
      "metadata": {
        "id": "7t268z2a_rK5"
      },
      "id": "7t268z2a_rK5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14 Modo Interativo S/ interface Gradio"
      ],
      "metadata": {
        "id": "YU3bIuXj_ty_"
      },
      "id": "YU3bIuXj_ty_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Modo interativo - digite suas próprias consultas\n",
        "print(\" MODO INTERATIVO\")\n",
        "print(\"Digite suas consultas (digite 'sair' para terminar)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"\\n Sua consulta: \").strip()\n",
        "\n",
        "        if query.lower() in ['sair', 'exit', 'quit', '']:\n",
        "            print(\" Até logo!\")\n",
        "            break\n",
        "\n",
        "        await executar_consulta(query, mostrar_detalhes=False)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n Interrompido pelo usuário!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\" Erro: {e}\")"
      ],
      "metadata": {
        "id": "DXM0Gh67_oiY"
      },
      "id": "DXM0Gh67_oiY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##15 Análise Manual dos Dados\n"
      ],
      "metadata": {
        "id": "YvXLkuzD_yrz"
      },
      "id": "YvXLkuzD_yrz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise dos dados originais para comparação\n",
        "print(\" ANÁLISE MANUAL DOS DADOS (para comparação)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Estatísticas básicas:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(f\"\\nContagem por categoria:\")\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].value_counts().head())\n",
        "\n",
        "print(f\"\\nValores nulos:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "ohovTGoP_7tG"
      },
      "id": "ohovTGoP_7tG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dicas e Informações"
      ],
      "metadata": {
        "id": "ETccYDWS_4Zc"
      },
      "id": "ETccYDWS_4Zc"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"💡 DICAS PARA USO:\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. Para consultas complexas, seja específico\")\n",
        "print(\"2. Use nomes exatos das colunas mostradas na análise\")\n",
        "print(\"3. Experimente diferentes tipos de análise:\")\n",
        "print(\"   - Agrupamentos: 'média por categoria'\")\n",
        "print(\"   - Rankings: 'top 10 produtos'\")\n",
        "print(\"   - Filtros: 'vendas acima de X valor'\")\n",
        "print(\"   - Séries temporais: 'vendas por mês'\")\n",
        "print(\"4. O sistema mostra o código Pandas usado!\")\n",
        "\n",
        "print(f\"\\n INFORMAÇÕES DO DATASET:\")\n",
        "print(f\"Suas colunas: {list(df.columns)}\")\n",
        "print(f\"Quantidade de dados: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
        "print(f\"Tipos de dados: {df.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "print(f\"\\n SUGESTÕES DE CONSULTAS:\")\n",
        "example_queries = [\n",
        "    \"Qual a correlação entre preço e quantidade vendida?\",\n",
        "    \"Mostre a distribuição de vendas por dia da semana\",\n",
        "    \"Qual filial tem o melhor desempenho?\",\n",
        "    \"Compare as vendas do primeiro e último trimestre\",\n",
        "    \"Identifique produtos com baixo estoque\",\n",
        "    \"Qual é o produto mais caro por categoria?\",\n",
        "    \"Mostre as vendas médias por vendedor\",\n",
        "    \"Qual região tem menor variação de preços?\"\n",
        "]\n",
        "\n",
        "for i, query in enumerate(example_queries, 1):\n",
        "    print(f\"{i}. {query}\")\n",
        "\n",
        "print(f\"\\n MODELO UTILIZADO: {config.model}\")\n",
        "print(f\" URL DOS DADOS: {config.data_url}\")"
      ],
      "metadata": {
        "id": "lEPP6tuj_925"
      },
      "id": "lEPP6tuj_925",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notas de Uso:\n",
        "\n",
        "*   Configuração da API: Configure sua chave Groq nos Secrets do Colab\n",
        "*   Execução sequencial: Execute as células na ordem\n",
        "*   Personalização: Modifique a URL dos dados na Célula 6\n",
        "*   Debug: Os logs mostram o código Pandas gerado\n",
        "*   Interativo: Use a Célula 12 para consultas livres\n",
        "\n",
        "##Estrutura final:\n",
        "\n",
        "*   Instalação automática de dependências\n",
        "* Configuração completa da API\n",
        "* Exploração automática dos dados\n",
        "* Sistema de workflow robusto\n",
        "* Exemplos prontos para uso\n",
        "* Modo interativo\n",
        "* Sistema de debug integrado\n",
        "\n",
        "\n",
        ">Desenvolvido por Yuri Arduino Bernardineli Alves | GitHub: [YuriArduino](https://github.com/YuriArduino) | E-mail: yuriarduino@gmail.com"
      ],
      "metadata": {
        "id": "_TbE6Un3AGjs"
      },
      "id": "_TbE6Un3AGjs"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}