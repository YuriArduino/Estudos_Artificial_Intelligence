{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlo8aYBAKtM7Dd8KNnLv3Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-index==0.10.37"
      ],
      "metadata": {
        "id": "N-AjpoONaaGE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8sCFAtLCTVF",
        "outputId": "62d6b8df-059b-4dae-8cef-24dbd3e36e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install llama-index-llms-openai llama-index-llms-groq llama-index-experimental gradio fpdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.experimental.query_engine import PandasQueryEngine\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "\n",
        "# ===== CONFIGURA√á√ÉO COM PYDANTIC V2 =====\n",
        "class LLMConfig(BaseModel):\n",
        "    model: str = Field(..., description=\"Nome do modelo Groq a ser usado\")\n",
        "    api_key: str = Field(..., description=\"Chave da API Groq\")\n",
        "    data_url: str = Field(..., description=\"URL do CSV com os dados\")\n",
        "\n",
        "    @field_validator(\"data_url\")\n",
        "    @classmethod\n",
        "    def validar_url(cls, v: str) -> str:\n",
        "        if not (v.startswith(\"http://\") or v.startswith(\"https://\")):\n",
        "            raise ValueError(\"data_url deve come√ßar com http:// ou https://\")\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"api_key\")\n",
        "    @classmethod\n",
        "    def validar_api_key(cls, v: str) -> str:\n",
        "        if not v or len(v.strip()) == 0:\n",
        "            raise ValueError(\"api_key n√£o pode ser vazia\")\n",
        "        return v\n",
        "\n",
        "    model_config = {\n",
        "        \"extra\": \"allow\",\n",
        "        \"json_schema_extra\": {\n",
        "            \"example\": {\n",
        "                \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                \"api_key\": \"<SUA_CHAVE_AQUI>\",\n",
        "                \"data_url\": \"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ===== Fun√ß√£o para formatar texto =====\n",
        "def formatar_texto(response, largura: int = 100, imprimir: bool = True):\n",
        "    texto = response.response\n",
        "    texto_formatado = textwrap.fill(texto, width=largura)\n",
        "    if imprimir:\n",
        "        print(texto_formatado)\n",
        "    return texto_formatado\n",
        "\n",
        "# ===== INICIALIZA√á√ÉO =====\n",
        "# Obter a chave via Colab userdata\n",
        "key = userdata.get('Groq_API')\n",
        "\n",
        "# Configurar\n",
        "config = LLMConfig(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    api_key=key,\n",
        "    data_url=\"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        ")\n",
        "\n",
        "# Carregar CSV\n",
        "df = pd.read_csv(config.data_url)\n",
        "\n",
        "# Inicializar LLM via Settings\n",
        "Settings.llm = Groq(model=config.model, api_key=config.api_key)\n",
        "\n",
        "# Criar engine de consulta (somente aqui, depois do df estar pronto)\n",
        "query_engine = PandasQueryEngine(df=df, verbose=True, synthesize_response=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2IbtC1eC0Ys",
        "outputId": "76a2c211-6579-4cd4-b875-f2b99d5eaf29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /usr/local/lib/python3.12/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modularidade\n",
        "\n",
        "Antes de criar o pipeline em si, voc√™ precisa definir alguns m√≥dulos.\n",
        "No primeiro, temos algumas instru√ß√µes do que √© para ser feito com os dados. Depois, temos o Pandas Prompt, com informa√ß√µes sobre o DataFrame que estamos trabalhando. Por fim, temos um prompt de respostas, onde vai sintetizar a resposta que ser√° gerada no final.\n",
        "\n",
        "Mas, vamos entender o que precisamos fazer. Se estamos trabalhando com dados em portugu√™s e queremos que o modelo s√≥ GAranta um bom resultado em portugu√™s, a primeira coisa que precisamos fazer √© alterar os textos todos para portugu√™s. Dessa forma n√£o haver√° confus√£o, o modelo n√£o se confundir√° e responder√° em ingl√™s, como aconteceu na aula anterior."
      ],
      "metadata": {
        "id": "p1QufimDFv8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.experimental.query_engine.pandas import PandasInstructionParser"
      ],
      "metadata": {
        "id": "qqfCpxtcFo4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para obter uma descri√ß√£o das colunas do DataFrame\n",
        "def descricao_colunas(df):\n",
        "  descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "  return 'Aqui est√£o os detalhes das colunas do DataFrame:\\n' + descricao\n",
        "\n",
        "\n",
        "# Instru√ß√µes para orientar o modelo a converter uma consulta em linguagem natural em c√≥digo Python execut√°vel com a biblioteca Pandas\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para c√≥digo Python execut√°vel usando Pandas.\\n\"\n",
        "    \"2. A linha final do c√≥digo deve ser uma express√£o Python que possa ser chamada com a fun√ß√£o `eval()`.\\n\"\n",
        "    \"3. O c√≥digo deve representar uma solu√ß√£o para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESS√ÉO.\\n\"\n",
        "    \"5. N√£o coloque a express√£o entre aspas.\\n\")\n",
        "\n",
        "# Prompt que ser√° enviado ao modelo para que ela gere o c√≥digo Pandas desejado\n",
        "pandas_prompt_str = (\n",
        "    \"Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este √© o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instru√ß√µes:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Express√£o:\"\n",
        ")\n",
        "\n",
        "# Prompt para guiar o modelo a sintetizar uma resposta com base nos resultados obtidos pela consulta Pandas\n",
        "response_synthesis_prompt_str = (\n",
        "   \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "   \"Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante.\\n\"\n",
        "   \"Consulta: {query_str}\\n\\n\"\n",
        "   \"Instru√ß√µes do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "   \"Sa√≠da do Pandas: {pandas_output}\\n\\n\"\n",
        "   \"Resposta:\"\n",
        "   \"Ao final, exibir o c√≥digo usado para gerar a resposta, no formato: O c√≥digo utilizado foi {pandas_instructions}\"\n",
        ")\n",
        "\n",
        "# M√≥dulo para obter as instru√ß√µes Pandas\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str, colunas_detalhes=descricao_colunas(df), df_str=df.head(5)\n",
        ")\n",
        "# M√≥dulo para executar as instru√ß√µes Pandas\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "\n",
        "# M√≥dulo para sintetizar a resposta\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "# Modelo\n",
        "#llm = Groq(model='llama3-70b-8192', api_key=key)\n",
        "#Carreguei um mais avan√ßado"
      ],
      "metadata": {
        "id": "9DOoXQA5FxrF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fun√ß√£o eval() e instru√ß√µes do modelo\n",
        "\n",
        "`eval()` √© uma fun√ß√£o Python que serve basicamente para executar um c√≥digo que esteja dentro de uma string. Ou seja, se temos um c√≥digo dentro de um texto, vai peg√°-lo e executar diretamente.\n",
        "\n",
        "O c√≥digo deve representar uma solu√ß√£o para a consulta. Ent√£o, se a pessoa fez uma pergunta, esse c√≥digo vai ter que encontrar uma solu√ß√£o usando um c√≥digo da biblioteca Pandas. Ap√≥s, √© definido que seja impresso apenas a express√£o, que deve ser uma coisa importante de ser feita. Ele tamb√©m pede para n√£o colocar a express√£o entre aspas no final, pois isso pode atrapalhar a execu√ß√£o da fun√ß√£o eval().\n",
        "\n",
        "Depois temos o pandas_prompt_str. Este √© o prompt que ser√° enviado para o modelo para que gere o c√≥digo Pandas desejado. Nele, informamos ao modelo que est√° trabalhando com o dataframe do Pandas em Python chamado df, esse √© o resultado do print df.head(). Repare que come√ßa mostrando quais s√£o as cinco primeiras linhas do arquivo para o modelo j√° ter um contexto. A partir disso, conseguimos fazer perguntas com maior facilidade.\n",
        "\n",
        "Ap√≥s, traz o df_str, que √© uma vari√°vel que est√° abaixo, que √© basicamente o df.head() para pegar as cinco primeiras linhas. Nisso, indica seguir as instru√ß√µes. Em seguida, temos a Consulta {query str}, que √© a consulta que faremos para o pipeline. E temos a express√£o sendo gerada.\n",
        "\n",
        "Por fim, temos esse response_synthesis_prompt_str, que serve para guiar o modelo a sintetizar uma resposta com base no resultado que foi obtido pela nossa consulta Pandas. Ent√£o, ele indica que \"Dada uma pergunta de entrada, elabore uma resposta a partir dos resultados da consulta\".\n",
        "\n",
        "Ap√≥s, traz qual foi a consulta feita, que √© query_str. Tamb√©m ter√° as instru√ß√µes do Pandas que ser√£o usadas para gerar a resposta, que √© o pandas_instructions. Depois, ele tem essas instru√ß√µes do Pandas, que s√£o as instru√ß√µes que ser√£o usadas para chegar no resultado. Temos tamb√©m a sa√≠da do Pandas, que √© o pandas_output e a resposta."
      ],
      "metadata": {
        "id": "QoD2KK2tFS4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Para saber mais: modificando os prompts do pipeline\n",
        "\n",
        "A capacidade de **customizar como as consultas s√£o processadas e as respostas apresentadas** √© essencial para atender √†s necessidades espec√≠ficas dos usu√°rios, aumentando a efici√™ncia e a relev√¢ncia dos resultados.\n",
        "\n",
        "---\n",
        "\n",
        "## Estrutura do pipeline\n",
        "\n",
        "A primeira etapa antes de criar o pipeline √© definir **3 m√≥dulos principais**:\n",
        "\n",
        "1. **pandas\\_prompt**\n",
        "\n",
        "   * Traduz consultas em linguagem natural para comandos espec√≠ficos do Pandas.\n",
        "   * Facilita a manipula√ß√£o direta dos dados.\n",
        "\n",
        "2. **pandas\\_output\\_parser**\n",
        "\n",
        "   * Executa os comandos gerados.\n",
        "   * Aplica-os diretamente ao DataFrame para processar as informa√ß√µes.\n",
        "\n",
        "3. **response\\_synthesis\\_prompt**\n",
        "\n",
        "   * Elabora uma resposta clara e informativa com base nos resultados obtidos.\n",
        "   * Essa resposta √© a que ser√° apresentada ao usu√°rio.\n",
        "\n",
        "---\n",
        "\n",
        "## C√≥digo base\n",
        "\n",
        "```python\n",
        "# M√≥dulo pandas_prompt\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str,\n",
        "    df_str=df.head(5),\n",
        "    colunas_detalhes=descricao_colunas(df)\n",
        ")\n",
        "\n",
        "# M√≥dulo pandas_output_parser\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "\n",
        "# M√≥dulo response_synthesis_prompt\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "# LLM\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=key)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Componentes essenciais\n",
        "\n",
        "### 1. Instruction String\n",
        "\n",
        "√â o conjunto de instru√ß√µes que orienta o modelo a converter consultas em linguagem natural para **c√≥digo Python execut√°vel** utilizando Pandas.\n",
        "\n",
        "üîë Requisito importante: o c√≥digo final deve ser uma express√£o Python que possa ser chamada com `eval()`.\n",
        "\n",
        "```python\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para c√≥digo Python execut√°vel usando Pandas.\\n\"\n",
        "    \"2. A linha final do c√≥digo deve ser uma express√£o Python que possa ser chamada com a fun√ß√£o `eval()`.\\n\"\n",
        "    \"3. O c√≥digo deve representar uma solu√ß√£o para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESS√ÉO.\\n\"\n",
        "    \"5. N√£o coloque a express√£o entre aspas.\\n\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Pandas Prompt String\n",
        "\n",
        "Esse prompt √© enviado ao modelo para gerar o c√≥digo Pandas desejado.\n",
        "\n",
        "Inclui:\n",
        "\n",
        "* As **instru√ß√µes detalhadas**.\n",
        "* Uma **vis√£o geral do DataFrame** (`df.head(5)`).\n",
        "* **Detalhes das colunas** para dar mais contexto ao modelo.\n",
        "\n",
        "Fun√ß√£o auxiliar para descrever colunas:\n",
        "\n",
        "```python\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return 'Aqui est√£o os detalhes das colunas do DataFrame:\\n' + descricao\n",
        "```\n",
        "\n",
        "Prompt traduzido e ajustado:\n",
        "\n",
        "```python\n",
        "pandas_prompt_str = (\n",
        "    \"Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este √© o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instru√ß√µes:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Express√£o:\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Response Synthesis Prompt String\n",
        "\n",
        "Respons√°vel por guiar o modelo a sintetizar uma resposta explicativa, de forma natural, **atuando como um analista de dados**.\n",
        "\n",
        "Inclui:\n",
        "\n",
        "* Pergunta feita.\n",
        "* C√≥digo Pandas utilizado.\n",
        "* Sa√≠da obtida.\n",
        "* Resposta em texto natural.\n",
        "\n",
        "```python\n",
        "response_synthesis_prompt_str = (\n",
        "    \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "    \"Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante.\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Instru√ß√µes do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "    \"Sa√≠da do Pandas: {pandas_output}\\n\\n\"\n",
        "    \"Resposta: \\n\\n\"\n",
        "    \"Ao final, exibir o c√≥digo usado em para gerar a resposta, no formato: O c√≥digo utilizado foi `{pandas_instructions}`\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Agora que os m√≥dulos est√£o definidos, o pr√≥ximo passo √© **construir o pipeline completo**.\n",
        "\n",
        "---\n",
        "\n",
        "##Construindo o pipeline de consulta\n",
        "\n",
        "###Estruturando o pipeline"
      ],
      "metadata": {
        "id": "U_5WBmdrKhwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from llama_index.core import Settings, PromptTemplate\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.experimental.query_engine import PandasQueryEngine\n",
        "from llama_index.core.query_pipeline import QueryPipeline as QP, Link, InputComponent\n",
        "from llama_index.experimental.query_engine.pandas import PandasInstructionParser\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "\n",
        "# ===== CONFIGURA√á√ÉO COM PYDANTIC V2 =====\n",
        "class LLMConfig(BaseModel):\n",
        "    model: str = Field(..., description=\"Nome do modelo Groq a ser usado\")\n",
        "    api_key: str = Field(..., description=\"Chave da API Groq\")\n",
        "    data_url: str = Field(..., description=\"URL do CSV com os dados\")\n",
        "\n",
        "    @field_validator(\"data_url\")\n",
        "    @classmethod\n",
        "    def validar_url(cls, v: str) -> str:\n",
        "        if not (v.startswith(\"http://\") or v.startswith(\"https://\")):\n",
        "            raise ValueError(\"data_url deve come√ßar com http:// ou https://\")\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"api_key\")\n",
        "    @classmethod\n",
        "    def validar_api_key(cls, v: str) -> str:\n",
        "        if not v or len(v.strip()) == 0:\n",
        "            raise ValueError(\"api_key n√£o pode ser vazia\")\n",
        "        return v\n",
        "\n",
        "    model_config = {\n",
        "        \"extra\": \"allow\",\n",
        "        \"json_schema_extra\": {\n",
        "            \"example\": {\n",
        "                \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                \"api_key\": \"<SUA_CHAVE_AQUI>\",\n",
        "                \"data_url\": \"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ===== Fun√ß√£o para formatar texto =====\n",
        "def formatar_texto(response, largura: int = 100, imprimir: bool = True):\n",
        "    texto = response.response if hasattr(response, \"response\") else str(response)\n",
        "    texto_formatado = textwrap.fill(texto, width=largura)\n",
        "    if imprimir:\n",
        "        print(texto_formatado)\n",
        "    return texto_formatado\n",
        "\n",
        "# ===== INICIALIZA√á√ÉO =====\n",
        "# Obter a chave via Colab userdata\n",
        "key = userdata.get('Groq_API')\n",
        "\n",
        "# Configurar\n",
        "config = LLMConfig(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    api_key=key,\n",
        "    data_url=\"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        ")\n",
        "\n",
        "# Carregar CSV\n",
        "df = pd.read_csv(config.data_url)\n",
        "\n",
        "# Inicializar LLM via Settings\n",
        "Settings.llm = Groq(model=config.model, api_key=config.api_key)\n",
        "\n",
        "# ===== PROMPTS =====\n",
        "# Instru√ß√µes para orientar o modelo\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para c√≥digo Python execut√°vel usando Pandas.\\n\"\n",
        "    \"2. A linha final do c√≥digo deve ser uma express√£o Python que possa ser chamada com a fun√ß√£o `eval()`.\\n\"\n",
        "    \"3. O c√≥digo deve representar uma solu√ß√£o para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESS√ÉO.\\n\"\n",
        "    \"5. N√£o coloque a express√£o entre aspas.\\n\"\n",
        ")\n",
        "\n",
        "# Fun√ß√£o para descri√ß√£o das colunas\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return 'Aqui est√£o os detalhes das colunas do DataFrame:\\n' + descricao\n",
        "\n",
        "# Prompt Pandas\n",
        "pandas_prompt_str = (\n",
        "    \"Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este √© o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instru√ß√µes:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Express√£o:\"\n",
        ")\n",
        "\n",
        "# Prompt de Resposta R√°pida\n",
        "response_synthesis_prompt_str = (\n",
        "   \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "   \"Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante.\\n\"\n",
        "   \"Consulta: {query_str}\\n\\n\"\n",
        "   \"Instru√ß√µes do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "   \"Sa√≠da do Pandas: {pandas_output}\\n\\n\"\n",
        "   \"Resposta:\\n\"\n",
        "   \"Ao final, exibir o c√≥digo usado para gerar a resposta, no formato: O c√≥digo utilizado foi {pandas_instructions}\"\n",
        ")\n",
        "\n",
        "# M√≥dulo para obter as instru√ß√µes Pandas\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str, colunas_detalhes=descricao_colunas(df), df_str=df.head(5)\n",
        ")\n",
        "\n",
        "# M√≥dulo para executar as instru√ß√µes Pandas\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "\n",
        "# M√≥dulo para sintetizar a resposta\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "\n",
        "# ===== DEFINI√á√ÉO DOS NODES (usando QueryPipeline) =====\n",
        "# Os nodes s√£o representados pelos pr√≥prios componentes no QueryPipeline\n",
        "\n",
        "# Criar o QueryPipeline\n",
        "qp = QP(verbose=True)\n",
        "\n",
        "# Adicionar modulos\n",
        "qp.add_modules({\n",
        "    \"input\": InputComponent(),\n",
        "    \"pandas_prompt\": pandas_prompt,\n",
        "    \"llm\": Settings.llm,\n",
        "    \"pandas_output_parser\": pandas_output_parser,\n",
        "    \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "    \"response_synthesizer\": Settings.llm, # Use the LLM to synthesize the response\n",
        "})\n",
        "\n",
        "# Adicionar Links\n",
        "qp.add_links(\n",
        "    [\n",
        "        Link(\"input\", \"pandas_prompt\"),\n",
        "        Link(\"pandas_prompt\", \"llm\"),\n",
        "        Link(\"llm\", \"pandas_output_parser\"),\n",
        "        Link(\"pandas_output_parser\", \"response_synthesis_prompt\", dest_key=\"pandas_output\"),\n",
        "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
        "        Link(\"llm\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"), # Link LLM output (the code)\n",
        "        Link(\"response_synthesis_prompt\", \"response_synthesizer\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "zdVXkf3hf8Ij"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Realizando consultas com a pipeline\n",
        "\n",
        "Criamos uma vari√°vel chamada response e atribu√≠mos um valor a ela. O processo ser√° um pouco diferente, pois, ao inv√©s de utilizar o Pandas Query Engine, estamos trabalhando diretamente com o pipeline.\n",
        "\n",
        "Para isso, usaremos a vari√°vel `qp`, que definimos anteriormente. A execu√ß√£o do pipeline ser√° feita por meio de `qp.run()`, passando como par√¢metro o valor de query_str, que j√° definimos em etapas anteriores. Esse par√¢metro √© o destino do nosso primeiro link, presente tanto no prompt de resposta quanto no prompt da biblioteca Pandas.\n",
        "\n",
        "Vamos perguntar: \"Qual √© a m√©dia gasta por cada tipo de cliente?\" Temos clientes que s√£o membros da nossa rede de varejo e clientes que n√£o s√£o membros. Ser√° que um gasta mais que o outro, em m√©dia?"
      ],
      "metadata": {
        "id": "cCqXnOGX6d-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== EXECU√á√ÉO =====\n",
        "response = qp.run(query_str=\"Qual √© a m√©dia gasta por cada tipo de cliente?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-XqwVNE6ayF",
        "outputId": "7d1573f3-1c85-4d50-c957-caf90f2bca54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
            "query_str: Qual √© a m√©dia gasta por cada tipo de cliente?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
            "query_str: Qual √© a m√©dia gasta por cada tipo de cliente?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\n",
            "Aqui est√£o os detalhes das colunas do DataFrame:\n",
            "`ID_compra`: object\n",
            "`filial`: object\n",
            "`cidade`: object\n",
            "`tipo_cliente`: object\n",
            "`...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
            "input: assistant: df.groupby('tipo_cliente')['total'].mean()\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
            "query_str: Qual √© a m√©dia gasta por cada tipo de cliente?\n",
            "pandas_instructions: assistant: df.groupby('tipo_cliente')['total'].mean()\n",
            "pandas_output: tipo_cliente\n",
            "Membro    327.791305\n",
            "Normal    318.122856\n",
            "Name: total, dtype: float64\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesizer with input: \n",
            "messages: Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\n",
            "Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante...\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Formatando a resposta\n",
        "\n",
        "\n",
        "Na pr√≥xima c√©lula, formatamos a resposta usando o m√≥dulo textwrap para melhorar a apresenta√ß√£o. Iniciamos criando uma vari√°vel chamada texto que ser√° igual a response.message.content para obter a resposta.\n",
        "\n",
        "Para formatar o texto, criamos uma vari√°vel texto_formatado que ser√° igual a textwrap.fill(texto, width=100), sendo o width a largura. Por fim, exibimos o texto_formatado utilizando o print():"
      ],
      "metadata": {
        "id": "HgwF0W9k7baY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = response.message.content\n",
        "texto_formatado = textwrap.fill(texto, width=100)\n",
        "print(texto_formatado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlVaqPRE7ihb",
        "outputId": "c006c1f2-e186-4580-b1a1-bedb7b1d7ec4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A m√©dia de gastos por tipo de cliente varia ligeiramente entre as categorias. Os clientes \"Membro\"\n",
            "apresentam uma m√©dia de gasto de aproximadamente R$ 327,79, enquanto os clientes \"Normal\" gastam em\n",
            "m√©dia R$ 318,12. Isso sugere que os membros tendem a gastar um pouco mais em compara√ß√£o com os\n",
            "clientes normais.  O c√≥digo utilizado foi df.groupby('tipo_cliente')['total'].mean()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O c√≥digo utilizado foi df.groupby('tipo_cliente')['total'].mean(). Isso √© √∫til para a equipe da Zoop, pois al√©m da resposta, podemos verificar se est√° tudo certo.\n",
        "\n",
        "Como estamos utilizando uma LLM, podemos solicitar que ela forne√ßa um insight sobre as diferen√ßas de pre√ßo. Podemos testar outra consulta: \"Por que clientes tipo membro t√™m maior m√©dia de gasto?\" utilizando a vari√°vel response que ser√° igual a qp.run(query_str=\"\") e passamos a pergunta:"
      ],
      "metadata": {
        "id": "iyLMBG_i7zdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = qp.run(query_str=\"Por que clientes do tipo membro tem maior m√©dia de gasto?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vppjh6qa7yks",
        "outputId": "0380d5b0-ecc7-4705-f34a-0cd0fc00a185"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
            "query_str: Por que clientes do tipo membro tem maior m√©dia de gasto?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
            "query_str: Por que clientes do tipo membro tem maior m√©dia de gasto?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\n",
            "Aqui est√£o os detalhes das colunas do DataFrame:\n",
            "`ID_compra`: object\n",
            "`filial`: object\n",
            "`cidade`: object\n",
            "`tipo_cliente`: object\n",
            "`...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
            "input: assistant: df.groupby('tipo_cliente')['total'].mean().sort_values(ascending=False)\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
            "query_str: Por que clientes do tipo membro tem maior m√©dia de gasto?\n",
            "pandas_instructions: assistant: df.groupby('tipo_cliente')['total'].mean().sort_values(ascending=False)\n",
            "pandas_output: tipo_cliente\n",
            "Membro    327.791305\n",
            "Normal    318.122856\n",
            "Name: total, dtype: float64\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesizer with input: \n",
            "messages: Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\n",
            "Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante...\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos exibir a resposta como fizemos anteriormente com o seguinte c√≥digo:"
      ],
      "metadata": {
        "id": "iQsTrvcg74HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = response.message.content\n",
        "texto_formatado = textwrap.fill(texto, width=100)\n",
        "print(texto_formatado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryilCoYO749K",
        "outputId": "2ef50428-9597-4547-a9d7-5657d8fd0bed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clientes do tipo membro apresentam uma m√©dia de gasto maior em compara√ß√£o com clientes do tipo\n",
            "normal. De acordo com os dados, a m√©dia de gasto para clientes membros √© de aproximadamente R$\n",
            "327,79, enquanto para clientes normais essa m√©dia fica em torno de R$ 318,12.   Essa diferen√ßa pode\n",
            "ser resultado de v√°rios fatores, como programas de fidelidade que incentivam membros a realizar\n",
            "compras mais frequentes ou de maior valor, ou caracter√≠sticas demogr√°ficas e comportamentais que\n",
            "distinguem os clientes membros dos normais.   O c√≥digo utilizado foi\n",
            "`df.groupby('tipo_cliente')['total'].mean().sort_values(ascending=False)`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exerc√≠cio: criando uma fun√ß√£o para o pipeline de consultas\n",
        "\n",
        "Nesta aula, aprendemos a criar um pipeline de consultas, que ser√° crucial para a constru√ß√£o de uma aplica√ß√£o futura. Para solidificar o aprendizado e preparar para usos pr√°ticos, sua tarefa √© encapsular o pipeline em uma fun√ß√£o, facilitando a reutiliza√ß√£o e integra√ß√£o na aplica√ß√£o que vamos come√ßar a construir na pr√≥xima aula.\n",
        "\n",
        "Dicas\n",
        "\n",
        "Nomeie a fun√ß√£o como pipeline_consulta.\n",
        "A fun√ß√£o deve aceitar um DataFrame chamado df como entrada.\n",
        "Inclua a fun√ß√£o descricao_colunas no corpo do c√≥digo para ajudar na descri√ß√£o das colunas do DataFrame.\n",
        "A fun√ß√£o deve retornar a vari√°vel qp, que representa o pipeline completo."
      ],
      "metadata": {
        "id": "-YG_kr_uIX8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Resposta da instrutora, a minha est√° no pipeline que usei:\n",
        "\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return \"Aqui est√£o os detalhes das colunas do dataframe:\\n\" + descricao\n",
        "\n",
        "# Defini√ß√£o de m√≥dulos da pipeline\n",
        "def pipeline_consulta(df):\n",
        "    instruction_str = (\n",
        "        \"1. Converta a consulta para c√≥digo Python execut√°vel usando Pandas.\\n\"\n",
        "        \"2. A linha final do c√≥digo deve ser uma express√£o Python que possa ser chamada com a fun√ß√£o `eval()`.\\n\"\n",
        "        \"3. O c√≥digo deve representar uma solu√ß√£o para a consulta.\\n\"\n",
        "        \"4. IMPRIMA APENAS A EXPRESS√ÉO.\\n\"\n",
        "        \"5. N√£o coloque a express√£o entre aspas.\\n\")\n",
        "\n",
        "    pandas_prompt_str = (\n",
        "        \"Voc√™ est√° trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "        \"{colunas_detalhes}\\n\\n\"\n",
        "        \"Este √© o resultado de `print(df.head())`:\\n\"\n",
        "        \"{df_str}\\n\\n\"\n",
        "        \"Siga estas instru√ß√µes:\\n\"\n",
        "        \"{instruction_str}\\n\"\n",
        "        \"Consulta: {query_str}\\n\\n\"\n",
        "        \"Express√£o:\"\n",
        ")\n",
        "\n",
        "    response_synthesis_prompt_str = (\n",
        "       \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "       \"Responda de forma natural, sem introdu√ß√µes como 'A resposta √©:' ou algo semelhante.\\n\"\n",
        "       \"Consulta: {query_str}\\n\\n\"\n",
        "       \"Instru√ß√µes do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "       \"Sa√≠da do Pandas: {pandas_output}\\n\\n\"\n",
        "       \"Resposta: \\n\\n\"\n",
        "       \"Ao final, exibir o c√≥digo usado em para gerar a resposta, no formato: O c√≥digo utilizado foi `{pandas_instructions}`\"\n",
        "    )\n",
        "\n",
        "    pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str,\n",
        "    df_str=df.head(5),\n",
        "    colunas_detalhes=descricao_colunas(df)\n",
        ")\n",
        "\n",
        "    pandas_output_parser = PandasInstructionParser(df)\n",
        "    response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "    # Cria√ß√£o do Query Pipeline\n",
        "    qp = QP(\n",
        "        modules={\n",
        "            \"input\": InputComponent(),\n",
        "            \"pandas_prompt\": pandas_prompt,\n",
        "            \"llm1\": llm,\n",
        "            \"pandas_output_parser\": pandas_output_parser,\n",
        "            \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "            \"llm2\": llm,\n",
        "        },\n",
        "        verbose=True,\n",
        "    )\n",
        "    qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
        "    qp.add_links(\n",
        "        [\n",
        "            Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
        "            Link(\"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"),\n",
        "            Link(\"pandas_output_parser\", \"response_synthesis_prompt\", dest_key=\"pandas_output\"),\n",
        "        ]\n",
        "    )\n",
        "    qp.add_link(\"response_synthesis_prompt\", \"llm2\")\n",
        "    return qp"
      ],
      "metadata": {
        "id": "xDC7Hs0OIXh3"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}