{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlo8aYBAKtM7Dd8KNnLv3Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-index==0.10.37"
      ],
      "metadata": {
        "id": "N-AjpoONaaGE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8sCFAtLCTVF",
        "outputId": "62d6b8df-059b-4dae-8cef-24dbd3e36e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install llama-index-llms-openai llama-index-llms-groq llama-index-experimental gradio fpdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.experimental.query_engine import PandasQueryEngine\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "\n",
        "# ===== CONFIGURAÇÃO COM PYDANTIC V2 =====\n",
        "class LLMConfig(BaseModel):\n",
        "    model: str = Field(..., description=\"Nome do modelo Groq a ser usado\")\n",
        "    api_key: str = Field(..., description=\"Chave da API Groq\")\n",
        "    data_url: str = Field(..., description=\"URL do CSV com os dados\")\n",
        "\n",
        "    @field_validator(\"data_url\")\n",
        "    @classmethod\n",
        "    def validar_url(cls, v: str) -> str:\n",
        "        if not (v.startswith(\"http://\") or v.startswith(\"https://\")):\n",
        "            raise ValueError(\"data_url deve começar com http:// ou https://\")\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"api_key\")\n",
        "    @classmethod\n",
        "    def validar_api_key(cls, v: str) -> str:\n",
        "        if not v or len(v.strip()) == 0:\n",
        "            raise ValueError(\"api_key não pode ser vazia\")\n",
        "        return v\n",
        "\n",
        "    model_config = {\n",
        "        \"extra\": \"allow\",\n",
        "        \"json_schema_extra\": {\n",
        "            \"example\": {\n",
        "                \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                \"api_key\": \"<SUA_CHAVE_AQUI>\",\n",
        "                \"data_url\": \"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ===== Função para formatar texto =====\n",
        "def formatar_texto(response, largura: int = 100, imprimir: bool = True):\n",
        "    texto = response.response\n",
        "    texto_formatado = textwrap.fill(texto, width=largura)\n",
        "    if imprimir:\n",
        "        print(texto_formatado)\n",
        "    return texto_formatado\n",
        "\n",
        "# ===== INICIALIZAÇÃO =====\n",
        "# Obter a chave via Colab userdata\n",
        "key = userdata.get('Groq_API')\n",
        "\n",
        "# Configurar\n",
        "config = LLMConfig(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    api_key=key,\n",
        "    data_url=\"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        ")\n",
        "\n",
        "# Carregar CSV\n",
        "df = pd.read_csv(config.data_url)\n",
        "\n",
        "# Inicializar LLM via Settings\n",
        "Settings.llm = Groq(model=config.model, api_key=config.api_key)\n",
        "\n",
        "# Criar engine de consulta (somente aqui, depois do df estar pronto)\n",
        "query_engine = PandasQueryEngine(df=df, verbose=True, synthesize_response=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2IbtC1eC0Ys",
        "outputId": "76a2c211-6579-4cd4-b875-f2b99d5eaf29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /usr/local/lib/python3.12/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modularidade\n",
        "\n",
        "Antes de criar o pipeline em si, você precisa definir alguns módulos.\n",
        "No primeiro, temos algumas instruções do que é para ser feito com os dados. Depois, temos o Pandas Prompt, com informações sobre o DataFrame que estamos trabalhando. Por fim, temos um prompt de respostas, onde vai sintetizar a resposta que será gerada no final.\n",
        "\n",
        "Mas, vamos entender o que precisamos fazer. Se estamos trabalhando com dados em português e queremos que o modelo só GAranta um bom resultado em português, a primeira coisa que precisamos fazer é alterar os textos todos para português. Dessa forma não haverá confusão, o modelo não se confundirá e responderá em inglês, como aconteceu na aula anterior."
      ],
      "metadata": {
        "id": "p1QufimDFv8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.experimental.query_engine.pandas import PandasInstructionParser"
      ],
      "metadata": {
        "id": "qqfCpxtcFo4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para obter uma descrição das colunas do DataFrame\n",
        "def descricao_colunas(df):\n",
        "  descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "  return 'Aqui estão os detalhes das colunas do DataFrame:\\n' + descricao\n",
        "\n",
        "\n",
        "# Instruções para orientar o modelo a converter uma consulta em linguagem natural em código Python executável com a biblioteca Pandas\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
        "    \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
        "    \"3. O código deve representar uma solução para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESSÃO.\\n\"\n",
        "    \"5. Não coloque a expressão entre aspas.\\n\")\n",
        "\n",
        "# Prompt que será enviado ao modelo para que ela gere o código Pandas desejado\n",
        "pandas_prompt_str = (\n",
        "    \"Você está trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este é o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instruções:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Expressão:\"\n",
        ")\n",
        "\n",
        "# Prompt para guiar o modelo a sintetizar uma resposta com base nos resultados obtidos pela consulta Pandas\n",
        "response_synthesis_prompt_str = (\n",
        "   \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "   \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
        "   \"Consulta: {query_str}\\n\\n\"\n",
        "   \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "   \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
        "   \"Resposta:\"\n",
        "   \"Ao final, exibir o código usado para gerar a resposta, no formato: O código utilizado foi {pandas_instructions}\"\n",
        ")\n",
        "\n",
        "# Módulo para obter as instruções Pandas\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str, colunas_detalhes=descricao_colunas(df), df_str=df.head(5)\n",
        ")\n",
        "# Módulo para executar as instruções Pandas\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "\n",
        "# Módulo para sintetizar a resposta\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "# Modelo\n",
        "#llm = Groq(model='llama3-70b-8192', api_key=key)\n",
        "#Carreguei um mais avançado"
      ],
      "metadata": {
        "id": "9DOoXQA5FxrF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Função eval() e instruções do modelo\n",
        "\n",
        "`eval()` é uma função Python que serve basicamente para executar um código que esteja dentro de uma string. Ou seja, se temos um código dentro de um texto, vai pegá-lo e executar diretamente.\n",
        "\n",
        "O código deve representar uma solução para a consulta. Então, se a pessoa fez uma pergunta, esse código vai ter que encontrar uma solução usando um código da biblioteca Pandas. Após, é definido que seja impresso apenas a expressão, que deve ser uma coisa importante de ser feita. Ele também pede para não colocar a expressão entre aspas no final, pois isso pode atrapalhar a execução da função eval().\n",
        "\n",
        "Depois temos o pandas_prompt_str. Este é o prompt que será enviado para o modelo para que gere o código Pandas desejado. Nele, informamos ao modelo que está trabalhando com o dataframe do Pandas em Python chamado df, esse é o resultado do print df.head(). Repare que começa mostrando quais são as cinco primeiras linhas do arquivo para o modelo já ter um contexto. A partir disso, conseguimos fazer perguntas com maior facilidade.\n",
        "\n",
        "Após, traz o df_str, que é uma variável que está abaixo, que é basicamente o df.head() para pegar as cinco primeiras linhas. Nisso, indica seguir as instruções. Em seguida, temos a Consulta {query str}, que é a consulta que faremos para o pipeline. E temos a expressão sendo gerada.\n",
        "\n",
        "Por fim, temos esse response_synthesis_prompt_str, que serve para guiar o modelo a sintetizar uma resposta com base no resultado que foi obtido pela nossa consulta Pandas. Então, ele indica que \"Dada uma pergunta de entrada, elabore uma resposta a partir dos resultados da consulta\".\n",
        "\n",
        "Após, traz qual foi a consulta feita, que é query_str. Também terá as instruções do Pandas que serão usadas para gerar a resposta, que é o pandas_instructions. Depois, ele tem essas instruções do Pandas, que são as instruções que serão usadas para chegar no resultado. Temos também a saída do Pandas, que é o pandas_output e a resposta."
      ],
      "metadata": {
        "id": "QoD2KK2tFS4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Para saber mais: modificando os prompts do pipeline\n",
        "\n",
        "A capacidade de **customizar como as consultas são processadas e as respostas apresentadas** é essencial para atender às necessidades específicas dos usuários, aumentando a eficiência e a relevância dos resultados.\n",
        "\n",
        "---\n",
        "\n",
        "## Estrutura do pipeline\n",
        "\n",
        "A primeira etapa antes de criar o pipeline é definir **3 módulos principais**:\n",
        "\n",
        "1. **pandas\\_prompt**\n",
        "\n",
        "   * Traduz consultas em linguagem natural para comandos específicos do Pandas.\n",
        "   * Facilita a manipulação direta dos dados.\n",
        "\n",
        "2. **pandas\\_output\\_parser**\n",
        "\n",
        "   * Executa os comandos gerados.\n",
        "   * Aplica-os diretamente ao DataFrame para processar as informações.\n",
        "\n",
        "3. **response\\_synthesis\\_prompt**\n",
        "\n",
        "   * Elabora uma resposta clara e informativa com base nos resultados obtidos.\n",
        "   * Essa resposta é a que será apresentada ao usuário.\n",
        "\n",
        "---\n",
        "\n",
        "## Código base\n",
        "\n",
        "```python\n",
        "# Módulo pandas_prompt\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str,\n",
        "    df_str=df.head(5),\n",
        "    colunas_detalhes=descricao_colunas(df)\n",
        ")\n",
        "\n",
        "# Módulo pandas_output_parser\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "\n",
        "# Módulo response_synthesis_prompt\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "# LLM\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=key)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Componentes essenciais\n",
        "\n",
        "### 1. Instruction String\n",
        "\n",
        "É o conjunto de instruções que orienta o modelo a converter consultas em linguagem natural para **código Python executável** utilizando Pandas.\n",
        "\n",
        "🔑 Requisito importante: o código final deve ser uma expressão Python que possa ser chamada com `eval()`.\n",
        "\n",
        "```python\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
        "    \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
        "    \"3. O código deve representar uma solução para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESSÃO.\\n\"\n",
        "    \"5. Não coloque a expressão entre aspas.\\n\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Pandas Prompt String\n",
        "\n",
        "Esse prompt é enviado ao modelo para gerar o código Pandas desejado.\n",
        "\n",
        "Inclui:\n",
        "\n",
        "* As **instruções detalhadas**.\n",
        "* Uma **visão geral do DataFrame** (`df.head(5)`).\n",
        "* **Detalhes das colunas** para dar mais contexto ao modelo.\n",
        "\n",
        "Função auxiliar para descrever colunas:\n",
        "\n",
        "```python\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return 'Aqui estão os detalhes das colunas do DataFrame:\\n' + descricao\n",
        "```\n",
        "\n",
        "Prompt traduzido e ajustado:\n",
        "\n",
        "```python\n",
        "pandas_prompt_str = (\n",
        "    \"Você está trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este é o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instruções:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Expressão:\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Response Synthesis Prompt String\n",
        "\n",
        "Responsável por guiar o modelo a sintetizar uma resposta explicativa, de forma natural, **atuando como um analista de dados**.\n",
        "\n",
        "Inclui:\n",
        "\n",
        "* Pergunta feita.\n",
        "* Código Pandas utilizado.\n",
        "* Saída obtida.\n",
        "* Resposta em texto natural.\n",
        "\n",
        "```python\n",
        "response_synthesis_prompt_str = (\n",
        "    \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "    \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "    \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
        "    \"Resposta: \\n\\n\"\n",
        "    \"Ao final, exibir o código usado em para gerar a resposta, no formato: O código utilizado foi `{pandas_instructions}`\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "✅ Agora que os módulos estão definidos, o próximo passo é **construir o pipeline completo**.\n",
        "\n",
        "---\n",
        "\n",
        "##Construindo o pipeline de consulta\n",
        "\n",
        "###Estruturando o pipeline"
      ],
      "metadata": {
        "id": "U_5WBmdrKhwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from llama_index.core import Settings, PromptTemplate\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.experimental.query_engine import PandasQueryEngine\n",
        "from llama_index.core.query_pipeline import QueryPipeline as QP, Link, InputComponent\n",
        "from llama_index.experimental.query_engine.pandas import PandasInstructionParser\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "\n",
        "# ===== CONFIGURAÇÃO COM PYDANTIC V2 =====\n",
        "class LLMConfig(BaseModel):\n",
        "    model: str = Field(..., description=\"Nome do modelo Groq a ser usado\")\n",
        "    api_key: str = Field(..., description=\"Chave da API Groq\")\n",
        "    data_url: str = Field(..., description=\"URL do CSV com os dados\")\n",
        "\n",
        "    @field_validator(\"data_url\")\n",
        "    @classmethod\n",
        "    def validar_url(cls, v: str) -> str:\n",
        "        if not (v.startswith(\"http://\") or v.startswith(\"https://\")):\n",
        "            raise ValueError(\"data_url deve começar com http:// ou https://\")\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"api_key\")\n",
        "    @classmethod\n",
        "    def validar_api_key(cls, v: str) -> str:\n",
        "        if not v or len(v.strip()) == 0:\n",
        "            raise ValueError(\"api_key não pode ser vazia\")\n",
        "        return v\n",
        "\n",
        "    model_config = {\n",
        "        \"extra\": \"allow\",\n",
        "        \"json_schema_extra\": {\n",
        "            \"example\": {\n",
        "                \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                \"api_key\": \"<SUA_CHAVE_AQUI>\",\n",
        "                \"data_url\": \"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ===== Função para formatar texto =====\n",
        "def formatar_texto(response, largura: int = 100, imprimir: bool = True):\n",
        "    texto = response.response if hasattr(response, \"response\") else str(response)\n",
        "    texto_formatado = textwrap.fill(texto, width=largura)\n",
        "    if imprimir:\n",
        "        print(texto_formatado)\n",
        "    return texto_formatado\n",
        "\n",
        "# ===== INICIALIZAÇÃO =====\n",
        "# Obter a chave via Colab userdata\n",
        "key = userdata.get('Groq_API')\n",
        "\n",
        "# Configurar\n",
        "config = LLMConfig(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    api_key=key,\n",
        "    data_url=\"https://raw.githubusercontent.com/YuriArduino/Estudos_Artificial_Intelligence/refs/heads/Dados/vendas.csv\"\n",
        ")\n",
        "\n",
        "# Carregar CSV\n",
        "df = pd.read_csv(config.data_url)\n",
        "\n",
        "# Inicializar LLM via Settings\n",
        "Settings.llm = Groq(model=config.model, api_key=config.api_key)\n",
        "\n",
        "# ===== PROMPTS =====\n",
        "# Instruções para orientar o modelo\n",
        "instruction_str = (\n",
        "    \"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
        "    \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
        "    \"3. O código deve representar uma solução para a consulta.\\n\"\n",
        "    \"4. IMPRIMA APENAS A EXPRESSÃO.\\n\"\n",
        "    \"5. Não coloque a expressão entre aspas.\\n\"\n",
        ")\n",
        "\n",
        "# Função para descrição das colunas\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return 'Aqui estão os detalhes das colunas do DataFrame:\\n' + descricao\n",
        "\n",
        "# Prompt Pandas\n",
        "pandas_prompt_str = (\n",
        "    \"Você está trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "    \"{colunas_detalhes}\\n\\n\"\n",
        "    \"Este é o resultado de `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Siga estas instruções:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Consulta: {query_str}\\n\\n\"\n",
        "    \"Expressão:\"\n",
        ")\n",
        "\n",
        "# Prompt de Resposta Rápida\n",
        "response_synthesis_prompt_str = (\n",
        "   \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "   \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
        "   \"Consulta: {query_str}\\n\\n\"\n",
        "   \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "   \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
        "   \"Resposta:\\n\"\n",
        "   \"Ao final, exibir o código usado para gerar a resposta, no formato: O código utilizado foi {pandas_instructions}\"\n",
        ")\n",
        "\n",
        "# Módulo para obter as instruções Pandas\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str, colunas_detalhes=descricao_colunas(df), df_str=df.head(5)\n",
        ")\n",
        "\n",
        "# Módulo para executar as instruções Pandas\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "\n",
        "# Módulo para sintetizar a resposta\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "\n",
        "# ===== DEFINIÇÃO DOS NODES (usando QueryPipeline) =====\n",
        "# Os nodes são representados pelos próprios componentes no QueryPipeline\n",
        "\n",
        "# Criar o QueryPipeline\n",
        "qp = QP(verbose=True)\n",
        "\n",
        "# Adicionar modulos\n",
        "qp.add_modules({\n",
        "    \"input\": InputComponent(),\n",
        "    \"pandas_prompt\": pandas_prompt,\n",
        "    \"llm\": Settings.llm,\n",
        "    \"pandas_output_parser\": pandas_output_parser,\n",
        "    \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "    \"response_synthesizer\": Settings.llm, # Use the LLM to synthesize the response\n",
        "})\n",
        "\n",
        "# Adicionar Links\n",
        "qp.add_links(\n",
        "    [\n",
        "        Link(\"input\", \"pandas_prompt\"),\n",
        "        Link(\"pandas_prompt\", \"llm\"),\n",
        "        Link(\"llm\", \"pandas_output_parser\"),\n",
        "        Link(\"pandas_output_parser\", \"response_synthesis_prompt\", dest_key=\"pandas_output\"),\n",
        "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
        "        Link(\"llm\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"), # Link LLM output (the code)\n",
        "        Link(\"response_synthesis_prompt\", \"response_synthesizer\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "zdVXkf3hf8Ij"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Realizando consultas com a pipeline\n",
        "\n",
        "Criamos uma variável chamada response e atribuímos um valor a ela. O processo será um pouco diferente, pois, ao invés de utilizar o Pandas Query Engine, estamos trabalhando diretamente com o pipeline.\n",
        "\n",
        "Para isso, usaremos a variável `qp`, que definimos anteriormente. A execução do pipeline será feita por meio de `qp.run()`, passando como parâmetro o valor de query_str, que já definimos em etapas anteriores. Esse parâmetro é o destino do nosso primeiro link, presente tanto no prompt de resposta quanto no prompt da biblioteca Pandas.\n",
        "\n",
        "Vamos perguntar: \"Qual é a média gasta por cada tipo de cliente?\" Temos clientes que são membros da nossa rede de varejo e clientes que não são membros. Será que um gasta mais que o outro, em média?"
      ],
      "metadata": {
        "id": "cCqXnOGX6d-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== EXECUÇÃO =====\n",
        "response = qp.run(query_str=\"Qual é a média gasta por cada tipo de cliente?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-XqwVNE6ayF",
        "outputId": "7d1573f3-1c85-4d50-c957-caf90f2bca54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
            "query_str: Qual é a média gasta por cada tipo de cliente?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
            "query_str: Qual é a média gasta por cada tipo de cliente?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: Você está trabalhando com um dataframe do pandas em Python chamado `df`.\n",
            "Aqui estão os detalhes das colunas do DataFrame:\n",
            "`ID_compra`: object\n",
            "`filial`: object\n",
            "`cidade`: object\n",
            "`tipo_cliente`: object\n",
            "`...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
            "input: assistant: df.groupby('tipo_cliente')['total'].mean()\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
            "query_str: Qual é a média gasta por cada tipo de cliente?\n",
            "pandas_instructions: assistant: df.groupby('tipo_cliente')['total'].mean()\n",
            "pandas_output: tipo_cliente\n",
            "Membro    327.791305\n",
            "Normal    318.122856\n",
            "Name: total, dtype: float64\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesizer with input: \n",
            "messages: Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\n",
            "Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante...\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Formatando a resposta\n",
        "\n",
        "\n",
        "Na próxima célula, formatamos a resposta usando o módulo textwrap para melhorar a apresentação. Iniciamos criando uma variável chamada texto que será igual a response.message.content para obter a resposta.\n",
        "\n",
        "Para formatar o texto, criamos uma variável texto_formatado que será igual a textwrap.fill(texto, width=100), sendo o width a largura. Por fim, exibimos o texto_formatado utilizando o print():"
      ],
      "metadata": {
        "id": "HgwF0W9k7baY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = response.message.content\n",
        "texto_formatado = textwrap.fill(texto, width=100)\n",
        "print(texto_formatado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlVaqPRE7ihb",
        "outputId": "c006c1f2-e186-4580-b1a1-bedb7b1d7ec4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A média de gastos por tipo de cliente varia ligeiramente entre as categorias. Os clientes \"Membro\"\n",
            "apresentam uma média de gasto de aproximadamente R$ 327,79, enquanto os clientes \"Normal\" gastam em\n",
            "média R$ 318,12. Isso sugere que os membros tendem a gastar um pouco mais em comparação com os\n",
            "clientes normais.  O código utilizado foi df.groupby('tipo_cliente')['total'].mean()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código utilizado foi df.groupby('tipo_cliente')['total'].mean(). Isso é útil para a equipe da Zoop, pois além da resposta, podemos verificar se está tudo certo.\n",
        "\n",
        "Como estamos utilizando uma LLM, podemos solicitar que ela forneça um insight sobre as diferenças de preço. Podemos testar outra consulta: \"Por que clientes tipo membro têm maior média de gasto?\" utilizando a variável response que será igual a qp.run(query_str=\"\") e passamos a pergunta:"
      ],
      "metadata": {
        "id": "iyLMBG_i7zdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = qp.run(query_str=\"Por que clientes do tipo membro tem maior média de gasto?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vppjh6qa7yks",
        "outputId": "0380d5b0-ecc7-4705-f34a-0cd0fc00a185"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
            "query_str: Por que clientes do tipo membro tem maior média de gasto?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
            "query_str: Por que clientes do tipo membro tem maior média de gasto?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: Você está trabalhando com um dataframe do pandas em Python chamado `df`.\n",
            "Aqui estão os detalhes das colunas do DataFrame:\n",
            "`ID_compra`: object\n",
            "`filial`: object\n",
            "`cidade`: object\n",
            "`tipo_cliente`: object\n",
            "`...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
            "input: assistant: df.groupby('tipo_cliente')['total'].mean().sort_values(ascending=False)\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
            "query_str: Por que clientes do tipo membro tem maior média de gasto?\n",
            "pandas_instructions: assistant: df.groupby('tipo_cliente')['total'].mean().sort_values(ascending=False)\n",
            "pandas_output: tipo_cliente\n",
            "Membro    327.791305\n",
            "Normal    318.122856\n",
            "Name: total, dtype: float64\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesizer with input: \n",
            "messages: Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\n",
            "Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante...\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos exibir a resposta como fizemos anteriormente com o seguinte código:"
      ],
      "metadata": {
        "id": "iQsTrvcg74HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = response.message.content\n",
        "texto_formatado = textwrap.fill(texto, width=100)\n",
        "print(texto_formatado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryilCoYO749K",
        "outputId": "2ef50428-9597-4547-a9d7-5657d8fd0bed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clientes do tipo membro apresentam uma média de gasto maior em comparação com clientes do tipo\n",
            "normal. De acordo com os dados, a média de gasto para clientes membros é de aproximadamente R$\n",
            "327,79, enquanto para clientes normais essa média fica em torno de R$ 318,12.   Essa diferença pode\n",
            "ser resultado de vários fatores, como programas de fidelidade que incentivam membros a realizar\n",
            "compras mais frequentes ou de maior valor, ou características demográficas e comportamentais que\n",
            "distinguem os clientes membros dos normais.   O código utilizado foi\n",
            "`df.groupby('tipo_cliente')['total'].mean().sort_values(ascending=False)`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercício: criando uma função para o pipeline de consultas\n",
        "\n",
        "Nesta aula, aprendemos a criar um pipeline de consultas, que será crucial para a construção de uma aplicação futura. Para solidificar o aprendizado e preparar para usos práticos, sua tarefa é encapsular o pipeline em uma função, facilitando a reutilização e integração na aplicação que vamos começar a construir na próxima aula.\n",
        "\n",
        "Dicas\n",
        "\n",
        "Nomeie a função como pipeline_consulta.\n",
        "A função deve aceitar um DataFrame chamado df como entrada.\n",
        "Inclua a função descricao_colunas no corpo do código para ajudar na descrição das colunas do DataFrame.\n",
        "A função deve retornar a variável qp, que representa o pipeline completo."
      ],
      "metadata": {
        "id": "-YG_kr_uIX8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Resposta da instrutora, a minha está no pipeline que usei:\n",
        "\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return \"Aqui estão os detalhes das colunas do dataframe:\\n\" + descricao\n",
        "\n",
        "# Definição de módulos da pipeline\n",
        "def pipeline_consulta(df):\n",
        "    instruction_str = (\n",
        "        \"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
        "        \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
        "        \"3. O código deve representar uma solução para a consulta.\\n\"\n",
        "        \"4. IMPRIMA APENAS A EXPRESSÃO.\\n\"\n",
        "        \"5. Não coloque a expressão entre aspas.\\n\")\n",
        "\n",
        "    pandas_prompt_str = (\n",
        "        \"Você está trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "        \"{colunas_detalhes}\\n\\n\"\n",
        "        \"Este é o resultado de `print(df.head())`:\\n\"\n",
        "        \"{df_str}\\n\\n\"\n",
        "        \"Siga estas instruções:\\n\"\n",
        "        \"{instruction_str}\\n\"\n",
        "        \"Consulta: {query_str}\\n\\n\"\n",
        "        \"Expressão:\"\n",
        ")\n",
        "\n",
        "    response_synthesis_prompt_str = (\n",
        "       \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "       \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
        "       \"Consulta: {query_str}\\n\\n\"\n",
        "       \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "       \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
        "       \"Resposta: \\n\\n\"\n",
        "       \"Ao final, exibir o código usado em para gerar a resposta, no formato: O código utilizado foi `{pandas_instructions}`\"\n",
        "    )\n",
        "\n",
        "    pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str,\n",
        "    df_str=df.head(5),\n",
        "    colunas_detalhes=descricao_colunas(df)\n",
        ")\n",
        "\n",
        "    pandas_output_parser = PandasInstructionParser(df)\n",
        "    response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "    # Criação do Query Pipeline\n",
        "    qp = QP(\n",
        "        modules={\n",
        "            \"input\": InputComponent(),\n",
        "            \"pandas_prompt\": pandas_prompt,\n",
        "            \"llm1\": llm,\n",
        "            \"pandas_output_parser\": pandas_output_parser,\n",
        "            \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "            \"llm2\": llm,\n",
        "        },\n",
        "        verbose=True,\n",
        "    )\n",
        "    qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
        "    qp.add_links(\n",
        "        [\n",
        "            Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
        "            Link(\"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"),\n",
        "            Link(\"pandas_output_parser\", \"response_synthesis_prompt\", dest_key=\"pandas_output\"),\n",
        "        ]\n",
        "    )\n",
        "    qp.add_link(\"response_synthesis_prompt\", \"llm2\")\n",
        "    return qp"
      ],
      "metadata": {
        "id": "xDC7Hs0OIXh3"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}