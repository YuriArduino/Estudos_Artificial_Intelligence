{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvYaYyHuyRv1kuqOaNARvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriArduino/Estudos_Artificial_Intelligence/blob/Imers%C3%A3o-Agentes-de-IA---Alura/RAG_Evaluation_and_Hyperparameter_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preâmbulo: A Jornada de um Framework de Avaliação de RAG**\n",
        "\n",
        "O que começou como um exercício da **Aula 2 da Imersão Dev (Agentes de IA) da Alura**, focado em construir um sistema simples de RAG (Retrieval-Augmented Generation), rapidamente se transformou em uma pergunta muito mais profunda e fundamental: **como podemos medir, otimizar e, acima de tudo, confiar nos resultados de um sistema de RAG?**\n",
        "\n",
        "A resposta a essa pergunta é a jornada documentada neste notebook.\n",
        "\n",
        "## **Da Incerteza à Engenharia**\n",
        "\n",
        "A intuição inicial era clara: aceitar os parâmetros padrão de um `splitter` ou `retriever` é trabalhar às cegas. Um sistema de IA que não pode ser medido não é confiável. O primeiro passo foi simples: construir um contador de tokens, um monitor de API. Mas isso apenas arranhou a superfície.\n",
        "\n",
        "A verdadeira virada aconteceu ao confrontar os desafios do mundo real:\n",
        "\n",
        "1.  **O Problema da Relevância:** Como saber se os `chunks` que o sistema de busca recupera são realmente úteis? Apenas a similaridade matemática não era suficiente.\n",
        "2.  **O Problema da Alucinação:** Como garantir que o sistema diga \"não sei\" em vez de inventar uma resposta quando a informação não existe nos documentos?\n",
        "3.  **O Problema da Otimização:** Qual combinação de `chunk_size` e `chunk_overlap` é objetivamente a melhor? E como provar isso com dados?\n",
        "\n",
        "## **A Arquitetura da Solução: Um Laboratório de MLOps**\n",
        "\n",
        "A resposta exigiu uma refatoração completa, quebrando um script monolítico em um conjunto de **especialistas de software**, cada um com sua responsabilidade única, inspirados nos princípios de design SOLID:\n",
        "\n",
        "*   Um **`LLMAdapter`** para criar uma camada resiliente sobre a API.\n",
        "*   Um **`RelevanceEvaluator`** para atuar como nosso \"LLM Juiz\".\n",
        "*   Um **`MetricsCalculator`** para computar métricas padrão da indústria (Precision, Recall, F1-Score).\n",
        "*   E a peça central: um **`GroundTruthGenerator`**.\n",
        "\n",
        "### **A Inovação: `LLM-derived Ground-Truth`**\n",
        "\n",
        "Em vez de criar um \"gabarito\" manual – um processo lento e sujeito a viés –, decidimos usar a própria IA como uma \"anotadora de dados\". O `GroundTruthGenerator` varre todo o nosso conhecimento e usa o Gemini para criar um gabarito objetivo, que se torna a nossa \"verdade fundamental\" para todos os testes subsequentes.\n",
        "\n",
        "## **O Resultado Final: Um Framework Científico**\n",
        "\n",
        "Este notebook é a documentação dessa jornada. Ele não é apenas uma implementação de RAG, mas um **framework de avaliação** completo, um verdadeiro laboratório de MLOps que permite a qualquer desenvolvedor:\n",
        "\n",
        "1.  **Carregar** um conjunto de documentos.\n",
        "2.  **Definir** múltiplas estratégias de RAG para comparar.\n",
        "3.  **Gerar** um gabarito de alta qualidade de forma automática.\n",
        "4.  **Executar** uma análise comparativa sistemática.\n",
        "5.  **Visualizar** os resultados em um dashboard claro e acionável."
      ],
      "metadata": {
        "id": "0VFPT0-3glI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 1: Preparação do Ambiente de Análise**\n",
        "\n",
        "Esta é a célula fundamental que prepara todo o nosso ambiente de trabalho. Ela é dividida em duas etapas essenciais: a **instalação das ferramentas** e a **importação dos módulos** que usaremos ao longo do notebook.\n",
        "\n",
        "### **1.1 Instalações Necessárias**\n",
        "\n",
        "A primeira linha de comando (`!pip install...`) funciona como a montagem da nossa \"bancada de trabalho\". Ela instala todas as bibliotecas externas que o nosso projeto precisa para funcionar. O `-q` (`quiet`) apenas torna a saída menos verbosa.\n",
        "\n",
        "**O \"Porquê\" de cada ferramenta:**\n",
        "\n",
        "*   **Análise e Visualização:**\n",
        "    *   `pandas`, `matplotlib`, `seaborn`: A tríade clássica da ciência de dados em Python. Usaremos para estruturar e plotar nossos resultados finais em tabelas e gráficos.\n",
        "*   **Componentes do Pipeline RAG (LangChain):**\n",
        "    *   `langchain_community`, `langchain_google_genai`: O coração da nossa conexão com os modelos do Google.\n",
        "    *   `faiss-cpu`: A nossa \"memória vetorial\". É o banco de dados da Meta AI que armazena os embeddings dos nossos documentos de forma eficiente, rodando na CPU (não exige GPU).\n",
        "    *   `pymupdf`: Um leitor de PDFs de alta performance, que o LangChain usa para extrair o texto dos nossos arquivos de políticas.\n",
        "*   **Utilitários:**\n",
        "    *   `requests`: O \"carteiro\" da internet. Usado para baixar os arquivos PDF a partir das URLs do GitHub.\n",
        "\n",
        "### **1.2 Imports**\n",
        "\n",
        "Após instalar as ferramentas, esta seção as \"liga\" e as torna disponíveis para uso no nosso notebook. Organizamos os imports em grupos lógicos para maior clareza.\n",
        "\n",
        "*   **Módulos Padrão do Python:**\n",
        "    *   Ferramentas essenciais para manipulação de sistema (`os`), dados (`json`), tempo (`time`), texto (`re`), arquivos (`tempfile`), estatísticas (`statistics`), erros (`traceback`) e datas (`datetime`).\n",
        "    *   `dataclasses`, `typing`, `abc`: Módulos que nos ajudam a escrever um código mais moderno, estruturado e seguro, definindo \"contratos\" claros para nossas classes e funções.\n",
        "\n",
        "*   **Módulos de Análise de Dados:**\n",
        "    *   `pandas as pd`, `matplotlib.pyplot as plt`, `seaborn as sns`: Importamos a tríade com seus apelidos convencionais (`pd`, `plt`, `sns`), uma prática padrão na comunidade.\n",
        "\n",
        "*   **Módulos Específicos do LangChain:**\n",
        "    *   Aqui importamos as \"peças de LEGO\" específicas que vamos montar: o **Modelo de Linguagem** (`ChatGoogleGenerativeAI`), o **Modelo de Embedding** (`GoogleGenerativeAIEmbeddings`), o **Carregador de Documentos** (`PyMuPDFLoader`), o **Divisor de Texto** (`RecursiveCharacterTextSplitter`) e a **Base de Vetores** (`FAISS`).\n",
        "\n",
        "> **Nota para o Futuro:** Se algum dia o notebook quebrar com um erro de `ModuleNotFoundError`, é muito provável que a solução esteja em adicionar a biblioteca faltante na lista de instalações da seção 1.1.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DCARpj9hQlN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Instalações necessárias\n",
        "!pip install -q pandas matplotlib seaborn langchain_community langchain_google_genai faiss-cpu pymupdf requests\n",
        "\n",
        "\n",
        "# 1.2 Imports\n",
        "# Built-in\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import tempfile\n",
        "import statistics\n",
        "import traceback\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional, Tuple, Protocol\n",
        "from datetime import datetime\n",
        "from urllib.parse import unquote\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import hashlib\n",
        "import logging\n",
        "from typing import List, Optional\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "\n",
        "# Third-party\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "_4SX5O-PQ_2b"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 2: Definindo as Estruturas de Dados (Data Models)**\n",
        "\n",
        "Antes de escrevermos a lógica complexa do nosso analisador, definimos claramente as **estruturas de dados** que ele irá manipular.  \n",
        "Separar a definição dos dados da lógica (o código que os processa) é uma prática de engenharia de software chamada **Separação de Preocupações** (`Separation of Concerns`), que torna nosso código mais organizado, seguro e fácil de entender.\n",
        "\n",
        "Desta vez, em vez de usarmos `@dataclass` do Python puro, adotamos **Pydantic v2 (`BaseModel`)**.  \n",
        "Isso nos traz **superpoderes adicionais**:\n",
        "\n",
        "* **Validação automática de tipos**: se tentarmos atribuir um `str` em um campo que deveria ser `float`, o Pydantic gera um erro imediatamente.\n",
        "* **Serialização prática**: podemos salvar e carregar resultados facilmente usando `.model_dump()` e `.model_validate_json()`.\n",
        "* **Imutabilidade opcional**: podemos \"congelar\" os objetos para que não sejam alterados acidentalmente depois de criados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Os Nossos Modelos de Dados (versão Pydantic v2):**\n",
        "\n",
        "1. **`ChunkAnalysisResult`**  \n",
        "   - **Propósito:** Representa o resultado da análise de **um único chunk** recuperado pelo sistema de busca.  \n",
        "   - Campos: conteúdo, score de similaridade, julgamento de relevância, confiança do LLM, raciocínio textual e tempo de execução.\n",
        "\n",
        "2. **`AnalysisMetrics`**  \n",
        "   - **Propósito:** Consolida todas as **métricas de performance quantitativas** de um experimento.  \n",
        "   - Campos: precisão, recall, f1-score, precisão em diferentes `k`, verdadeiros positivos, contagem de recuperados e de ground truth.\n",
        "\n",
        "3. **`ChunkingStrategy`**  \n",
        "   - **Propósito:** Define formalmente uma \"receita\" de chunking, com seu nome, tamanho e sobreposição.  \n",
        "   - Campos: `name`, `chunk_size` e `chunk_overlap`.\n",
        "\n",
        "4. **`RetrievalExperiment`**  \n",
        "   - **Propósito:** É o nosso **objeto principal de resultados**.  \n",
        "   - Campos: estratégia usada, query, `k` testado, chunks recuperados, resultados individuais, métricas consolidadas, tempo de execução e recomendações adicionais.\n",
        "\n",
        "> **Nota para o Futuro:** Ao usar Pydantic v2, qualquer tentativa de instanciar esses objetos com tipos errados gerará um erro de validação imediato. Isso previne muitos bugs silenciosos durante o desenvolvimento.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yDngmkpCRpJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Any, Set, Dict, Iterable, Optional\n",
        "\n",
        "@dataclass\n",
        "class ChunkAnalysisResult:\n",
        "    id: str\n",
        "    content: str\n",
        "    score: float\n",
        "    is_relevant: bool\n",
        "    confidence_score: float\n",
        "    reasoning: Optional[str] = None\n",
        "    evaluation_time: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class AnalysisMetrics:\n",
        "    precision: float\n",
        "    recall: float\n",
        "    f1_score: float\n",
        "    precision_at_k: Dict[int, float]\n",
        "    true_positives: int\n",
        "    retrieved_count: int\n",
        "    ground_truth_count: int\n",
        "\n",
        "class MetricsCalculator:\n",
        "    \"\"\"\n",
        "    Calculadora de métricas baseada em comparação de CONTEÚDO dos chunks.\n",
        "    Aceita ground-truth como:\n",
        "      - lista de objetos com atributo `page_content`,\n",
        "      - lista/set de strings (conteúdo),\n",
        "      - lista/set de chunk_ids (tratamento externo — aqui assumimos conteúdo/string).\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_gt_contents(ground_truth_docs: Iterable[Any]) -> Set[str]:\n",
        "        \"\"\"\n",
        "        Normaliza a entrada de ground truth para um conjunto de strings com os conteúdos.\n",
        "        \"\"\"\n",
        "        if ground_truth_docs is None:\n",
        "            return set()\n",
        "\n",
        "        contents = set()\n",
        "        for item in ground_truth_docs:\n",
        "            if item is None:\n",
        "                continue\n",
        "            # objeto com page_content (Document do LangChain, SimpleNamespace, etc.)\n",
        "            if hasattr(item, \"page_content\"):\n",
        "                try:\n",
        "                    contents.add(str(item.page_content))\n",
        "                except Exception:\n",
        "                    contents.add(str(item))\n",
        "            else:\n",
        "                # Se for string ou outro tipo, converte para string e guarda\n",
        "                contents.add(str(item))\n",
        "        return contents\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_precision_at_k(\n",
        "        results: List[ChunkAnalysisResult],\n",
        "        gt_contents: Set[str],\n",
        "        k_values: List[int]\n",
        "    ) -> Dict[int, float]:\n",
        "        \"\"\"\n",
        "        Calcula Precision@K baseado em match por conteúdo (substring containment).\n",
        "        Retorna dicionário {k: precision_at_k}.\n",
        "        \"\"\"\n",
        "        precision_at_k: Dict[int, float] = {}\n",
        "        if not k_values:\n",
        "            return precision_at_k\n",
        "\n",
        "        # Ordenamos results por score decrescente caso não estejam ordenados\n",
        "        ordered = sorted(results, key=lambda r: r.score, reverse=True)\n",
        "\n",
        "        for k in sorted(k_values):\n",
        "            if k <= 0:\n",
        "                precision_at_k[k] = 0.0\n",
        "                continue\n",
        "\n",
        "            top_k = ordered[:k]\n",
        "            top_k_contents = [r.content for r in top_k]\n",
        "\n",
        "            tp_at_k = 0\n",
        "            # Contagem: quantos itens distintos do GT aparecem no top_k (matching por substring)\n",
        "            for gt_text in gt_contents:\n",
        "                if any(gt_text in retrieved_text for retrieved_text in top_k_contents):\n",
        "                    tp_at_k += 1\n",
        "\n",
        "            precision_at_k[k] = tp_at_k / k if k > 0 else 0.0\n",
        "\n",
        "        return precision_at_k\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_metrics(\n",
        "        results: List[ChunkAnalysisResult],\n",
        "        ground_truth_docs: Iterable[Any],\n",
        "        k_values: List[int] = None\n",
        "    ) -> AnalysisMetrics:\n",
        "        \"\"\"\n",
        "        Calcula precision, recall, f1 e precision@k comparando por conteúdo (substring).\n",
        "        - results: lista de ChunkAnalysisResult (geralmente ordenada por relevância)\n",
        "        - ground_truth_docs: coleção de documentos/strings representando o gabarito\n",
        "        \"\"\"\n",
        "        if k_values is None:\n",
        "            k_values = [1, 3, 5, 8, 10]\n",
        "\n",
        "        gt_contents = MetricsCalculator._extract_gt_contents(ground_truth_docs)\n",
        "        retrieved_contents = [res.content for res in results]\n",
        "\n",
        "        true_positives = 0\n",
        "        # Para cada texto do gabarito, verifica se ele está contido em algum texto recuperado\n",
        "        for gt_text in gt_contents:\n",
        "            if any(gt_text in retrieved_text for retrieved_text in retrieved_contents):\n",
        "                true_positives += 1\n",
        "\n",
        "        total_retrieved = len(retrieved_contents)\n",
        "        total_relevant_in_corpus = len(gt_contents)\n",
        "\n",
        "        precision = true_positives / total_retrieved if total_retrieved > 0 else 0.0\n",
        "        recall = true_positives / total_relevant_in_corpus if total_relevant_in_corpus > 0 else 0.0\n",
        "        f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "        precision_at_k = MetricsCalculator._calculate_precision_at_k(results, gt_contents, k_values)\n",
        "\n",
        "        return AnalysisMetrics(\n",
        "            precision=precision,\n",
        "            recall=recall,\n",
        "            f1_score=f1_score,\n",
        "            precision_at_k=precision_at_k,\n",
        "            true_positives=true_positives,\n",
        "            retrieved_count=total_retrieved,\n",
        "            ground_truth_count=total_relevant_in_corpus\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def find_optimal_threshold(results: List[ChunkAnalysisResult]) -> float:\n",
        "        \"\"\"\n",
        "        Encontra o threshold ótimo baseado no F1-score, usando o julgamento do \"LLM Juiz\".\n",
        "        Retorna o score (threshold) que maximiza F1 entre cortes.\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return 0.0\n",
        "\n",
        "        sorted_results = sorted(results, key=lambda x: x.score, reverse=True)\n",
        "        best_threshold = sorted_results[0].score\n",
        "        best_f1 = 0.0\n",
        "        total_relevant_in_results = sum(1 for r in sorted_results if r.is_relevant)\n",
        "\n",
        "        if total_relevant_in_results == 0:\n",
        "            return best_threshold\n",
        "\n",
        "        for result in sorted_results:\n",
        "            threshold = result.score\n",
        "            filtered = [x for x in sorted_results if x.score >= threshold]\n",
        "            relevant_retrieved = sum(1 for x in filtered if x.is_relevant)\n",
        "            total_retrieved = len(filtered)\n",
        "            precision = relevant_retrieved / total_retrieved if total_retrieved else 0\n",
        "            recall = relevant_retrieved / total_relevant_in_results if total_relevant_in_results else 0\n",
        "            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "            if f1 >= best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        return float(best_threshold)"
      ],
      "metadata": {
        "id": "h7Y1wVauDjno"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 3: Definindo os Contratos de Comportamento (Protocolos)**\n",
        "\n",
        "Nesta célula, definimos as **interfaces** ou **contratos** para os componentes externos do nosso sistema, como o modelo de linguagem (LLM) e o banco de dados de vetores (Vector Store).\n",
        "\n",
        "Em vez de acoplar nosso analisador diretamente a uma classe específica (como `ChatGoogleGenerativeAI` ou `FAISS`), nós usamos `Protocol` do Python.  \n",
        "Um protocolo define **o que um objeto deve ser capaz de fazer**, sem se importar com **como ele faz**.  \n",
        "É como definir o que significa ser um \"veículo\" (ter rodas, acelerar, frear) sem se prender a uma \"Ferrari\" ou um \"Fusca\".\n",
        "\n",
        "Essa abordagem, conhecida como **Inversão de Dependência**, torna nosso código extremamente flexível e fácil de testar.\n",
        "\n",
        "---\n",
        "\n",
        "### **Os Nossos Contratos:**\n",
        "\n",
        "1. **`LLMClient`**\n",
        "   - **Contrato:** Qualquer objeto que queira atuar como um cliente de LLM em nosso sistema **deve** ter um método chamado `invoke` que aceita uma `string` (o prompt) e retorna um dicionário com os resultados.\n",
        "   - **Benefício:** Se amanhã quisermos trocar o Gemini pelo OpenAI, ou por um modelo local via Ollama, não precisamos mudar nada no nosso `AdvancedRAGAnalyzer`. Basta criar um novo \"adaptador\" que implemente este protocolo.\n",
        "\n",
        "2. **`VectorStore`**\n",
        "   - **Contrato:** Qualquer objeto que queira atuar como um banco de dados de vetores **deve** ter um método `similarity_search_with_score` que busca por uma `query` e retorna uma lista de documentos e seus scores.\n",
        "   - **Benefício:** Se no futuro decidirmos que o `FAISS` não é mais a melhor opção e quisermos migrar para `ChromaDB`, `Pinecone` ou outro serviço, nosso analisador continuará funcionando perfeitamente, desde que o novo banco de dados respeite este contrato.\n",
        "\n",
        "> **Nota para o Futuro:** O uso de Protocolos é o que permite a **injeção de dependência**.  \n",
        "> Em vez de o nosso analisador *criar* suas próprias dependências (o LLM, o Vector Store), ele as recebe prontas em seu construtor (`__init__`).  \n",
        "> Isso facilita imensamente os testes, pois podemos \"injetar\" versões falsas (mocks) do LLM e do Vector Store para testar a lógica do analisador de forma isolada, sem fazer chamadas de API reais.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Oidi6eyhEv5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Protocol, Any\n",
        "\n",
        "\n",
        "class LLMClient(Protocol):\n",
        "    \"\"\"Protocolo para clientes LLM.\"\"\"\n",
        "    def invoke(self, prompt: str) -> dict[str, Any]:\n",
        "        \"\"\"Invoca o LLM com um prompt e retorna resposta normalizada.\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "class VectorStore(Protocol):\n",
        "    \"\"\"Protocolo para vector stores.\"\"\"\n",
        "    def similarity_search_with_score(self, query: str, k: int) -> list[tuple[Any, float]]:\n",
        "        \"\"\"Busca por similaridade retornando documentos e scores.\"\"\"\n",
        "        ..."
      ],
      "metadata": {
        "id": "7TtoWg3QEzuM"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 4: Implementando o Sistema de Cache Persistente**\n",
        "\n",
        "\"\"\"\n",
        "A etapa de geração de \"ground truth\" e a avaliação de cada chunk são os gargalos de performance\n",
        "e custo do nosso pipeline. Executar dezenas ou centenas de chamadas de API a cada vez que rodamos\n",
        "o notebook é inviável.\n",
        "\n",
        "A classe `ExperimentCache` resolve esse problema com um sistema de **cache persistente** que:\n",
        "1. Gera chaves únicas baseadas em parâmetros + conteúdo dos documentos.\n",
        "2. Usa `pickle` para salvar e restaurar experimentos completos.\n",
        "3. Faz logging estruturado para depuração profissional.\n",
        "\n",
        "Assim, rodamos uma análise pesada uma vez e depois reutilizamos os resultados em segundos.\n"
      ],
      "metadata": {
        "id": "zG1e12Xk3QMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import hashlib\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Any, Dict, Set\n",
        "\n",
        "# Configuração básica de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# --- Funções Auxiliares de Cache Seguro (nível de módulo) ---\n",
        "\n",
        "def _safe_write_json(path: Path, obj: dict) -> None:\n",
        "    \"\"\"\n",
        "    Escrita atômica: escreve em um arquivo temporário e depois o renomeia para\n",
        "    evitar a criação de arquivos de cache corrompidos ou vazios.\n",
        "    \"\"\"\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Cria um arquivo temporário no mesmo diretório para garantir que a renomeação seja atômica\n",
        "    fd, tmp_path_str = tempfile.mkstemp(dir=str(path.parent))\n",
        "    tmp_path = Path(tmp_path_str)\n",
        "    try:\n",
        "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(obj, f, indent=2, ensure_ascii=False)\n",
        "            f.flush()\n",
        "            os.fsync(f.fileno()) # Garante que os dados sejam escritos no disco\n",
        "        # Renomeia o arquivo temporário para o nome final.\n",
        "        tmp_path.replace(path)\n",
        "    except Exception as e:\n",
        "        # Em caso de falha, remove o arquivo temporário para não deixar lixo\n",
        "        if tmp_path.exists():\n",
        "            tmp_path.unlink()\n",
        "        raise e\n",
        "\n",
        "def _load_json_safe(path: Path, logger: logging.Logger) -> Optional[dict]:\n",
        "    \"\"\"\n",
        "    Carrega um arquivo JSON de forma segura. Se o arquivo estiver corrompido,\n",
        "    move o arquivo para .corrupt para auditoria e retorna None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        if not isinstance(data, dict):\n",
        "            raise ValueError(\"O cache JSON não contém um objeto (dicionário) na raiz.\")\n",
        "        return data\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        logger.warning(f\"Cache corrompido detectado em '{path}': {e}. Movendo para '{path}.corrupt'.\")\n",
        "        try:\n",
        "            corrupt_path = path.with_suffix(path.suffix + \".corrupt\")\n",
        "            if corrupt_path.exists():\n",
        "                # Adiciona um timestamp para não sobrescrever arquivos corrompidos anteriores\n",
        "                ts = int(datetime.now(timezone.utc).timestamp())\n",
        "                corrupt_path = path.with_name(f\"{path.stem}.corrupt.{ts}{path.suffix}\")\n",
        "            path.replace(corrupt_path)\n",
        "        except Exception as ee:\n",
        "            logger.error(f\"Falha ao mover arquivo corrompido '{path}': {ee}. Tentando apagar.\")\n",
        "            if path.exists(): path.unlink(missing_ok=True)\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Classe ExperimentCache Aprimorada ---\n",
        "\n",
        "class ExperimentCache:\n",
        "    \"\"\"\n",
        "    Sistema de cache robusto com escrita atômica, autocorreção e metadados de rastreabilidade.\n",
        "    \"\"\"\n",
        "    def __init__(self, cache_dir: str = \".rag_cache\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.gt_cache_dir = self.cache_dir / \"ground_truth\"\n",
        "        self.vectorstore_meta_dir = self.cache_dir / \"vectorstore_meta\"\n",
        "        self.gt_cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.vectorstore_meta_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.logger.info(f\"Cache inicializado no diretório: {self.cache_dir.resolve()}\")\n",
        "\n",
        "\n",
        "    def _hash_docs_content(self, documents: List[Any]) -> str:\n",
        "        \"\"\"Gera um hash do conteúdo dos documentos para detectar mudanças.\"\"\"\n",
        "        parts = []\n",
        "        for doc in sorted(documents, key=lambda d: getattr(d, \"metadata\", {}).get(\"source\", \"\")):\n",
        "            # Corrected: Assign meta first before using it\n",
        "            meta = getattr(doc, \"metadata\", {}) or {}\n",
        "            src = str(meta.get(\"source\", \"unknown\"))\n",
        "            page = str(meta.get(\"page\", \"0\"))\n",
        "            sample = (getattr(doc, \"page_content\", \"\") or \"\")[:200]\n",
        "            parts.append(f\"{src}|p{page}|{sample}\")\n",
        "        return hashlib.md5(\"\\n\".join(parts).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "    def _get_gt_cache_path(self, query: str, docs_hash: str) -> Path:\n",
        "        query_hash = hashlib.sha1(query.encode()).hexdigest()[:12]\n",
        "        return self.gt_cache_dir / f\"gt_{query_hash}_{docs_hash}.json\"\n",
        "\n",
        "    def get_ground_truth(self, query: str, documents: List[Any]) -> Optional[Set[str]]:\n",
        "        \"\"\"Carrega o GT do cache, validando o conteúdo e retornando um set.\"\"\"\n",
        "        docs_hash = self._hash_docs_content(documents)\n",
        "        cache_path = self._get_gt_cache_path(query, docs_hash)\n",
        "        if not cache_path.exists(): return None\n",
        "\n",
        "        data = _load_json_safe(cache_path, self.logger)\n",
        "        if not data: return None\n",
        "\n",
        "        relevant_ids = data.get(\"relevant_ids\")\n",
        "        if relevant_ids is None:\n",
        "            self.logger.warning(f\"Cache de GT em '{cache_path}' não tem a chave 'relevant_ids'. Tratando como corrompido.\")\n",
        "            corrupt_path = cache_path.with_suffix(cache_path.suffix + \".invalid\")\n",
        "            cache_path.replace(corrupt_path)\n",
        "            return None\n",
        "\n",
        "        return set(relevant_ids)\n",
        "\n",
        "    def save_ground_truth(self, ground_truth_set: Set[str], query: str, documents: List[Any], model_info: Optional[Dict] = None):\n",
        "        \"\"\"Salva o GT em um formato JSON estruturado usando escrita atômica e com metadados.\"\"\"\n",
        "        docs_hash = self._hash_docs_content(documents)\n",
        "        cache_path = self._get_gt_cache_path(query, docs_hash)\n",
        "\n",
        "        payload = {\n",
        "            \"schema_version\": 1.1,\n",
        "            \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "            \"query\": query,\n",
        "            \"doc_hash\": docs_hash,\n",
        "            \"relevant_ids\": sorted(list(ground_truth_set)),\n",
        "            \"meta\": model_info or {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            _safe_write_json(cache_path, payload)\n",
        "            self.logger.info(f\"Ground Truth salvo em: {cache_path} (itens: {len(ground_truth_set)})\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Falha ao salvar Ground Truth de forma segura: {e}\")\n",
        "\n",
        "    # --- Métodos de metadados do Vectorstore ---\n",
        "    def save_vectorstore_meta(self, cache_folder: str, meta: Dict[str, Any]):\n",
        "        meta_path = self.vectorstore_meta_dir / f\"{Path(cache_folder).name}_meta.json\"\n",
        "        try:\n",
        "            _safe_write_json(meta_path, meta)\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Falha ao salvar metadados do vectorstore: {e}\")\n",
        "\n",
        "    def load_vectorstore_meta(self, cache_folder: str) -> Optional[Dict[str, Any]]:\n",
        "        meta_path = self.vectorstore_meta_dir / f\"{Path(cache_folder).name}_meta.json\"\n",
        "        if meta_path.exists():\n",
        "            return _load_json_safe(meta_path, self.logger)\n",
        "        return None"
      ],
      "metadata": {
        "id": "CXK92pY43OBK"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 5: Implementando o Adaptador de LLM**\n",
        "\n",
        "\"\"\"\n",
        "A classe `LLMAdapter` atua como uma camada de adaptação entre nosso sistema e qualquer cliente LLM.\n",
        "Sua única responsabilidade é fornecer uma interface unificada, resiliente e previsível para chamadas\n",
        "a LLMs, independentemente da biblioteca utilizada.\n",
        "\n",
        "Ela resolve três problemas principais:\n",
        "1. Diferentes formas de invocar LLMs (`invoke`, `generate`, função direta).\n",
        "2. Diferentes formatos de resposta (AIMessage, Generation, dict, string).\n",
        "3. Fragilidade em chamadas de API (adiciona retry com exponential backoff).\n"
      ],
      "metadata": {
        "id": "EomExbgxE5HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMAdapter:\n",
        "    \"\"\"\n",
        "    Adapter para normalizar diferentes clientes LLM.\n",
        "    Responsabilidade única: interface unificada para LLMs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm, max_retries: int = 2, retry_backoff: float = 1.0):\n",
        "        self.llm = llm\n",
        "        self.max_retries = max_retries\n",
        "        self.retry_backoff = retry_backoff\n",
        "\n",
        "    def invoke(self, prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Invoca o LLM com retry automático.\"\"\"\n",
        "        last_exception = None\n",
        "\n",
        "        for attempt in range(self.max_retries + 1):\n",
        "            try:\n",
        "                response = self._try_invoke(prompt)\n",
        "                return self._normalize_response(response)\n",
        "            except Exception as e:\n",
        "                last_exception = e\n",
        "                if attempt < self.max_retries:\n",
        "                    # Implementa Exponential Backoff\n",
        "                    wait_time = self.retry_backoff * (2 ** attempt)\n",
        "                    print(f\"⚠️ Erro na chamada do LLM: {e}. Tentando novamente em {wait_time:.1f}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "\n",
        "        # Se todas as tentativas falharem, retorna uma resposta de erro padronizada\n",
        "        return {\"content\": f\"__LLM_ERROR__: {str(last_exception)}\"}\n",
        "\n",
        "    def _try_invoke(self, prompt: str) -> Any:\n",
        "        \"\"\"Tenta invocar o LLM usando diferentes interfaces comuns.\"\"\"\n",
        "        if hasattr(self.llm, \"invoke\"):\n",
        "            return self.llm.invoke(prompt)\n",
        "        if hasattr(self.llm, \"generate\"):\n",
        "            return self.llm.generate([prompt])\n",
        "        # Fallback para objetos que são \"chamáveis\" (callable)\n",
        "        if callable(self.llm):\n",
        "            return self.llm(prompt)\n",
        "        raise TypeError(\"O objeto LLM fornecido não é chamável nem possui os métodos 'invoke' ou 'generate'.\")\n",
        "\n",
        "    def _normalize_response(self, response: Any) -> Dict[str, Any]:\n",
        "        \"\"\"Normaliza diferentes formatos de resposta para um formato padrão e seguro.\"\"\"\n",
        "        try:\n",
        "            if isinstance(response, dict) and \"content\" in response:\n",
        "                return response\n",
        "            if hasattr(response, \"generations\"): # Formato comum no LangChain\n",
        "                gen = response.generations[0][0]\n",
        "                text = getattr(gen, \"text\", str(gen))\n",
        "                return {\"content\": text}\n",
        "            if hasattr(response, \"content\"): # Formato comum em objetos AIMessage\n",
        "                return {\"content\": response.content}\n",
        "            if hasattr(response, \"text\"):\n",
        "                return {\"content\": response.text}\n",
        "            if isinstance(response, str):\n",
        "                return {\"content\": response}\n",
        "            # Se nada der certo, converte a resposta inteira para string\n",
        "            return {\"content\": str(response)}\n",
        "        except Exception as e:\n",
        "            # Retorna uma resposta de erro padronizada se a normalização falhar\n",
        "            return {\"content\": f\"__LLM_NORMALIZE_ERROR__: {e} -- raw: {str(response)}\"}"
      ],
      "metadata": {
        "id": "l903EzGME9t0"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 6: Implementando o \"LLM Juiz\" (Relevance Evaluator)**\n",
        "\n",
        "Esta classe, `RelevanceEvaluator`, é o nosso \"LLM Juiz\" encapsulado.  \n",
        "Ela é a segunda implementação de um componente focado no **Princípio da Responsabilidade Única**.  \n",
        "\n",
        "Sua única missão é: **receber uma pergunta e uma lista de trechos (`chunks`) e determinar, com a ajuda de um LLM, se cada trecho é relevante para responder à pergunta.**\n",
        "\n",
        "Ao isolar essa lógica em sua própria classe, nosso sistema principal (`AdvancedRAGAnalyzer`) se torna mais limpo e independente dos detalhes de como a relevância é julgada.\n",
        "\n",
        "---\n",
        "\n",
        "### **Responsabilidades da Classe**\n",
        "\n",
        "1. **Construção de Prompts (`_build_batch_prompt`):**\n",
        "   - **Função:** Cria o prompt exato que será enviado ao LLM.  \n",
        "     O formato exige resposta **em JSON válido e estrito**, o que aumenta a confiabilidade.  \n",
        "   - **Benefício:** Centraliza a engenharia de prompt em um só método.\n",
        "\n",
        "2. **Parsing Robusto da Resposta (`_parse_json_from_response`):**\n",
        "   - **Função:** Extrai e valida o JSON retornado pelo LLM, tolerando pequenas variações na saída.  \n",
        "   - **Benefício:** Torna a aplicação resiliente a erros de API e outputs inesperados.\n",
        "\n",
        "3. **Orquestração da Avaliação (`evaluate_batch`):**\n",
        "   - **Função:** Coordena o fluxo: divide os trechos em lotes, constrói prompts, chama o LLM e interpreta as respostas.  \n",
        "   - **Benefício:** Mantém a classe testável e previsível.\n",
        "\n",
        "---\n",
        "\n",
        "> **Nota de Design:**  \n",
        "> A classe foi projetada para suportar avaliação em lote (batch), garantindo eficiência no uso da API.  \n",
        "> Em um cenário real, é recomendável criar **testes unitários específicos para `_parse_json_from_response`**, simulando saídas bem e mal formatadas do LLM, sem precisar consumir a API de verdade.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-sQOLlEfFCtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelevanceEvaluator:\n",
        "    \"\"\"\n",
        "    Avalia a relevância de chunks usando um LLM de forma eficiente e robusta.\n",
        "\n",
        "    - Suporta avaliação em lote (batch) para reduzir chamadas de API.\n",
        "    - Espera que cada chunk possua um 'chunk_id' único e estável.\n",
        "    - Exige que o LLM responda em JSON válido para maior confiabilidade.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm_adapter: LLMAdapter,\n",
        "                 confidence_threshold: float = 50.0,\n",
        "                 batch_size: int = 5):\n",
        "        self.llm_adapter = llm_adapter\n",
        "        self.confidence_threshold = float(confidence_threshold)\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.SYSTEM_INSTRUCTION = (\n",
        "            \"Você é um avaliador de relevância. RESPONDA APENAS com um JSON válido e nada mais. \"\n",
        "            \"Formato esperado: {\\\"results\\\":[{\\\"id\\\":\\\"<chunk_id>\\\",\\\"is_relevant\\\":\\\"SIM\\\"|\\\"NAO\\\",\\\"confidence\\\":0-100}]}\"\n",
        "        )\n",
        "\n",
        "    def evaluate_batch(self, query: str, items: List[Tuple[str, str]]) -> Dict[str, Tuple[bool, float]]:\n",
        "        \"\"\"\n",
        "        Avalia uma lista de trechos (chunks) em lotes.\n",
        "\n",
        "        Args:\n",
        "            query (str): Pergunta que guia a avaliação.\n",
        "            items (List[Tuple[str, str]]): Lista de tuplas (chunk_id, snippet).\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Tuple[bool, float]]:\n",
        "                Mapeamento chunk_id -> (is_relevant, confidence).\n",
        "        \"\"\"\n",
        "        results: Dict[str, Tuple[bool, float]] = {}\n",
        "        for i in range(0, len(items), self.batch_size):\n",
        "            sub_batch = items[i:i + self.batch_size]\n",
        "            prompt = self._build_batch_prompt(query, sub_batch)\n",
        "\n",
        "            raw_content = self.llm_adapter.invoke(prompt).get(\"content\", \"\")\n",
        "            parsed_batch = self._parse_json_from_response(raw_content)\n",
        "\n",
        "            if parsed_batch is None:\n",
        "                # Retry com instrução ainda mais explícita se o parsing falhar\n",
        "                raw_content_retry = self.llm_adapter.invoke(\"RESPOSTA APENAS COM JSON: \" + prompt).get(\"content\", \"\")\n",
        "                parsed_batch = self._parse_json_from_response(raw_content_retry)\n",
        "\n",
        "            parsed_map = {res.get(\"id\"): res for res in parsed_batch.get(\"results\", [])} if parsed_batch else {}\n",
        "\n",
        "            for chunk_id, _ in sub_batch:\n",
        "                item = parsed_map.get(chunk_id)\n",
        "                if not item:\n",
        "                    # Fallback se o LLM não retornar um item esperado\n",
        "                    results[chunk_id] = (False, 0.0)\n",
        "                    continue\n",
        "\n",
        "                is_relevant_text = str(item.get(\"is_relevant\", \"\")).upper()\n",
        "                confidence = float(item.get(\"confidence\", 0.0))\n",
        "\n",
        "                is_relevant = (\"SIM\" in is_relevant_text) and (confidence >= self.confidence_threshold)\n",
        "                results[chunk_id] = (is_relevant, confidence)\n",
        "        return results\n",
        "\n",
        "    def _build_batch_prompt(self, query: str, items: List[Tuple[str, str]]) -> str:\n",
        "        \"\"\"Constrói o prompt para avaliação de vários trechos de uma vez.\"\"\"\n",
        "        prompt_snippets = \"\"\n",
        "        for chunk_id, snippet in items:\n",
        "            # json.dumps garante que o texto seja serializado de forma segura\n",
        "            sanitized_snippet = json.dumps(snippet[:1200])\n",
        "            prompt_snippets += f\"[ID:{chunk_id}]\\n{sanitized_snippet}\\n\\n\"\n",
        "\n",
        "        return (\n",
        "            f\"{self.SYSTEM_INSTRUCTION}\\n\"\n",
        "            f\"PERGUNTA: \\\"{query}\\\"\\n\\n\"\n",
        "            f\"TRECHOS PARA AVALIAR:\\n{prompt_snippets}\"\n",
        "            \"RETORNE apenas o JSON pedido.\"\n",
        "        )\n",
        "\n",
        "    def _parse_json_from_response(self, raw_text: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrai e valida o primeiro JSON encontrado em um texto.\n",
        "\n",
        "        Args:\n",
        "            raw_text (str): Saída textual do LLM.\n",
        "\n",
        "        Returns:\n",
        "            Optional[Dict[str, Any]]: Estrutura JSON extraída, ou None em caso de falha.\n",
        "        \"\"\"\n",
        "        if not raw_text:\n",
        "            return None\n",
        "        match = re.search(r'\\{.*\\}', raw_text, re.DOTALL)\n",
        "        if not match:\n",
        "            return None\n",
        "        try:\n",
        "            return json.loads(match.group(0))\n",
        "        except json.JSONDecodeError:\n",
        "            return None  # Retorna None se o JSON estiver malformado\n"
      ],
      "metadata": {
        "id": "sn0v67NzFCZc"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 7: Implementando a Calculadora de Métricas**\n",
        "\n",
        "A classe `MetricsCalculator` é o nosso especialista em matemática.  \n",
        "Ela encapsula toda a lógica para calcular as métricas de performance que definimos em nossos modelos de dados.\n",
        "\n",
        "Seguindo o **Princípio da Responsabilidade Única**, sua única função é:  \n",
        "**receber os resultados de um experimento e um \"gabarito\" (`ground_truth_set`) e transformá-los em números que nos dizem quão bem o sistema se comportou.**\n",
        "\n",
        "Ao isolar esses cálculos, o nosso orquestrador (`AdvancedRAGAnalyzer`) se torna mais limpo e focado em sua tarefa principal: **gerenciar o fluxo do experimento.**\n",
        "\n",
        "---\n",
        "\n",
        "### **Responsabilidades da Classe**\n",
        "\n",
        "1. **Cálculo de Métricas Principais (`calculate_metrics`):**\n",
        "   - **Função:** Método principal que calcula a tríade sagrada:\n",
        "     - **Precision (Precisão):** Dos documentos recuperados, quantos eram corretos?\n",
        "     - **Recall (Revocação):** De todos os documentos corretos, quantos foram recuperados?\n",
        "     - **F1-Score:** A média harmônica entre Precision e Recall, que resume o equilíbrio do sistema.\n",
        "   - **Benefício:** Fornece uma visão holística da qualidade do sistema.\n",
        "\n",
        "2. **Cálculo de Precision@K (`_calculate_precision_at_k`):**\n",
        "   - **Função:** Mede a precisão considerando apenas os **k** primeiros resultados.  \n",
        "   - **Benefício:** Essencial para avaliar a \"primeira página\" de resultados, já que usuários raramente olham além dela.\n",
        "\n",
        "3. **Otimização de Threshold (`find_optimal_threshold`):**\n",
        "   - **Função:** Itera sobre os scores de similaridade e encontra o `threshold` que maximiza o F1-Score.  \n",
        "   - **Benefício:** Fornece uma recomendação quantitativa para ajustar o parâmetro `score_threshold` do sistema.\n",
        "\n",
        "---\n",
        "\n",
        "> **Nota de Design:**  \n",
        "> Todos os métodos são `@staticmethod`, ou seja, **não dependem de estado interno**.  \n",
        "> Isso os torna **funções puras**, fáceis de testar isoladamente e reutilizar em outros projetos.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kIHubYHeFO2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Any, Set, Dict, Union\n",
        "\n",
        "class MetricsCalculator:\n",
        "    @staticmethod\n",
        "    def calculate_metrics(\n",
        "        results: List[ChunkAnalysisResult],\n",
        "        ground_truth_docs: List[Any],\n",
        "        k_values: List[int] = None\n",
        "    ) -> \"AnalysisMetrics\":\n",
        "        \"\"\"\n",
        "        Calcula métricas comparando o CONTEÚDO dos chunks.\n",
        "        Aceita ground_truth_docs como:\n",
        "         - lista de Documents (com .page_content),\n",
        "         - lista/iterable de strings (conteúdo),\n",
        "         - ou lista de ids (nesse caso você deve fornecer um map id->content).\n",
        "        \"\"\"\n",
        "\n",
        "        if k_values is None:\n",
        "            k_values = [1, 3, 5, 8, 10]\n",
        "\n",
        "        # --- Normaliza ground truth para um set de strings (conteúdos) ---\n",
        "        gt_contents: Set[str] = set()\n",
        "        if not ground_truth_docs:\n",
        "            gt_contents = set()\n",
        "        else:\n",
        "            # Se for lista/iterável de strings\n",
        "            if all(isinstance(x, str) for x in ground_truth_docs):\n",
        "                gt_contents = set(ground_truth_docs)\n",
        "            else:\n",
        "                # Tenta extrair .page_content quando possível (Document-like)\n",
        "                extracted = set()\n",
        "                for item in ground_truth_docs:\n",
        "                    content = None\n",
        "                    try:\n",
        "                        content = getattr(item, \"page_content\", None)\n",
        "                    except Exception:\n",
        "                        content = None\n",
        "                    if content is None:\n",
        "                        # fallback para str(item)\n",
        "                        content = str(item)\n",
        "                    extracted.add(content)\n",
        "                gt_contents = extracted\n",
        "\n",
        "        # Recuperados (ordenados)\n",
        "        retrieved_contents = [res.content for res in results]\n",
        "\n",
        "        # Conta true positives por conteúdo (substring match — robusto entre chunkings)\n",
        "        true_positives = 0\n",
        "        for gt_text in gt_contents:\n",
        "            if any(gt_text in retrieved_text for retrieved_text in retrieved_contents):\n",
        "                true_positives += 1\n",
        "\n",
        "        total_retrieved = len(retrieved_contents)\n",
        "        total_relevant_in_corpus = len(gt_contents)\n",
        "\n",
        "        precision = true_positives / total_retrieved if total_retrieved > 0 else 0.0\n",
        "        recall = true_positives / total_relevant_in_corpus if total_relevant_in_corpus > 0 else 0.0\n",
        "        f1_score = (2 * (precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "        precision_at_k = MetricsCalculator._calculate_precision_at_k(results, gt_contents, k_values)\n",
        "\n",
        "        return AnalysisMetrics(\n",
        "            precision=precision,\n",
        "            recall=recall,\n",
        "            f1_score=f1_score,\n",
        "            precision_at_k=precision_at_k,\n",
        "            true_positives=true_positives,\n",
        "            retrieved_count=total_retrieved,\n",
        "            ground_truth_count=total_relevant_in_corpus\n",
        "        )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_precision_at_k_by_id(results: List[ChunkAnalysisResult], ground_truth_ids: Set[str], k_values: List[int]) -> Dict[int, float]:\n",
        "        \"\"\"Precision@K usando comparação por chunk_id (IDs).\"\"\"\n",
        "        precision_at_k = {}\n",
        "        retrieved_ids = [getattr(r, \"id\", None) for r in results]\n",
        "        for k in k_values:\n",
        "            if k <= len(retrieved_ids):\n",
        "                top_k_ids = [i for i in retrieved_ids[:k] if i is not None]\n",
        "                tp_at_k = len(set(top_k_ids).intersection(ground_truth_ids))\n",
        "                precision_at_k[k] = tp_at_k / k if k > 0 else 0.0\n",
        "        return precision_at_k\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_precision_at_k_by_content(results: List[ChunkAnalysisResult], gt_contents: Set[str], k_values: List[int]) -> Dict[int, float]:\n",
        "        \"\"\"Precision@K usando comparação por conteúdo (fallback).\"\"\"\n",
        "        precision_at_k = {}\n",
        "        for k in k_values:\n",
        "            if k <= len(results):\n",
        "                top_k_contents = [res.content for res in results[:k]]\n",
        "                tp_at_k = 0\n",
        "                for gt_text in gt_contents:\n",
        "                    if any(gt_text in retrieved_text for retrieved_text in top_k_contents):\n",
        "                        tp_at_k += 1\n",
        "                precision_at_k[k] = tp_at_k / k if k > 0 else 0.0\n",
        "        return precision_at_k\n",
        "\n",
        "    @staticmethod\n",
        "    def find_optimal_threshold(results: List[ChunkAnalysisResult]) -> float:\n",
        "        # (mantive tua implementação — sem alterações)\n",
        "        if not results:\n",
        "            return 0.0\n",
        "\n",
        "        sorted_results = sorted(results, key=lambda x: x.score, reverse=True)\n",
        "        best_threshold = sorted_results[0].score\n",
        "        best_f1 = 0.0\n",
        "        total_relevant_in_results = sum(1 for r in sorted_results if r.is_relevant)\n",
        "\n",
        "        if total_relevant_in_results == 0:\n",
        "            return best_threshold\n",
        "\n",
        "        for result in sorted_results:\n",
        "            threshold = result.score\n",
        "            filtered = [x for x in sorted_results if x.score >= threshold]\n",
        "            relevant_retrieved = sum(1 for x in filtered if x.is_relevant)\n",
        "            total_retrieved = len(filtered)\n",
        "            precision = relevant_retrieved / total_retrieved if total_retrieved else 0\n",
        "            recall = relevant_retrieved / total_relevant_in_results if total_relevant_in_results else 0\n",
        "            f1 = (2 * (precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "            if f1 >= best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        return float(best_threshold)"
      ],
      "metadata": {
        "id": "Rgmjse0DFUWS"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 8: Implementando o \"Anotador de Dados\" (Ground Truth Generator)**\n",
        "\n",
        "Esta classe, `GroundTruthGenerator`, é o componente que automatiza a tarefa mais crítica e trabalhosa em qualquer projeto de avaliação de Machine Learning: a criação de um **\"ground truth\"** ou \"gabarito\".\n",
        "\n",
        "Sua **responsabilidade única** é iterar sobre um conjunto completo de documentos (`chunks`) e, para uma pergunta específica, usar a inteligência de um LLM para criar uma lista definitiva de quais chunks contêm a resposta. Este processo é conhecido como **`LLM-derived ground-truth`**.\n",
        "\n",
        "### **Responsabilidades da Classe:**\n",
        "\n",
        "1.  **Orquestração da Análise (`derive_ground_truth`):**\n",
        "    *   **Função:** O método principal que gerencia o processo. Ele percorre cada chunk do nosso corpus, um por um, e chama o LLM para julgá-lo.\n",
        "    *   **Importância:** Este é um processo que consome muitos recursos (tempo e chamadas de API), pois analisa **todo** o nosso conhecimento, não apenas os resultados de uma busca. Ele é executado apenas uma vez no início do nosso experimento para criar o gabarito.\n",
        "\n",
        "2.  **Avaliação Individual (`_is_chunk_relevant`):**\n",
        "    *   **Função:** Constrói um prompt simples e direto (SIM/NÃO) e usa o `LLMAdapter` para obter o veredito do LLM sobre um único chunk.\n",
        "    *   **Design:** A lógica está isolada neste método, tornando-o fácil de modificar no futuro se quisermos usar um prompt de avaliação diferente.\n",
        "\n",
        "> **Nota para o Futuro:** A criação de um `ground_truth` é o que nos permite calcular métricas objetivas como **Recall** e **F1-Score**. Sem um gabarito, só poderíamos medir a Precision (a qualidade do que foi recuperado), mas nunca saberíamos o que ficou de fora (Recall). Esta classe, embora simples, é o que eleva nosso framework de uma simples ferramenta de diagnóstico para uma suíte de avaliação de performance completa."
      ],
      "metadata": {
        "id": "wAsoWEeVFXU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Set\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "class GroundTruthResult(BaseModel):\n",
        "    \"\"\"Modelo validado para cada item do ground truth.\"\"\"\n",
        "    id: str = Field(..., description=\"Identificador único do chunk avaliado\")\n",
        "    is_relevant: str = Field(..., pattern=\"^(SIM|NAO)$\", description=\"Relevância do chunk\")\n",
        "\n",
        "\n",
        "class GroundTruthGenerator:\n",
        "    \"\"\"\n",
        "    Classe responsável por derivar o 'ground truth' usando um LLM.\n",
        "    Ela avalia todos os chunks disponíveis e determina, via prompt estruturado,\n",
        "    quais realmente contêm informação suficiente para responder a uma query.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_adapter: \"LLMAdapter\"):\n",
        "        self.llm_adapter = llm_adapter\n",
        "\n",
        "    def _parse_batch_response(self, raw_text: str) -> Dict[str, GroundTruthResult]:\n",
        "        \"\"\"Extrai e parseia um JSON da resposta do LLM de forma robusta.\"\"\"\n",
        "        match = re.search(r'\\{.*\\}', raw_text, re.DOTALL)\n",
        "        if not match:\n",
        "            # Heurística fallback: detecta \"SIM\" solto no texto\n",
        "            if re.search(r'\\bSIM\\b', raw_text, re.IGNORECASE):\n",
        "                first_id_match = re.search(r'\\[ID:(.+?)\\]', raw_text)\n",
        "                if first_id_match:\n",
        "                    return {\n",
        "                        first_id_match.group(1): GroundTruthResult(\n",
        "                            id=first_id_match.group(1),\n",
        "                            is_relevant=\"SIM\"\n",
        "                        )\n",
        "                    }\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            parsed_json = json.loads(match.group(0))\n",
        "            results_list = parsed_json.get(\"results\", [])\n",
        "\n",
        "            validated_results = {}\n",
        "            for item in results_list:\n",
        "                try:\n",
        "                    result = GroundTruthResult(**item)\n",
        "                    validated_results[result.id] = result\n",
        "                except ValidationError:\n",
        "                    continue  # ignora itens inválidos\n",
        "            return validated_results\n",
        "\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            return {}\n",
        "\n",
        "    def derive_ground_truth(\n",
        "        self,\n",
        "        chunks: List[Document],\n",
        "        query: str,\n",
        "        batch_size: int = 5,\n",
        "        pause_between: float = 6.0\n",
        "    ) -> Set[str]:\n",
        "        \"\"\"\n",
        "        Deriva o conjunto de IDs de chunks relevantes para a query fornecida.\n",
        "        Analisa TODOS os chunks, em lotes, usando o LLM.\n",
        "        \"\"\"\n",
        "        print(f\"\\n🤖 Derivando Ground Truth com LLM (lotes de {batch_size}) para: '{query}'\")\n",
        "        print(f\"📚 Total de chunks a analisar: {len(chunks)}\")\n",
        "\n",
        "        relevant_chunk_ids: Set[str] = set()\n",
        "\n",
        "        system_prompt = (\n",
        "            \"Você é um avaliador de relevância. Analise a lista de trechos e, para cada um, \"\n",
        "            \"determine se ele contém informação suficiente para responder à pergunta. \"\n",
        "            \"RESPONDA APENAS COM UM ÚNICO BLOCO JSON VÁLIDO contendo um array 'results'.\"\n",
        "        )\n",
        "\n",
        "        for i in range(0, len(chunks), batch_size):\n",
        "            batch_chunks = chunks[i:i + batch_size]\n",
        "\n",
        "            prompt_snippets = \"\"\n",
        "            for chunk in batch_chunks:\n",
        "                chunk_id = chunk.metadata.get(\"chunk_id\", f\"chunk_{i}\")\n",
        "                snippet = chunk.page_content.replace(\"\\n\", \" \").replace('\"', \"'\")[:700]\n",
        "                prompt_snippets += f\"[ID:{chunk_id}]\\n{snippet}\\n\\n\"\n",
        "\n",
        "            prompt = (\n",
        "                f\"{system_prompt}\\n\\n\"\n",
        "                f\"PERGUNTA: \\\"{query}\\\"\\n\\n\"\n",
        "                f\"TRECHOS PARA AVALIAR:\\n{prompt_snippets}\"\n",
        "                \"FORMATO DE RESPOSTA (JSON):\\n\"\n",
        "                \"{\\\"results\\\":[{\\\"id\\\":\\\"ID_DO_TRECHO_1\\\",\\\"is_relevant\\\":\\\"SIM\\\"|\\\"NAO\\\"},\"\n",
        "                \" {\\\"id\\\":\\\"ID_DO_TRECHO_2\\\",\\\"is_relevant\\\":\\\"SIM\\\"|\\\"NAO\\\"}]}\"\n",
        "            )\n",
        "\n",
        "            response_text = self.llm_adapter.invoke(prompt).get(\"content\", \"\")\n",
        "            parsed_results = self._parse_batch_response(response_text)\n",
        "\n",
        "            # Atualiza o ground truth\n",
        "            for chunk in batch_chunks:\n",
        "                chunk_id = chunk.metadata.get(\"chunk_id\")\n",
        "                result_for_chunk = parsed_results.get(chunk_id)\n",
        "                is_relevant = result_for_chunk and result_for_chunk.is_relevant == \"SIM\"\n",
        "\n",
        "                if is_relevant:\n",
        "                    relevant_chunk_ids.add(chunk_id)\n",
        "\n",
        "                print(f\"  - Chunk ID {chunk_id}: {'✅ Relevante' if is_relevant else '❌ Irrelevante'}\")\n",
        "\n",
        "            if i + batch_size < len(chunks):\n",
        "                time.sleep(pause_between)\n",
        "\n",
        "        print(f\"\\n✅ Ground Truth concluído! Total relevantes: {len(relevant_chunk_ids)}\")\n",
        "        return relevant_chunk_ids"
      ],
      "metadata": {
        "id": "a21VGHbFFdeu"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 9: O Orquestrador Principal (RAG Analyzer)**\n",
        "\n",
        "Finalmente, chegamos ao cérebro da nossa operação: a classe `RAGAnalyzer`.\n",
        "\n",
        "Após termos criado nossos componentes especialistas (`LLMAdapter`, `RelevanceEvaluator`, `MetricsCalculator`, `GroundTruthGenerator`), cada um com sua responsabilidade única, o `RAGAnalyzer` atua como o **maestro da orquestra**. Sua única função é **coordenar** esses especialistas para executar o fluxo de trabalho completo de uma análise de RAG.\n",
        "\n",
        "Este design, baseado no princípio de **Composição**, resulta em um código limpo, desacoplado e que espelha uma arquitetura de software de produção.\n",
        "\n",
        "### **Responsabilidades da Classe:**\n",
        "\n",
        "1.  **Composição dos Especialistas (`__init__`):**\n",
        "    *   **Função:** No momento de sua criação, o `RAGAnalyzer` constrói as instâncias de todos os seus \"assistentes\" especialistas. Ele recebe o `llm_adapter` como dependência externa, praticando a **Injeção de Dependência**.\n",
        "\n",
        "2.  **Orquestração da Análise (`analyze_strategy`):**\n",
        "    *   **Função:** Este é o método principal. Ele executa a sequência lógica de uma análise:\n",
        "        1.  Delega a busca de documentos para o `VectorStore`.\n",
        "        2.  Delega a avaliação de cada documento recuperado para o `RelevanceEvaluator`.\n",
        "        3.  Delega o cálculo das métricas finais para o `MetricsCalculator`.\n",
        "        4.  Monta e armazena o resultado final em um objeto `RetrievalExperiment`.\n",
        "\n",
        "3.  **Apresentação dos Resultados (`_display_results`):**\n",
        "    *   **Função:** Formata e imprime um resumo claro e informativo dos resultados de um experimento, tornando a saída fácil de interpretar.\n",
        "\n",
        "4.  **Interface de Conveniência (`generate_ground_truth`):**\n",
        "    *   **Função:** Fornece um atalho simples para que o usuário final possa, através do `RAGAnalyzer`, acessar a funcionalidade do `GroundTruthGenerator` sem precisar interagir com ele diretamente.\n",
        "\n",
        "> **Nota para o Futuro:** A beleza desta arquitetura é a sua extensibilidade. Se quisermos adicionar um novo passo ao nosso pipeline (por exemplo, um \"Re-ranker\" que reordena os resultados antes da avaliação), precisaríamos apenas criar uma nova classe especialista (`ReRanker`) e adicionar uma nova chamada de delegação no método `analyze_strategy`. O resto do sistema permaneceria inalterado."
      ],
      "metadata": {
        "id": "W2L5r-_xFgpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import re\n",
        "from types import SimpleNamespace\n",
        "from typing import List, Tuple, Optional, Any, Dict\n",
        "\n",
        "class RAGAnalyzer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm_adapter: Any,\n",
        "        relevance_evaluator: Optional[Any] = None,\n",
        "        metrics_calculator: Optional[Any] = None,\n",
        "        ground_truth_generator: Optional[Any] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        llm_adapter: adaptador para chamar o LLM (deve expor .invoke(prompt) -> dict/obj com \"content\")\n",
        "        relevance_evaluator: (opcional) instância com método evaluate_batch(query, items)->dict{chunk_id:(bool,float)}\n",
        "        metrics_calculator: (opcional) instância com métodos calculate_metrics(...) e find_optimal_threshold(...)\n",
        "        ground_truth_generator: (opcional) instância com método derive_ground_truth(chunks, query, pause)\n",
        "        \"\"\"\n",
        "        self.llm_adapter = llm_adapter\n",
        "\n",
        "        # RelevanceEvaluator\n",
        "        if relevance_evaluator is not None:\n",
        "            self.relevance_evaluator = relevance_evaluator\n",
        "        else:\n",
        "            # tenta instanciar se a classe existir no escopo global\n",
        "            try:\n",
        "                self.relevance_evaluator = RelevanceEvaluator(llm_adapter)  # type: ignore[name-defined]\n",
        "            except NameError:\n",
        "                raise RuntimeError(\n",
        "                    \"RelevanceEvaluator não foi fornecido e não está disponível. \"\n",
        "                    \"Passe uma instância via 'relevance_evaluator' ao criar RAGAnalyzer.\"\n",
        "                )\n",
        "\n",
        "        # MetricsCalculator\n",
        "        if metrics_calculator is not None:\n",
        "            self.metrics_calculator = metrics_calculator\n",
        "        else:\n",
        "            try:\n",
        "                self.metrics_calculator = MetricsCalculator()  # type: ignore[name-defined]\n",
        "            except NameError:\n",
        "                raise RuntimeError(\n",
        "                    \"MetricsCalculator não foi fornecido e não está disponível. \"\n",
        "                    \"Passe uma instância via 'metrics_calculator' ao criar RAGAnalyzer.\"\n",
        "                )\n",
        "\n",
        "        # GroundTruthGenerator\n",
        "        if ground_truth_generator is not None:\n",
        "            self.ground_truth_generator = ground_truth_generator\n",
        "        else:\n",
        "            try:\n",
        "                self.ground_truth_generator = GroundTruthGenerator(llm_adapter)  # type: ignore[name-defined]\n",
        "            except NameError:\n",
        "                raise RuntimeError(\n",
        "                    \"GroundTruthGenerator não foi fornecido e não está disponível. \"\n",
        "                    \"Passe uma instância via 'ground_truth_generator' ao criar RAGAnalyzer.\"\n",
        "                )\n",
        "\n",
        "        # histórico de experimentos (lista de RetrievalExperiment)\n",
        "        self.experiments_history: List[Any] = []\n",
        "\n",
        "    # -----------------------\n",
        "    # Re-ranker com parsing robusto\n",
        "    # -----------------------\n",
        "    def _rerank_results(self, query: str, docs_with_scores: List[Tuple[Any, float]], top_k: int = 10) -> List[Tuple[Any, float]]:\n",
        "        \"\"\"Usa o LLM para reordenar os top_k resultados com parsing de JSON/regex robusto.\"\"\"\n",
        "        top_results = docs_with_scores[:top_k]\n",
        "        if not top_results or len(top_results) < 2:\n",
        "            return docs_with_scores\n",
        "\n",
        "        print(f\"  - 🔄 Re-ranking dos top {len(top_results)} resultados com LLM...\")\n",
        "        id_to_doc_map: Dict[str, Tuple[Any, float]] = {}\n",
        "        items_prompt = \"\"\n",
        "        for doc, score in top_results:\n",
        "            meta = getattr(doc, \"metadata\", {}) or {}\n",
        "            chunk_id = meta.get(\"chunk_id\", None) or f\"id_desconhecido_{abs(hash(doc)) % 100000}\"\n",
        "            id_to_doc_map[chunk_id] = (doc, score)\n",
        "            content_excerpt = (getattr(doc, \"page_content\", \"\") or str(doc))[:300].replace(\"\\n\", \" \")\n",
        "            items_prompt += f\"- {chunk_id}: {content_excerpt}\\n\"\n",
        "\n",
        "        prompt = (\n",
        "            \"Você é um especialista em re-ranking. Dada uma pergunta e uma lista de trechos, \"\n",
        "            \"retorne SOMENTE um objeto JSON com a chave 'ranked' contendo a lista de chunk_ids \"\n",
        "            \"ordenados do mais relevante para o menos relevante para responder à pergunta.\\n\\n\"\n",
        "            f\"PERGUNTA: \\\"{query}\\\"\\n\\nTRECHOS:\\n{items_prompt}\"\n",
        "        )\n",
        "\n",
        "        response = None\n",
        "        try:\n",
        "            response = self.llm_adapter.invoke(prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ Erro ao chamar LLM para re-ranking: {e}. Pulando re-rank.\")\n",
        "            return docs_with_scores\n",
        "\n",
        "        content = \"\"\n",
        "        if isinstance(response, dict):\n",
        "            content = (response.get(\"content\") or \"\") if response is not None else \"\"\n",
        "        else:\n",
        "            content = str(response) if response is not None else \"\"\n",
        "\n",
        "        ranked_ids: List[str] = []\n",
        "        # várias tentativas de parse até encontrar os ids\n",
        "        try:\n",
        "            parsed = json.loads(content)\n",
        "            if isinstance(parsed, dict) and \"ranked\" in parsed:\n",
        "                ranked_ids = parsed[\"ranked\"]\n",
        "        except Exception:\n",
        "            try:\n",
        "                match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "                if match:\n",
        "                    parsed = json.loads(match.group(0))\n",
        "                    ranked_ids = parsed.get(\"ranked\", [])\n",
        "            except Exception:\n",
        "                ranked_ids = re.findall(r\"[\\w\\.\\-:]+::p\\d+::c\\d+\", content)\n",
        "\n",
        "        if not ranked_ids:\n",
        "            maybe_ids = []\n",
        "            for line in content.splitlines():\n",
        "                found = re.findall(r\"[\\w\\.\\-:]+::p\\d+::c\\d+\", line)\n",
        "                maybe_ids.extend(found)\n",
        "            if maybe_ids:\n",
        "                ranked_ids = maybe_ids\n",
        "\n",
        "        if not ranked_ids:\n",
        "            return docs_with_scores\n",
        "\n",
        "        reordered_top: List[Tuple[Any, float]] = []\n",
        "        seen_ids = set()\n",
        "        for cid in ranked_ids:\n",
        "            if cid in id_to_doc_map:\n",
        "                reordered_top.append(id_to_doc_map[cid])\n",
        "                seen_ids.add(cid)\n",
        "\n",
        "        for cid, tuple_doc in id_to_doc_map.items():\n",
        "            if cid not in seen_ids:\n",
        "                reordered_top.append(tuple_doc)\n",
        "\n",
        "        return reordered_top + docs_with_scores[top_k:]\n",
        "\n",
        "    # -----------------------\n",
        "    # Analyze strategy: recuperação, rerank e avaliação\n",
        "    # -----------------------\n",
        "    def analyze_strategy(\n",
        "        self,\n",
        "        vectorstore: Any,\n",
        "        strategy: Any,\n",
        "        query: str,\n",
        "        ground_truth_docs: List[Any],\n",
        "        total_chunks: int,\n",
        "        k_values: Optional[List[int]] = None,\n",
        "        pause_between_evals: float = 0,\n",
        "    ) -> Optional[Any]:\n",
        "        \"\"\"\n",
        "        Executa a avaliação de uma estratégia:\n",
        "         - recupera mais resultados (k * 2) para re-ranking\n",
        "         - re-rankeia com LLM\n",
        "         - avalia top-k com o 'LLM Juiz'\n",
        "         - calcula métricas\n",
        "        \"\"\"\n",
        "        if k_values is None:\n",
        "            k_values = [1, 3, 5, 8, 10]\n",
        "        max_k = max(k_values)\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n🔍 Analisando estratégia: {getattr(strategy, 'name', str(strategy))} (k={max_k})\")\n",
        "\n",
        "        try:\n",
        "            docs_with_scores = vectorstore.similarity_search_with_score(query, k=max_k * 2)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro na recuperação de documentos: {e}\")\n",
        "            return None\n",
        "\n",
        "        if not docs_with_scores:\n",
        "            print(\"  - Nenhum documento encontrado na busca.\")\n",
        "            return None\n",
        "\n",
        "        reranked_docs = self._rerank_results(query, docs_with_scores, top_k=max_k)\n",
        "        results = self._evaluate_retrieved_chunks(reranked_docs[:max_k], query, pause_between_evals)\n",
        "\n",
        "        normalized_gt_docs = self._normalize_ground_truth(ground_truth_docs)\n",
        "        metrics = self.metrics_calculator.calculate_metrics(results, normalized_gt_docs, k_values)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "        # cria um RetrievalExperiment — assumimos que a classe está disponível no escopo\n",
        "        try:\n",
        "            experiment = RetrievalExperiment(\n",
        "                strategy=strategy,\n",
        "                query=query,\n",
        "                k_tested=max_k,\n",
        "                total_chunks=total_chunks,\n",
        "                results=results,\n",
        "                metrics=metrics,\n",
        "                execution_time=execution_time,\n",
        "                recommended_threshold=self.metrics_calculator.find_optimal_threshold(results),\n",
        "            )\n",
        "        except NameError:\n",
        "            # fallback: encapsula num SimpleNamespace se RetrievalExperiment não estiver importado\n",
        "            experiment = SimpleNamespace(\n",
        "                strategy=strategy,\n",
        "                query=query,\n",
        "                k_tested=max_k,\n",
        "                total_chunks=total_chunks,\n",
        "                results=results,\n",
        "                metrics=metrics,\n",
        "                execution_time=execution_time,\n",
        "                recommended_threshold=self.metrics_calculator.find_optimal_threshold(results),\n",
        "            )\n",
        "\n",
        "        self._display_results(experiment)\n",
        "        self.experiments_history.append(experiment)\n",
        "        return experiment\n",
        "\n",
        "    # -----------------------\n",
        "    # Avaliação em lote (LLM Juiz)\n",
        "    # -----------------------\n",
        "    def _evaluate_retrieved_chunks(self, docs_with_scores: List[Tuple[Any, float]], query: str, pause_between_evals: float) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Avalia os chunks recuperados em lote, usando RelevanceEvaluator.evaluate_batch.\n",
        "        Retorna lista de ChunkAnalysisResult (ou SimpleNamespace fallback).\n",
        "        \"\"\"\n",
        "        results: List[Any] = []\n",
        "        print(f\"  - Avaliando {len(docs_with_scores)} chunks recuperados com o 'LLM Juiz'...\")\n",
        "\n",
        "        items_to_evaluate = []\n",
        "        for doc, score in docs_with_scores:\n",
        "            meta = getattr(doc, \"metadata\", {}) or {}\n",
        "            chunk_id = meta.get(\"chunk_id\") or f\"id_nao_encontrado_{abs(hash(doc)) % 100000}\"\n",
        "            content = getattr(doc, \"page_content\", None)\n",
        "            if content is None:\n",
        "                content = str(doc)\n",
        "            items_to_evaluate.append((chunk_id, content))\n",
        "\n",
        "        batch_results: Dict[str, Tuple[bool, float]] = {}\n",
        "        try:\n",
        "            batch_results = self.relevance_evaluator.evaluate_batch(query, items_to_evaluate)\n",
        "            if not isinstance(batch_results, dict):\n",
        "                raise ValueError(\"evaluate_batch retornou formato inesperado\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ Falha na avaliação em lote do RelevanceEvaluator: {e}. Continuando com defaults.\")\n",
        "            batch_results = {cid: (False, 0.0) for cid, _ in items_to_evaluate}\n",
        "\n",
        "        for i, (doc, score) in enumerate(docs_with_scores):\n",
        "            meta = getattr(doc, \"metadata\", {}) or {}\n",
        "            chunk_id = meta.get('chunk_id') or f\"id_nao_encontrado_{i}\"\n",
        "            is_relevant, confidence = batch_results.get(chunk_id, (False, 0.0))\n",
        "\n",
        "            # tenta criar ChunkAnalysisResult; se não existir, usa SimpleNamespace\n",
        "            try:\n",
        "                result = ChunkAnalysisResult(\n",
        "                    id=chunk_id,\n",
        "                    content=getattr(doc, \"page_content\", str(doc)),\n",
        "                    score=float(score),\n",
        "                    is_relevant=bool(is_relevant),\n",
        "                    confidence_score=float(confidence),\n",
        "                    reasoning=\"Avaliado em lote\",\n",
        "                    evaluation_time=None,\n",
        "                )\n",
        "            except NameError:\n",
        "                result = SimpleNamespace(\n",
        "                    id=chunk_id,\n",
        "                    content=getattr(doc, \"page_content\", str(doc)),\n",
        "                    score=float(score),\n",
        "                    is_relevant=bool(is_relevant),\n",
        "                    confidence_score=float(confidence),\n",
        "                    reasoning=\"Avaliado em lote\",\n",
        "                    evaluation_time=None,\n",
        "                )\n",
        "\n",
        "            results.append(result)\n",
        "            print(f\"    - Chunk ID {chunk_id}: {'✅ Relevante' if is_relevant else '❌ Irrelevante'} (Confiança: {confidence:.0f}%)\")\n",
        "\n",
        "            if pause_between_evals and i < len(docs_with_scores) - 1:\n",
        "                try:\n",
        "                    time.sleep(pause_between_evals)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        return results\n",
        "\n",
        "    # -----------------------\n",
        "    # Exibição de resultados\n",
        "    # -----------------------\n",
        "    def _display_results(self, experiment: Any):\n",
        "        \"\"\"Exibe os resultados formatados de um experimento (aceita RetrievalExperiment ou SimpleNamespace).\"\"\"\n",
        "        metrics = getattr(experiment, \"metrics\", None)\n",
        "        if metrics is None:\n",
        "            print(\"Sem métricas para exibir.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 60)\n",
        "        print(\"📊 RESULTADOS DA ANÁLISE\")\n",
        "        print(\"-\" * 60)\n",
        "        strategy = getattr(experiment, \"strategy\", None)\n",
        "        chunk_info = f\" ({getattr(strategy, 'chunk_size', '?')}/{getattr(strategy, 'chunk_overlap', '?')})\"\n",
        "        print(f\"Estratégia: {getattr(strategy, 'name', str(strategy))}{chunk_info}\")\n",
        "        print(f\"Ground Truth: {getattr(metrics, 'ground_truth_count', '?')} chunks relevantes\")\n",
        "        print(f\"Recuperados: {getattr(metrics, 'retrieved_count', '?')} | Acertos: {getattr(metrics, 'true_positives', '?')}\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"🎯 Precision: {getattr(metrics, 'precision', 0.0):.2%}\")\n",
        "        print(f\"🎣 Recall: {getattr(metrics, 'recall', 0.0):.2%}\")\n",
        "        print(f\"⚖️ F1-Score: {getattr(metrics, 'f1_score', 0.0):.2%}\")\n",
        "        print(\"\\n📈 Precision@K:\")\n",
        "        for k, v in sorted(getattr(metrics, \"precision_at_k\", {}).items()):\n",
        "            print(f\"  P@{k}: {v:.2%}\")\n",
        "        if getattr(experiment, \"recommended_threshold\", None) is not None:\n",
        "            print(f\"🎚️ Threshold Recomendado: {experiment.recommended_threshold:.4f}\")\n",
        "        print(f\"⏱️ Tempo Total da Análise: {getattr(experiment, 'execution_time', 0.0):.2f}s\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # -----------------------\n",
        "    # Conveniência: gera ground truth usando o gerador (se precisar)\n",
        "    # -----------------------\n",
        "    def generate_ground_truth(self, chunks: List[Any], query: str, pause_between: float = 0) -> set:\n",
        "        \"\"\"Método de conveniência para acessar o gerador de ground truth.\"\"\"\n",
        "        return self.ground_truth_generator.derive_ground_truth(chunks, query, pause_between)\n",
        "\n",
        "    # -----------------------\n",
        "    # Helpers\n",
        "    # -----------------------\n",
        "    def _normalize_ground_truth(self, ground_truth_docs: List[Any]) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Normaliza o ground_truth para uma lista de objetos com atributo `.page_content`.\n",
        "        Aceita:\n",
        "         - lista/set de objetos que já possuem `.page_content`\n",
        "         - lista/set de strings (cada string será embrulhada em SimpleNamespace(page_content=...))\n",
        "        \"\"\"\n",
        "        if not ground_truth_docs:\n",
        "            return []\n",
        "\n",
        "        normalized: List[Any] = []\n",
        "        for item in ground_truth_docs:\n",
        "            if isinstance(item, str):\n",
        "                normalized.append(SimpleNamespace(page_content=item))\n",
        "            else:\n",
        "                if hasattr(item, \"page_content\"):\n",
        "                    normalized.append(item)\n",
        "                else:\n",
        "                    normalized.append(SimpleNamespace(page_content=str(item)))\n",
        "        return normalized\n"
      ],
      "metadata": {
        "id": "tvKaYR8JFkbu"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 10: Implementando o Utilitário de Carregamento de Documentos**\n",
        "\n",
        "Com todos os nossos componentes de análise e lógica definidos, a última peça da nossa arquitetura de software é um utilitário limpo para a **entrada de dados**.\n",
        "\n",
        "A classe `DocumentLoader` segue o **Princípio da Responsabilidade Única** ao encapsular toda a complexidade de baixar e extrair texto de arquivos PDF a partir de URLs.\n",
        "\n",
        "### **Responsabilidades da Classe:**\n",
        "\n",
        "1.  **Orquestração do Carregamento (`load_from_urls`):**\n",
        "    *   **Função:** Itera sobre uma lista de URLs e gerencia o processo de carregamento de cada um.\n",
        "    *   **Resiliência:** Se o download de um arquivo falhar, ele imprime um erro e continua para o próximo, em vez de quebrar todo o processo.\n",
        "\n",
        "2.  **Lógica de Download e Parsing (`_load_single_url`):**\n",
        "    *   **Função:** Contém a lógica de baixo nível para um único arquivo.\n",
        "    *   **Inteligência de URL:** Inclui uma lógica específica para detectar URLs do GitHub e convertê-los para o formato de download \"raw\" (`/raw/`), garantindo que estamos baixando o arquivo PDF real, e não a página HTML de visualização.\n",
        "    *   **Gerenciamento de Arquivos:** Usa a biblioteca `tempfile` do Python para criar um arquivo temporário de forma segura, garantindo que ele seja automaticamente limpo após o uso, mesmo em caso de erro.\n",
        "\n",
        "> **Nota para o Futuro:** O design desta classe torna muito fácil estendê-la para suportar novos formatos no futuro. Poderíamos adicionar métodos como `_load_single_docx_url` ou `_load_single_html_url` e o método principal `load_from_urls` saberia como chamá-los com base na extensão do arquivo, sem que o resto do nosso notebook precise mudar."
      ],
      "metadata": {
        "id": "6F8S_3ICFoE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentLoader:\n",
        "    \"\"\"\n",
        "    Utilitário para carregar documentos de fontes externas como URLs.\n",
        "    Responsabilidade: Abstrair a lógica de download, parsing e CORREÇÃO\n",
        "    DE METADADOS dos arquivos remotos.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_urls(urls: List[str], timeout: int = 20) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Carrega documentos de uma lista de URLs, lidando com erros individuais.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Etapa 2: Carregando Documentos ---\")\n",
        "        loaded_docs = []\n",
        "\n",
        "        for url in urls:\n",
        "            try:\n",
        "                # O método _load_single_url agora retorna os docs com metadados corrigidos\n",
        "                docs = DocumentLoader._load_single_url(url, timeout)\n",
        "                loaded_docs.extend(docs)\n",
        "                print(f\"  ✅ Carregado: {unquote(url)} ({len(docs)} páginas)\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao carregar {unquote(url)}: {e}\")\n",
        "\n",
        "        print(f\"✅ {len(loaded_docs)} documentos carregados no total.\")\n",
        "        return loaded_docs\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_single_url(url: str, timeout: int) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Carrega um documento PDF de uma única URL, corrigindo os metadados 'source'.\n",
        "        \"\"\"\n",
        "        download_url = url\n",
        "        if \"github.com\" in url and \"/blob/\" in url and \"?raw=true\" not in url:\n",
        "            download_url = url.replace(\"/blob/\", \"/raw/\", 1)\n",
        "\n",
        "        response = requests.get(download_url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".pdf\") as tmp_file:\n",
        "            tmp_file.write(response.content)\n",
        "            tmp_file.flush()\n",
        "\n",
        "            loader = PyMuPDFLoader(tmp_file.name)\n",
        "            docs_carregados = loader.load()\n",
        "\n",
        "            # Decodifica o nome do arquivo original\n",
        "            nome_arquivo_original = unquote(url.split('/')[-1])\n",
        "            # Itera sobre cada página carregada e corrige seus metadados\n",
        "            for doc in docs_carregados:\n",
        "                # Substitui a 'source' temporária pela 'source' permanente\n",
        "                doc.metadata['source'] = nome_arquivo_original\n",
        "\n",
        "            return docs_carregados"
      ],
      "metadata": {
        "id": "dGEw_b-XFrMY"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 11: O Orquestrador de Experimentos (Comparative Analyzer)**\n",
        "\n",
        "Chegamos à camada mais alta da nossa arquitetura de software: o `ComparativeAnalyzer`. Se o `RAGAnalyzer` é o \"motor de análise\" que avalia uma configuração, o `ComparativeAnalyzer` é o \"engenheiro de testes\" que executa uma bateria de experimentos.\n",
        "\n",
        "Sua **responsabilidade única** é gerenciar e comparar múltiplos `RetrievalExperiment`, permitindo-nos testar diferentes hipóteses (neste caso, estratégias de chunking) de forma sistemática para encontrar a configuração ótima.\n",
        "\n",
        "### **Responsabilidades da Classe:**\n",
        "\n",
        "1.  **Orquestração de Alto Nível (`run_analysis`):**\n",
        "    *   **Função:** O método principal que recebe uma lista de `ChunkingStrategy` e, para cada uma, executa o ciclo completo de análise:\n",
        "        1.  Cria os `chunks` e o `VectorStore` específicos para a estratégia.\n",
        "        2.  Delega a análise dessa configuração para o `RAGAnalyzer`.\n",
        "        3.  Coleta e compila as métricas de performance de cada experimento.\n",
        "    *   **Benefício:** Abstrai toda a complexidade de um teste A/B/C, tornando o processo de comparação de estratégias uma única chamada de função.\n",
        "\n",
        "2.  **Gerenciamento do Ciclo de Vida do Experimento:**\n",
        "    *   **Função:** Inclui a lógica para pausar entre os experimentos, garantindo que não sobrecarreguemos a API do LLM com muitas chamadas em um curto período.\n",
        "\n",
        "3.  **Compilação dos Resultados:**\n",
        "    *   **Função:** Ao final de todos os testes, ele consolida os resultados chave em um `pandas DataFrame`, o formato ideal para a visualização e análise de dados que faremos a seguir.\n",
        "\n",
        "> **Nota para o Futuro:** O design desta classe é o pináculo da modularidade. Ela depende apenas das **interfaces** (Protocolos) e das **estruturas de dados** (Dataclasses) que definimos. Isso significa que poderíamos reutilizar o `ComparativeAnalyzer` para testar outras variáveis do nosso sistema, como diferentes **modelos de embedding** ou diferentes **tipos de retriever**, com modificações mínimas, simplesmente criando novas listas de \"estratégias\" para testar."
      ],
      "metadata": {
        "id": "BxjpWlhGFw4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pickle\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Any, Optional, Tuple, Set, Iterable, Dict\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class ComparativeAnalyzer:\n",
        "    \"\"\"\n",
        "    Executa análises comparativas entre diferentes estratégias de RAG.\n",
        "    Responsabilidade única: Orquestrar e comparar múltiplos experimentos\n",
        "    para encontrar a melhor configuração.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rag_analyzer: Any, cache: Optional[Any] = None, cache_base_folder: str = \".rag_cache/vectorstores\"):\n",
        "        self.rag_analyzer = rag_analyzer\n",
        "        self.cache = cache  # ExperimentCache opcional (pode expor load_vectorstore_meta / save_vectorstore_meta)\n",
        "        self.cache_base_folder = Path(cache_base_folder)\n",
        "        self.cache_base_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Método principal\n",
        "    # -----------------------\n",
        "    def run_analysis(self,\n",
        "                     docs: List[Any],\n",
        "                     embeddings_model: Any,\n",
        "                     strategies: List[Any],\n",
        "                     query: str,\n",
        "                     ground_truth_set: Iterable[Any],\n",
        "                     pause_between: int = 15) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Orquestra a execução de uma análise comparativa completa,\n",
        "        testando cada estratégia e compilando os resultados.\n",
        "\n",
        "        - docs: lista de documentos originais (Document do LangChain ou similar)\n",
        "        - embeddings_model: instância/modelo de embeddings usado pelo FAISS.from_documents\n",
        "        - strategies: lista de objetos ChunkingStrategy (com .chunk_size, .chunk_overlap, .name)\n",
        "        - query: string com a consulta a ser avaliada\n",
        "        - ground_truth_set: pode ser:\n",
        "            * lista/set de Document (com .page_content)\n",
        "            * lista/set de strings (conteúdos)\n",
        "            * lista/set de chunk_ids (ex: \"arquivo.pdf::p0::c3\") -> será mapeado para conteudos em `docs`\n",
        "        \"\"\"\n",
        "        analysis_results = []\n",
        "\n",
        "        # --- Normaliza ground_truth para uma lista de objetos com .page_content ---\n",
        "        normalized_gt = self._normalize_ground_truth(ground_truth_set, docs)\n",
        "\n",
        "        for i, strategy in enumerate(strategies):\n",
        "            print(f\"\\n{'='*90}\")\n",
        "            print(f\"🧪 Testando Estratégia {i+1}/{len(strategies)}: {getattr(strategy,'name',str(strategy))} ({strategy.chunk_size}/{strategy.chunk_overlap})\")\n",
        "\n",
        "            # 1. DELEGA a criação de chunks e do vectorstore (agora usa cache se disponível)\n",
        "            vectorstore, total_chunks = self._create_vectorstore(docs, strategy, embeddings_model)\n",
        "            if vectorstore is None:\n",
        "                print(\"  - Pulando estratégia devido a erro na criação do vectorstore.\")\n",
        "                continue\n",
        "\n",
        "            # 2. DELEGA a análise da estratégia para o RAGAnalyzer\n",
        "            experiment = self.rag_analyzer.analyze_strategy(\n",
        "                vectorstore, strategy, query, normalized_gt, total_chunks,\n",
        "                pause_between_evals=6  # gancho para pausar entre avaliações se necessário\n",
        "            )\n",
        "\n",
        "            # 3. COMPILA os resultados para a tabela de comparação\n",
        "            if experiment:\n",
        "                metrics = getattr(experiment, \"metrics\", None)\n",
        "                if metrics is None:\n",
        "                    print(\"  - Experimento retornou sem métricas, ignorando.\")\n",
        "                else:\n",
        "                    strategy_results = {\n",
        "                        'Query': query,\n",
        "                        'Estratégia': getattr(experiment.strategy, \"name\", str(experiment.strategy)),\n",
        "                        'Precision (%)': float(getattr(metrics, 'precision', 0.0)) * 100.0,\n",
        "                        'Recall (%)': float(getattr(metrics, 'recall', 0.0)) * 100.0,\n",
        "                        'F1-Score (%)': float(getattr(metrics, 'f1_score', 0.0)) * 100.0,\n",
        "                        'Acertos': int(getattr(metrics, 'true_positives', 0)),\n",
        "                        'Recuperados': int(getattr(metrics, 'retrieved_count', 0)),\n",
        "                        'Gabarito': int(getattr(metrics, 'ground_truth_count', 0))\n",
        "                    }\n",
        "                    analysis_results.append(strategy_results)\n",
        "\n",
        "            # Pausa entre estratégias (para respeitar rate limits)\n",
        "            if i < len(strategies) - 1:\n",
        "                print(f\"⏳ Pausando {pause_between}s para respeitar os limites da API...\")\n",
        "                time.sleep(pause_between)\n",
        "\n",
        "        # Retorna DataFrame (pode estar vazio)\n",
        "        df = pd.DataFrame(analysis_results)\n",
        "        return df\n",
        "\n",
        "    # -----------------------\n",
        "    # Criação / carregamento do vectorstore (com cache)\n",
        "    # -----------------------\n",
        "    def _create_vectorstore(self, docs: List[Any], strategy: Any, embeddings_model: Any) -> Tuple[Optional[Any], int]:\n",
        "        \"\"\"\n",
        "        Cria ou carrega um vectorstore FAISS para a estratégia especificada.\n",
        "        Retorna (vectorstore, total_chunks).\n",
        "        \"\"\"\n",
        "        print(\"  - Criando chunks e gerando embeddings...\")\n",
        "\n",
        "        cache_folder = self.cache_base_folder / f\"vs_{strategy.chunk_size}_{strategy.chunk_overlap}\"\n",
        "        cache_folder_str = str(cache_folder)\n",
        "\n",
        "        # Tenta carregar do cache\n",
        "        if cache_folder.exists():\n",
        "            try:\n",
        "                from langchain.vectorstores import FAISS  # import local\n",
        "                vectorstore = FAISS.load_local(cache_folder_str, embeddings_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "                # Tenta obter total_chunks de várias maneiras\n",
        "                total_chunks = 0\n",
        "                # 1) via obj cache (se cache tiver meta)\n",
        "                if self.cache and hasattr(self.cache, \"load_vectorstore_meta\"):\n",
        "                    try:\n",
        "                        meta = self.cache.load_vectorstore_meta(cache_folder_str)\n",
        "                        if isinstance(meta, dict) and \"total_chunks\" in meta:\n",
        "                            total_chunks = int(meta[\"total_chunks\"])\n",
        "                    except Exception:\n",
        "                        total_chunks = 0\n",
        "\n",
        "                # 2) tentar ler um docstore.pkl (padrão de algumas versões)\n",
        "                if total_chunks == 0:\n",
        "                    pkl_path = cache_folder / \"docstore.pkl\"\n",
        "                    if pkl_path.exists():\n",
        "                        try:\n",
        "                            with open(pkl_path, \"rb\") as f:\n",
        "                                loaded = pickle.load(f)\n",
        "                                # Em algumas implementações, pickle contém (index, docstore)\n",
        "                                if isinstance(loaded, tuple) and len(loaded) >= 2:\n",
        "                                    _, docstore = loaded[:2]\n",
        "                                    try:\n",
        "                                        total_chunks = len(docstore)\n",
        "                                    except Exception:\n",
        "                                        total_chunks = 0\n",
        "                        except Exception:\n",
        "                            total_chunks = 0\n",
        "\n",
        "                # 3) tentar usar atributo index.ntotal do FAISS\n",
        "                if total_chunks == 0:\n",
        "                    try:\n",
        "                        idx = getattr(vectorstore, \"index\", None)\n",
        "                        if idx is not None and hasattr(idx, \"ntotal\"):\n",
        "                            total_chunks = int(idx.ntotal)\n",
        "                    except Exception:\n",
        "                        total_chunks = 0\n",
        "\n",
        "                print(f\"    ✅ Vectorstore carregado do cache: {cache_folder_str} ({total_chunks} chunks)\")\n",
        "                return vectorstore, total_chunks\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ⚠️ Falha ao carregar cache do vectorstore: {e}. Recriando...\")\n",
        "\n",
        "        # Se não houver cache ou falha no carregamento, cria do zero\n",
        "        try:\n",
        "            print(\"    🔄 Criando chunks e gerando embeddings (primeira vez)...\")\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=strategy.chunk_size,\n",
        "                chunk_overlap=strategy.chunk_overlap\n",
        "            )\n",
        "            chunks = splitter.split_documents(docs)\n",
        "\n",
        "            # Adiciona IDs estáveis aos chunks ANTES de criar o vectorstore\n",
        "            chunks = ComparativeAnalyzer.adicionar_ids_estaveis_aos_chunks(chunks)\n",
        "\n",
        "            total_chunks = len(chunks)\n",
        "            print(f\"      → {total_chunks} chunks gerados.\")\n",
        "\n",
        "            from langchain.vectorstores import FAISS  # import local\n",
        "            vectorstore = FAISS.from_documents(chunks, embeddings_model)\n",
        "\n",
        "            # Salva no cache se possível\n",
        "            try:\n",
        "                vectorstore.save_local(cache_folder_str)\n",
        "                if self.cache and hasattr(self.cache, \"save_vectorstore_meta\"):\n",
        "                    try:\n",
        "                        self.cache.save_vectorstore_meta(cache_folder_str, {\"total_chunks\": total_chunks})\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                print(f\"    ✅ Vectorstore criado e salvo no cache: {cache_folder_str}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ⚠️ Falha ao salvar vectorstore no cache: {e}\")\n",
        "\n",
        "            return vectorstore, total_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ Erro ao criar VectorStore: {e}\")\n",
        "            return None, 0\n",
        "\n",
        "    # -----------------------\n",
        "    # Utilitário: garante chunk_id estável\n",
        "    # -----------------------\n",
        "    @staticmethod\n",
        "    def adicionar_ids_estaveis_aos_chunks(chunks: List[Any]) -> List[Any]:\n",
        "        \"\"\"Garante que cada chunk tenha um 'chunk_id' único e determinístico.\"\"\"\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            meta = getattr(chunk, \"metadata\", {}) or {}\n",
        "            source_path = Path(meta.get('source', 'doc_desconhecido'))\n",
        "            source = source_path.name\n",
        "            page = meta.get('page', '0')\n",
        "            chunk_id = f\"{source}::p{page}::c{i}\"\n",
        "\n",
        "            if not hasattr(chunk, \"metadata\"):\n",
        "                chunk.metadata = {}\n",
        "            chunk.metadata['chunk_id'] = chunk_id\n",
        "        return chunks\n",
        "\n",
        "    # -----------------------\n",
        "    # Normalização do ground truth\n",
        "    # -----------------------\n",
        "    def _normalize_ground_truth(self, ground_truth: Iterable[Any], docs: List[Any]) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Converte o ground_truth (que pode ser várias formas) para uma LISTA de objetos com `.page_content`.\n",
        "        - Se um item for string:\n",
        "            * se parecer um chunk_id (regex ::p<page>::c<index>), tenta mapear para `docs` pelo metadata['chunk_id']\n",
        "            * senão, assume que é conteúdo textual e embrulha em SimpleNamespace(page_content=...)\n",
        "        - Se for objeto com .page_content, mantem.\n",
        "        - Retorna lista de objetos (Document-like) com .page_content\n",
        "        \"\"\"\n",
        "        if ground_truth is None:\n",
        "            return []\n",
        "\n",
        "        # Normalize to list\n",
        "        items = list(ground_truth)\n",
        "\n",
        "        normalized: List[Any] = []\n",
        "        # Build quick lookup from docs metadata chunk_id -> doc\n",
        "        chunkid_to_doc = {}\n",
        "        for doc in docs:\n",
        "            meta = getattr(doc, \"metadata\", {}) or {}\n",
        "            cid = meta.get(\"chunk_id\")\n",
        "            if cid:\n",
        "                chunkid_to_doc[cid] = doc\n",
        "\n",
        "        chunk_id_pattern = re.compile(r\"[\\w\\.\\-:]+::p\\d+::c\\d+\")\n",
        "\n",
        "        for item in items:\n",
        "            # Already a doc-like with page_content\n",
        "            if hasattr(item, \"page_content\"):\n",
        "                normalized.append(item)\n",
        "                continue\n",
        "\n",
        "            # strings: could be chunk_id or content\n",
        "            if isinstance(item, str):\n",
        "                # chunk_id?\n",
        "                if chunk_id_pattern.fullmatch(item):\n",
        "                    doc_found = chunkid_to_doc.get(item)\n",
        "                    if doc_found:\n",
        "                        normalized.append(doc_found)\n",
        "                        continue\n",
        "                    else:\n",
        "                        # Se não encontrou o chunk_id nos docs, adiciona como string (não muito ideal,\n",
        "                        # mas evita crash — será tratado como conteúdo literal)\n",
        "                        normalized.append(SimpleNamespace(page_content=item))\n",
        "                        continue\n",
        "                else:\n",
        "                    # considerado conteúdo textual\n",
        "                    normalized.append(SimpleNamespace(page_content=item))\n",
        "                    continue\n",
        "\n",
        "            # Outros tipos: embrulha\n",
        "            normalized.append(SimpleNamespace(page_content=str(item)))\n",
        "\n",
        "        return normalized\n"
      ],
      "metadata": {
        "id": "YWr-bdaEFwla"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 12: A Célula de Visualização (O Dashboard de Resultados)**\n",
        "\n",
        "Esta é a célula final do nosso pipeline de implementação. Sua única responsabilidade é pegar os dados numéricos compilados pelo `ComparativeAnalyzer` e traduzi-los em **visualizações claras e intuitivas**.\n",
        "\n",
        "Um bom gráfico é, muitas vezes, a forma mais eficaz de comunicar os resultados de uma análise complexa. Em vez de uma única visualização, criamos um **dashboard 2x2**, onde cada gráfico responde a uma pergunta específica sobre a performance das nossas estratégias.\n",
        "\n",
        "### **Análise do Dashboard:**\n",
        "\n",
        "1.  **Gráfico 1 (Superior Esquerdo): Comparativo de Métricas Principais**\n",
        "    *   **O que é:** Um gráfico de barras agrupado.\n",
        "    *   **Pergunta que responde:** \"Como as estratégias se comparam nas três métricas fundamentais (Precision, Recall, F1-Score) lado a lado?\"\n",
        "    *   **Como ler:** Ele nos dá a visão mais completa da performance. Uma estratégia ideal teria barras altas em todas as três métricas.\n",
        "\n",
        "2.  **Gráfico 2 (Superior Direito): Contagem de Acertos**\n",
        "    *   **O que é:** Um gráfico de barras simples.\n",
        "    *   **Pergunta que responde:** \"Em termos absolutos, qual estratégia foi mais eficaz em *encontrar* a informação relevante?\"\n",
        "    *   **Como ler:** Esta é a nossa medida de eficácia bruta. Uma barra mais alta significa que a estratégia conseguiu recuperar mais \"pedaços\" corretos do nosso gabarito.\n",
        "\n",
        "3.  **Gráfico 3 (Inferior Esquerdo): O Trade-off: Precision vs. Recall**\n",
        "    *   **O que é:** Um gráfico de dispersão (scatter plot).\n",
        "    *   **Pergunta que responde:** \"Existe um trade-off entre ser preciso e encontrar tudo? Qual estratégia oferece o melhor equilíbrio?\"\n",
        "    *   **Como ler:** O \"canto superior direito\" deste gráfico é o ponto ideal (alta precisão e alto recall). O **tamanho e a cor da bolha** representam o F1-Score, nos mostrando visualmente qual estratégia encontrou o melhor equilíbrio.\n",
        "\n",
        "4.  **Gráfico 4 (Inferior Direito): Ranking Final por F1-Score**\n",
        "    *   **O que é:** Um gráfico de barras horizontal e ordenado.\n",
        "    *   **Pergunta que responde:** \"Qual é a conclusão final? Qual estratégia é a vencedora?\"\n",
        "    *   **Como ler:** Este é o nosso \"pódio\". As estratégias são ranqueadas pela métrica de equilíbrio (F1-Score), da pior para a melhor. As cores e as linhas de referência nos dão uma classificação instantânea de performance (vermelho/laranja/verde).\n",
        "\n",
        "> **Nota para o Futuro:** Este dashboard é a ponte entre nossa análise técnica e a tomada de decisão. É a parte do projeto que você mostraria para um líder de equipe, um gerente de produto ou um cliente para justificar suas escolhas de arquitetura de forma clara, visual e baseada em dados."
      ],
      "metadata": {
        "id": "kasEUPWqHOwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_evaluation_heatmap(results_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Plota um heatmap mostrando a performance (F1-Score) de cada estratégia\n",
        "    em cada uma das perguntas de teste.\n",
        "    \"\"\"\n",
        "    if results_df.empty:\n",
        "        print(\"❌ Nenhum resultado para plotar.\")\n",
        "        return\n",
        "\n",
        "    # Prepara a matriz para o heatmap\n",
        "    heatmap_data = results_df.pivot(index='Estratégia', columns='Query', values='F1-Score (%)')\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", linewidths=.5, cmap=\"viridis\",\n",
        "                cbar_kws={'label': 'F1-Score (%)'})\n",
        "\n",
        "    plt.title('Heatmap de Performance de Estratégias de RAG (F1-Score %)', fontsize=18, weight='bold')\n",
        "    plt.xlabel('Perguntas de Teste', fontsize=12)\n",
        "    plt.ylabel('Estratégias de Chunking', fontsize=12)\n",
        "    plt.xticks(rotation=15, ha=\"right\")\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "def print_final_analysis_aggregated(results_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Imprime uma análise final agregada, calculando a média de performance\n",
        "    e declarando a estratégia vencedora.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🏆 ANÁLISE FINAL AGREGADA DOS RESULTADOS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Calcula a performance média de cada estratégia\n",
        "    summary = results_df.groupby('Estratégia').agg(\n",
        "        Avg_F1_Score=('F1-Score (%)', 'mean'),\n",
        "        Avg_Precision=('Precision (%)', 'mean'),\n",
        "        Avg_Recall=('Recall (%)', 'mean')\n",
        "    ).reset_index().sort_values(by='Avg_F1_Score', ascending=False)\n",
        "\n",
        "    print(\"📊 TABELA DE PERFORMANCE MÉDIA:\")\n",
        "    print(\"=\"*80)\n",
        "    print(summary.to_string(index=False, float_format='%.2f'))\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Declara o vencedor\n",
        "    if not summary.empty:\n",
        "        winner = summary.iloc[0]\n",
        "        print(f\"\\n🥇 ESTRATÉGIA VENCEDORA (Maior Média de F1-Score): '{winner['Estratégia']}'\")\n",
        "        print(f\"   - ⚖️ Média de F1-Score: {winner['Avg_F1_Score']:.2f}%\")\n",
        "        print(f\"   - 🎯 Média de Precision: {winner['Avg_Precision']:.2f}%\")\n",
        "        print(f\"   - 🎣 Média de Recall: {winner['Avg_Recall']:.2f}%\")\n",
        "\n",
        "    print(\"\\n💡 RECOMENDAÇÃO:\")\n",
        "    print(\"A estratégia vencedora demonstrou o melhor equilíbrio de performance em todo o conjunto de testes.\")\n",
        "    print(\"O heatmap acima mostra visualmente a consistência de cada estratégia. Estratégias com cores consistentemente 'quentes' (amarelo/verde) são as mais robustas.\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "ls-LVaJyHYMq"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 13: O Utilitário de Carregamento de Documentos**\n",
        "\n",
        "Esta é a última classe de \"especialista\" da nossa arquitetura. A classe `DocumentLoader` encapsula toda a lógica de **entrada de dados**, baixando e extraindo o texto de arquivos PDF a partir de URLs.\n",
        "\n",
        "Ao isolar esta funcionalidade, nosso script de execução principal se torna muito mais limpo, focando apenas em orquestrar a análise, sem se preocupar com os detalhes de como os documentos são obtidos."
      ],
      "metadata": {
        "id": "IHZGfAyaHiB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_documentos_de_urls(lista_de_urls: list, timeout: int = 20) -> list:\n",
        "    \"\"\"\n",
        "    Baixa e carrega PDFs, sobrescrevendo os metadados 'source' com o nome\n",
        "    do arquivo original para a criação de IDs estáveis.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Carregando Documentos ---\")\n",
        "    documentos_finais = []\n",
        "\n",
        "    for url in lista_de_urls:\n",
        "        # Decodifica o nome do arquivo para usar como 'source' estável\n",
        "        nome_arquivo = unquote(url.split('/')[-1])\n",
        "\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".pdf\") as tmp:\n",
        "                dl_url = url + \"?raw=true\" if \"github.com\" in url and \"?raw=true\" not in url else url\n",
        "                resp = requests.get(dl_url, timeout=timeout)\n",
        "                resp.raise_for_status()\n",
        "                tmp.write(resp.content)\n",
        "                tmp.flush()\n",
        "\n",
        "                loader = PyMuPDFLoader(tmp.name)\n",
        "                docs_carregados = loader.load()\n",
        "\n",
        "                # <<< A CORREÇÃO CRÍTICA ESTÁ AQUI >>>\n",
        "                # Itera sobre cada página carregada e corrige seus metadados\n",
        "                for doc in docs_carregados:\n",
        "                    doc.metadata['source'] = nome_arquivo\n",
        "\n",
        "                documentos_finais.extend(docs_carregados)\n",
        "            print(f\"  ✅ '{nome_arquivo}' carregado ({len(docs_carregados)} páginas).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erro ao carregar '{nome_arquivo}': {e}\")\n",
        "\n",
        "    print(f\"Carregamento concluído: {len(documentos_finais)} páginas totais carregadas.\")\n",
        "    return documentos_finais"
      ],
      "metadata": {
        "id": "tS0PwPwyb8lJ"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 14: A Função Principal `main()`**\n",
        "\n",
        "Com todos os nossos componentes e classes especialistas definidos, esta célula contém a função `main()`, que atua como o **orquestrador mestre** de todo o nosso pipeline.\n",
        "\n",
        "Ela define a sequência lógica de operações, do início ao fim:\n",
        "1.  Configura os modelos de LLM e Embedding.\n",
        "2.  Usa o `DocumentLoader` para carregar os dados.\n",
        "3.  Define as `ChunkingStrategy` que queremos comparar.\n",
        "4.  Usa o `GroundTruthGenerator` para criar nosso gabarito.\n",
        "5.  Usa o `ComparativeAnalyzer` para executar os experimentos.\n",
        "6.  Chama as funções de visualização e análise para apresentar os resultados.\n",
        "\n",
        "Manter a lógica de execução principal dentro de uma função `main()` é uma prática padrão que torna o código organizado e fácil de entender.\n"
      ],
      "metadata": {
        "id": "kdkCYKAfd2Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Função principal que executa a avaliação completa, testando\n",
        "    múltiplas estratégias contra um conjunto de perguntas críticas.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\" INICIANDO FRAMEWORK DE AVALIAÇÃO DE RAG\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Etapa 0: Inicializar o Cache ---\n",
        "    experiment_cache = ExperimentCache()\n",
        "    # Descomente a linha abaixo se quiser forçar a recriação de todos os caches\n",
        "    #experiment_cache.gt_cache_dir.rmdir() # CUIDADO: apaga o cache de GT\n",
        "\n",
        "    # --- Etapa 1: Configuração dos Modelos ---\n",
        "    print(\"\\n--- Etapa 1: Configurando Modelos ---\")\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY_4')\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
        "        llm_adapter = LLMAdapter(llm)\n",
        "        print(\"✅ Modelos configurados com sucesso!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro na configuração: {e}.\")\n",
        "        return\n",
        "\n",
        "    # --- Etapa 2: Carregamento dos Documentos ---\n",
        "    urls = [\n",
        "        \"https://github.com/YuriArduino/Estudos_Artificial_Intelligence/blob/Dados/Pol%C3%ADtica%20de%20Reembolsos%20(Viagens%20e%20Despesas).pdf\",\n",
        "        \"https://github.com/YuriArduino/Estudos_Artificial_Intelligence/blob/Dados/Pol%C3%ADtica%20de%20Uso%20de%20E-mail%20e%20Seguran%C3%A7a%20da%20Informa%C3%A7%C3%A3o.pdf\",\n",
        "        \"https://github.com/YuriArduino/Estudos_Artificial_Intelligence/blob/Dados/Pol%C3%ADticas%20de%20Home%20Office.pdf\"\n",
        "    ]\n",
        "    docs = DocumentLoader.load_from_urls(urls)\n",
        "    if not docs: return\n",
        "\n",
        "    # --- Etapa 3: Definição do Conjunto de Avaliação ---\n",
        "    print(\"\\n--- Etapa 3: Definindo o Conjunto de Avaliação ---\")\n",
        "    strategies = [\n",
        "        ChunkingStrategy(name=\"Pequeno (70/20)\", chunk_size=70, chunk_overlap=20),\n",
        "        ChunkingStrategy(name=\"Médio (150/25)\", chunk_size=150, chunk_overlap=25),\n",
        "        ChunkingStrategy(name=\"Grande (300/30)\", chunk_size=300, chunk_overlap=30)\n",
        "    ]\n",
        "    evaluation_queries = [\n",
        "        \"Posso reembolsar a internet do meu home office?\",\n",
        "        \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "        \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "        \"Quantas capivaras tem no Rio Pinheiros?\"\n",
        "    ]\n",
        "    print(f\"✅ {len(strategies)} estratégias e {len(evaluation_queries)} perguntas de teste definidas.\")\n",
        "\n",
        "    # --- Etapa 4: Gerando o Corpus de Ground Truth ---\n",
        "    print(\"\\n--- Etapa 4: Gerando o Corpus de Ground Truth ---\")\n",
        "    ground_truth_generator = GroundTruthGenerator(llm_adapter)\n",
        "\n",
        "    # <<< CORREÇÃO DE DESIGN: A estratégia de GT agora é a mais granular das que estamos testando >>>\n",
        "    gt_strategy = strategies[0] # Usa \"Pequeno (70/20)\" como a base da verdade\n",
        "    print(f\"Usando a estratégia '{gt_strategy.name}' como base para o Ground Truth.\")\n",
        "\n",
        "    splitter_gt = RecursiveCharacterTextSplitter(chunk_size=gt_strategy.chunk_size, chunk_overlap=gt_strategy.chunk_overlap)\n",
        "    chunks_for_gt = splitter_gt.split_documents(docs)\n",
        "    chunks_for_gt = ComparativeAnalyzer.adicionar_ids_estaveis_aos_chunks(chunks_for_gt)\n",
        "\n",
        "    model_metadata = {\"model_name\": getattr(llm, 'model_name', 'unknown'), \"temperature\": getattr(llm, 'temperature', -1.0)}\n",
        "\n",
        "    ground_truth_corpus = {}\n",
        "    for query in evaluation_queries:\n",
        "        cached_gt = experiment_cache.get_ground_truth(query, docs)\n",
        "        if cached_gt is not None:\n",
        "            print(f\"  ✅ Ground Truth para a query '{query[:40]}...' carregado do cache.\")\n",
        "            ground_truth_corpus[query] = cached_gt\n",
        "        else:\n",
        "            gt_set = ground_truth_generator.derive_ground_truth(chunks_for_gt, query, batch_size=5, pause_between=6)\n",
        "            ground_truth_corpus[query] = gt_set\n",
        "            experiment_cache.save_ground_truth(gt_set, query, docs, model_info=model_metadata)\n",
        "\n",
        "    # --- Etapa 5: Executando Análise Comparativa com Cache ---\n",
        "    print(\"\\n--- Etapa 5: Executando Análise Comparativa com Cache ---\")\n",
        "    rag_analyzer = RAGAnalyzer(llm_adapter)\n",
        "    comparative_analyzer = ComparativeAnalyzer(rag_analyzer, cache=experiment_cache)\n",
        "\n",
        "    all_results_list = []\n",
        "    for query in evaluation_queries:\n",
        "        df_for_query = comparative_analyzer.run_analysis(\n",
        "            docs=docs, embeddings_model=embeddings, strategies=strategies,\n",
        "            query=query, ground_truth_set=ground_truth_corpus.get(query, set()),\n",
        "            pause_between=15\n",
        "        )\n",
        "        all_results_list.append(df_for_query)\n",
        "\n",
        "    results_df = pd.concat(all_results_list, ignore_index=True)\n",
        "\n",
        "    # --- Etapa 6: Visualização e Análise Final ---\n",
        "    print(\"\\n--- Etapa 6: Exibindo Resultados Finais Agregados ---\")\n",
        "    if not results_df.empty:\n",
        "        plot_evaluation_heatmap(results_df)\n",
        "        print_final_analysis_aggregated(results_df)\n",
        "        export_results_to_csv(results_df)\n",
        "    else:\n",
        "        print(\"❌ Nenhum resultado foi gerado na análise comparativa.\")"
      ],
      "metadata": {
        "id": "XzWS_3wjeRkj"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 15: Funções Auxiliares para Execução (O Painel de Controle)**\n",
        "\n",
        "Com toda a nossa arquitetura de classes especialistas definida, esta célula cria um conjunto de **funções de alto nível** que servem como nosso \"painel de controle\". Elas usam as classes que construímos para realizar tarefas completas, tornando o script de execução final muito mais limpo e legível.\n",
        "\n",
        "### **As Ferramentas do Painel de Controle:**\n",
        "\n",
        "1.  **`analyze_single_strategy`**:\n",
        "    *   **Propósito:** É a nossa \"lupa\". Permite executar o pipeline de análise completo para **uma única estratégia**, do início ao fim. É a ferramenta perfeita para depuração ou para um \"mergulho profundo\" em uma configuração específica que parece promissora.\n",
        "\n",
        "2.  **`export_results_to_csv`**:\n",
        "    *   **Propósito:** Persistência de dados. Após uma análise comparativa demorada e cara, esta função salva os resultados finais em um arquivo CSV. Isso nos permite fechar o notebook e voltar depois para re-analisar ou re-visualizar os dados sem ter que rodar todo o experimento novamente.\n",
        "\n",
        "3.  **`load_results_from_csv`**:\n",
        "    *   **Propósito:** Continuidade do trabalho. A contraparte da função de exportação, ela carrega os resultados de um arquivo CSV, permitindo-nos retomar a análise de onde paramos."
      ],
      "metadata": {
        "id": "EYurJ3S1Hsef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_single_strategy(strategy: ChunkingStrategy,\n",
        "                           docs: list,\n",
        "                           embeddings_model: any,\n",
        "                           query: str,\n",
        "                           llm: any,\n",
        "                           k_values: list = None) -> dict:\n",
        "    \"\"\"\n",
        "    Executa uma análise detalhada em uma única estratégia.\n",
        "    Útil para \"mergulhar fundo\" e depurar uma configuração específica.\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔬 ANÁLISE DETALHADA DA ESTRATÉGIA: {strategy.name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Preparação ---\n",
        "    llm_adapter = LLMAdapter(llm)\n",
        "    analyzer = RAGAnalyzer(llm_adapter)\n",
        "\n",
        "    # Cria os chunks para esta estratégia específica\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=strategy.chunk_size, chunk_overlap=strategy.chunk_overlap)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    # Gera o ground truth específico para ESTA configuração de chunks\n",
        "    ground_truth = analyzer.generate_ground_truth(chunks, query, pause_between=6)\n",
        "\n",
        "    # Cria o vectorstore\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings_model)\n",
        "\n",
        "    # --- Execução da Análise ---\n",
        "    experiment = analyzer.analyze_strategy(\n",
        "        vectorstore=vectorstore,\n",
        "        strategy=strategy,\n",
        "        query=query,\n",
        "        ground_truth_set=ground_truth,\n",
        "        total_chunks=len(chunks),\n",
        "        k_values=k_values or [1, 3, 5, 8, 10]\n",
        "    )\n",
        "\n",
        "    print(\"✅ Análise detalhada concluída.\")\n",
        "    return {\n",
        "        'experiment': experiment,\n",
        "        'chunks': chunks,\n",
        "        'ground_truth': ground_truth,\n",
        "        'vectorstore': vectorstore\n",
        "    }\n",
        "\n",
        "def export_results_to_csv(results_df: pd.DataFrame, filename: str = None):\n",
        "    \"\"\"Exporta o DataFrame de resultados para um arquivo CSV.\"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"rag_analysis_results_{timestamp}.csv\"\n",
        "\n",
        "    results_df.to_csv(filename, index=False)\n",
        "    print(f\"\\n💾 Resultados da análise comparativa exportados para: {filename}\")\n",
        "\n",
        "def load_results_from_csv(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"Carrega resultados de um arquivo CSV para re-análise ou visualização.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filename)\n",
        "        print(f\"✅ Resultados carregados de: {filename}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Erro: Arquivo '{filename}' não encontrado.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao carregar {filename}: {e}\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "zm3vXVaHHqfQ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capítulo 16: Ponto de Entrada - Execução do Pipeline**\n",
        "\n",
        "Esta é a célula final e mais simples. Sua única responsabilidade é **chamar a função `main()`**, que inicia todo o processo de análise que construímos.\n",
        "\n",
        "Ao separar a **definição** (`main()`) da **execução** (`main()`), tornamos nosso notebook mais flexível. Poderíamos, por exemplo, importar a função `main()` em outro script e executá-la com diferentes parâmetros, se necessário."
      ],
      "metadata": {
        "id": "BvYD-RncHzyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qSLdMs5vHy3j",
        "outputId": "5dd05aac-5545-4471-c4ca-a61272a60100"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            " INICIANDO FRAMEWORK DE AVALIAÇÃO DE RAG\n",
            "================================================================================\n",
            "\n",
            "--- Etapa 1: Configurando Modelos ---\n",
            "✅ Modelos configurados com sucesso!\n",
            "\n",
            "--- Etapa 2: Carregando Documentos ---\n",
            "  ✅ Carregado: https://github.com/YuriArduino/Estudos_Artificial_Intelligence/blob/Dados/Política de Reembolsos (Viagens e Despesas).pdf (1 páginas)\n",
            "  ✅ Carregado: https://github.com/YuriArduino/Estudos_Artificial_Intelligence/blob/Dados/Política de Uso de E-mail e Segurança da Informação.pdf (1 páginas)\n",
            "  ✅ Carregado: https://github.com/YuriArduino/Estudos_Artificial_Intelligence/blob/Dados/Políticas de Home Office.pdf (1 páginas)\n",
            "✅ 3 documentos carregados no total.\n",
            "\n",
            "--- Etapa 3: Definindo o Conjunto de Avaliação ---\n",
            "✅ 3 estratégias e 4 perguntas de teste definidas.\n",
            "\n",
            "--- Etapa 4: Gerando o Corpus de Ground Truth ---\n",
            "Usando a estratégia 'Pequeno (70/20)' como base para o Ground Truth.\n",
            "  ✅ Ground Truth para a query 'Posso reembolsar a internet do meu home ...' carregado do cache.\n",
            "  ✅ Ground Truth para a query 'Quero mais 5 dias de trabalho remoto. Co...' carregado do cache.\n",
            "  ✅ Ground Truth para a query 'Posso reembolsar cursos ou treinamentos ...' carregado do cache.\n",
            "  ✅ Ground Truth para a query 'Quantas capivaras tem no Rio Pinheiros?...' carregado do cache.\n",
            "\n",
            "--- Etapa 5: Executando Análise Comparativa com Cache ---\n",
            "\n",
            "==========================================================================================\n",
            "🧪 Testando Estratégia 1/3: Pequeno (70/20) (70/20)\n",
            "  - Criando chunks e gerando embeddings...\n",
            "    ✅ Vectorstore carregado do cache: .rag_cache/vectorstores/vs_70_20 (53 chunks)\n",
            "\n",
            "🔍 Analisando estratégia: Pequeno (70/20) (k=10)\n",
            "  - 🔄 Re-ranking dos top 10 resultados com LLM...\n",
            "  - Avaliando 10 chunks recuperados com o 'LLM Juiz'...\n",
            "    - Chunk ID Política de Reembolsos (Viagens e Despesas).pdf::p0::c10: ✅ Relevante (Confiança: 100%)\n",
            "    - Chunk ID Políticas de Home Office.pdf::p0::c47: ✅ Relevante (Confiança: 95%)\n",
            "    - Chunk ID Políticas de Home Office.pdf::p0::c49: ✅ Relevante (Confiança: 90%)\n",
            "    - Chunk ID Política de Reembolsos (Viagens e Despesas).pdf::p0::c1: ✅ Relevante (Confiança: 80%)\n",
            "    - Chunk ID Política de Reembolsos (Viagens e Despesas).pdf::p0::c12: ❌ Irrelevante (Confiança: 40%)\n",
            "    - Chunk ID Políticas de Home Office.pdf::p0::c48: ❌ Irrelevante (Confiança: 90%)\n",
            "    - Chunk ID Política de Reembolsos (Viagens e Despesas).pdf::p0::c6: ✅ Relevante (Confiança: 80%)\n",
            "    - Chunk ID Políticas de Home Office.pdf::p0::c50: ❌ Irrelevante (Confiança: 90%)\n",
            "    - Chunk ID Políticas de Home Office.pdf::p0::c34: ✅ Relevante (Confiança: 70%)\n",
            "    - Chunk ID Política de Reembolsos (Viagens e Despesas).pdf::p0::c0: ✅ Relevante (Confiança: 95%)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'MetricsCalculator' has no attribute '_calculate_precision_at_k'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3832242952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1138332972.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mall_results_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_queries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         df_for_query = comparative_analyzer.run_analysis(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mground_truth_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2779405189.py\u001b[0m in \u001b[0;36mrun_analysis\u001b[0;34m(self, docs, embeddings_model, strategies, query, ground_truth_set, pause_between)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# 2. DELEGA a análise da estratégia para o RAGAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             experiment = self.rag_analyzer.analyze_strategy(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mvectorstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_chunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mpause_between_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m  \u001b[0;31m# gancho para pausar entre avaliações se necessário\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1232609564.py\u001b[0m in \u001b[0;36manalyze_strategy\u001b[0;34m(self, vectorstore, strategy, query, ground_truth_docs, total_chunks, k_values, pause_between_evals)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mnormalized_gt_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_ground_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_calculator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_gt_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mexecution_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3657845632.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(results, ground_truth_docs, k_values)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mprecision_at_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetricsCalculator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_precision_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_contents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         return AnalysisMetrics(\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'MetricsCalculator' has no attribute '_calculate_precision_at_k'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Possiveis Melhorias\n",
        "\n",
        "1- Fallback Api\n",
        "\n",
        "#### **Nova Célula: Configuração de Modelos com Fallback**\n",
        "\n",
        "Substituir a célula de \"Etapa 1: Configurando Modelos\" por esta.\n",
        "\n",
        "```python\n",
        "# CÉLULA DE CONFIGURAÇÃO AVANÇADA (com Fallback de API Key)\n",
        "\n",
        "def setup_models_with_fallback(api_key_names: List[str]) -> Tuple[Optional[ChatGoogleGenerativeAI], Optional[GoogleGenerativeAIEmbeddings]]:\n",
        "    \"\"\"\n",
        "    Tenta inicializar os modelos do Gemini com uma lista de chaves de API em ordem.\n",
        "    Retorna o primeiro par de modelos que funcionar com sucesso.\n",
        "    \"\"\"\n",
        "    from google.colab import userdata\n",
        "    \n",
        "    for key_name in api_key_names:\n",
        "        print(f\"🔄 Tentando inicializar com a chave: '{key_name}'...\")\n",
        "        api_key = userdata.get(key_name)\n",
        "        \n",
        "        if not api_key:\n",
        "            print(f\"  - ⚠️ Chave '{key_name}' não encontrada nos segredos. Pulando.\")\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Tenta inicializar ambos os serviços com a chave atual\n",
        "            llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0, api_key=api_key)\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key=api_key)\n",
        "            \n",
        "            # (Opcional) Poderíamos fazer uma pequena chamada de teste aqui para validar a chave,\n",
        "            # mas por enquanto, a inicialização já é um bom teste.\n",
        "            \n",
        "            print(f\"  - ✅ Sucesso! Modelos configurados com a chave '{key_name}'.\")\n",
        "            return llm, embeddings\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Captura qualquer erro (rate limit, chave inválida, etc.)\n",
        "            print(f\"  - ❌ Falha com a chave '{key_name}'. Erro: {str(e)[:100]}...\") # Mostra preview do erro\n",
        "            continue # Tenta a próxima chave\n",
        "\n",
        "    print(\"\\n❌ ERRO FATAL: Nenhuma das chaves de API fornecidas funcionou.\")\n",
        "    return None, None\n",
        "\n",
        "```\n",
        "\n",
        "### **Como Usar Isso no seu `main()`**\n",
        "\n",
        "Agora, a sua célula `main()` fica incrivelmente limpa e legível. Ela apenas define a lista de chaves e chama nossa nova função de setup.\n",
        "\n",
        "**Na sua célula `main()`:**```python\n",
        "# Dentro da sua função main()\n",
        "\n",
        "# --- Etapa 1: Configurando Modelos com Fallback ---\n",
        "print(\"\\n--- Etapa 1: Configurando Modelos com Fallback ---\")\n",
        "\n",
        "# Defina a ordem de prioridade das suas chaves\n",
        "api_key_prioridade = ['GEMINI_API_KEY', 'GEMINI_API_KEY_2']\n",
        "\n",
        "# Chama a nossa nova função robusta\n",
        "llm, embeddings = setup_models_with_fallback(api_key_prioridade)\n",
        "\n",
        "if not llm or not embeddings:\n",
        "    print(\"Encerrando a análise devido à falha na configuração dos modelos.\")\n",
        "    return\n",
        "\n",
        "# Cria o adapter para o LLM que funcionou\n",
        "llm_adapter = LLMAdapter(llm)\n",
        "```"
      ],
      "metadata": {
        "id": "hRK969EeDCWw"
      }
    }
  ]
}